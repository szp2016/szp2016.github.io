<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[系统调用跟踪]]></title>
    <url>%2FLinux%2Fxv6-syscall%2F</url>
    <content type="text"><![CDATA[1.概述本文记录xv6操作系统的系统调用跟踪实验，xv6是一个类Unix的简单操作系统。该实验是要求实现一个trace系统调用，该系统调用的功能是根据用户传入的系统调用号跟踪某个或者某些进程的系统调用情况。 2. 实验要求2.1 实验铺垫有一个已给出的用户态trace.c程序如下： 1234567891011121314151617181920212223242526272829#include "kernel/param.h"#include "kernel/types.h"#include "kernel/stat.h"#include "user/user.h"intmain(int argc, char *argv[])&#123; int i; //存储待跟踪程序的名称和参数 char *nargv[MAXARG]; //保证trace的参数不少于三个，并且跟踪的系统调用号在0-99之间 if(argc &lt; 3 || (argv[1][0] &lt; '0' || argv[1][0] &gt; '9'))&#123; fprintf(2, "Usage: %s mask command\n", argv[0]); exit(1); &#125; //调用trace系统调用，传入待跟踪系统调用号 if (trace(atoi(argv[1])) &lt; 0) &#123; fprintf(2, "%s: trace failed\n", argv[0]); exit(1); &#125; //保存待跟踪程序的名称和参数 for(i = 2; i &lt; argc &amp;&amp; i &lt; MAXARG; i++)&#123; nargv[i-2] = argv[i]; &#125; //运行待跟踪的程序 exec(nargv[0], nargv); exit(0);&#125; 该程序应该实现的执行效果如下： 12345$ trace 32 grep hello README3: syscall read -&gt; 10233: syscall read -&gt; 9663: syscall read -&gt; 703: syscall read -&gt; 0 命令格式为 $ trace &lt;command_args …&gt; syscall_num是系统调用号，command是待跟踪的程序，command_args是待跟踪程序的参数。例如上面的例子就是跟踪32号系统调用，跟踪的程序是grep。输出的格式如下： : syscall -&gt; pid是进程序号， syscallname是系统调用名称，returnvalue是该系统调用返回值，并且要求各个进程的输出是独立的，不相互干扰。 2.2 实验内容 用户态程序trace.c已经给出，需要我们实现对应的内核态程序trace系统调用(sys_trace)并添加进xv6系统。 trace()函数传入了一个待跟踪的系统调用号，因为需要跟踪的系统调用是进程间独立的，因此sys_trace()系统调用应当实现功能：保存下这个待跟踪的系统调用号到当前进程（为trace.c程序创建的进程）。 xv6中使用struct proc{}表示一个进程，在系统调用被触发的时候就可以根据proc中存储的系统调用号判断当前这个系统调用是不是需要进行跟踪。 trac.c在运行待跟踪程序（如上文的grep命令）的时候并没有发生进程切换，这是将待跟踪系统调用号保存到当前进程（为trace.c程序创建的进程）proc中，并且能够在grep程序运行的时候从proc中取出系统调用号并捕获待跟踪系统调用（read）的关键。当进程调用exec函数时，该进程执行的程序完全替换为新程序，但是并不会创建新的进程，前后的进程id并未改变。exec只是用磁盘上的一个新程序替换了当前进程的代码段、数据段、堆、栈。 内核态系统调用入口程序syscall.c中判断当前进程proc中保存的待跟踪系统调用号是否与当前的系统调用号相同，如果相同则表示捕获到，输出进程id，系统调用函数名称，系统调用返回值。 3. xv6代码结构介绍代码主要有三个部分组成： 第一个是kernel。我们可以ls kernel的内容，里面包含了基本上所有的内核文件。因为XV6是一个宏内核结构，这里所有的文件会被编译成一个叫做kernel的二进制文件，然后这个二进制文件会被运行在kernle mode中。 第二个部分是user。这基本上是运行在user mode的程序。这也是为什么一个目录称为kernel，另一个目录称为user的原因。 第三部分叫做mkfs。它会创建一个空的文件镜像fs.img，我们会将这个镜像存在磁盘上，这样我们就可以直接使用一个空的文件系统。 3.1 创建第一个进程xv6运行在QEMU虚拟机中，QEMU仿真了RISC-V处理器。xv6的起始运行地址是0x80000000，当RISC-V仿真器启动时，它初始化自己并运行一个存储在只读内存中的引导加载程序（boot loader）。引导加载程序将xv6内核加载到内存中。然后,CPU开始在_entry（/kernel/entry.S）以machine mode执行xv6，此时还没有启用分页机制，虚拟地址直接映射到物理地址。 123456789101112131415161718192021 # qemu -kernel loads the kernel at 0x80000000 # and causes each CPU to jump there. # kernel.ld causes the following code to # be placed at 0x80000000..section .text_entry: # set up a stack for C. # stack0 is declared in start.c, # with a 4096-byte stack per CPU. # sp = stack0 + (hartid * 4096) la sp, stack0 li a0, 1024*4 csrr a1, mhartid addi a1, a1, 1 mul a0, a0, a1 add sp, sp, a0 # jump to start() in start.c call startspin: j spin boot loader将xv6内核加载到物理地址0x80000000的内存中，地址区间0x0:0x80000000预留给IO设备。_entry中的指令设置了一个栈，用来运行C程序，随后调用了start函数（start.c）。 123456789101112131415161718192021222324252627282930313233// entry.S jumps here in machine mode on stack0.voidstart()&#123; // set M Previous Privilege mode to Supervisor, for mret. unsigned long x = r_mstatus(); x &amp;= ~MSTATUS_MPP_MASK; x |= MSTATUS_MPP_S; w_mstatus(x); // set M Exception Program Counter to main, for mret. // requires gcc -mcmodel=medany w_mepc((uint64)main); // disable paging for now. w_satp(0); // delegate all interrupts and exceptions to supervisor mode. w_medeleg(0xffff); w_mideleg(0xffff); w_sie(r_sie() | SIE_SEIE | SIE_STIE | SIE_SSIE); // ask for clock interrupts. timerinit(); // keep each CPU's hartid in its tp register, for cpuid(). int id = r_mhartid(); w_tp(id); // switch to supervisor mode and jump to main(). asm volatile("mret");&#125; start函数执行了一些只允许在machine mode模式下的配置，然后转入supervisor mode，要进入supervisor mode，RISC-V 提供了指令 mret。它通过将main的地址写入寄存器mepc将返回地址设置为main，随后程序计数器指向main函数（kernel/main.c） 12345678910111213141516171819202122232425262728293031323334353637// start() jumps here in supervisor mode on all CPUs.voidmain()&#123; if(cpuid() == 0)&#123; consoleinit(); printfinit(); printf("\n"); printf("xv6 kernel is booting\n"); printf("\n"); kinit(); // physical page allocator kvminit(); // create kernel page table kvminithart(); // turn on paging procinit(); // process table trapinit(); // trap vectors trapinithart(); // install kernel trap vector plicinit(); // set up interrupt controller plicinithart(); // ask PLIC for device interrupts binit(); // buffer cache iinit(); // inode cache fileinit(); // file table virtio_disk_init(); // emulated hard disk userinit(); // first user process __sync_synchronize(); started = 1; &#125; else &#123; while(started == 0) ; __sync_synchronize(); printf("hart %d starting\n", cpuid()); kvminithart(); // turn on paging trapinithart(); // install kernel trap vector plicinithart(); // ask PLIC for device interrupts &#125; scheduler(); &#125; main函数初始化一些设备和子系统后，通过userinit()函数创建第一个进程，该进程执行了一段用户态汇编程序initcode.S，如下所示。使用exec系统调用将/user/init.c程序替换执行，Init()随后创建标准输入输出设备文件，启动一个shell程序，系统启动完成。 123456789101112131415161718192021222324252627282930# initcode.S# Initial process that execs /init.# This code runs in user space.#include &quot;syscall.h&quot;# exec(init, argv).globl startstart: la a0, init la a1, argv li a7, SYS_exec ecall# for(;;) exit();exit: li a7, SYS_exit ecall jal exit# char init[] = &quot;/init\0&quot;;init: .string &quot;/init\0&quot;# char *argv[] = &#123; init, 0 &#125;;.p2align 2argv: .long init .long 0 3.2 系统调用userinit()函数中调用的汇编代码initcode.S，所执行的exec是系统的执行的第一个系统调用。这个汇编程序中，它首先将init中的地址加载到a0（la a0, init），argv中的地址加载到a1（la a1, argv），exec系统调用对应的数字加载到a7（li a7, SYS_exec），最后调用ECALL。所以这里执行了3条指令，之后在第4条指令将控制权交给了操作系统。 userinit会创建初始进程，返回到用户空间，执行刚刚介绍的3条指令，再回到内核空间。查看系统调用处理函数syscall.c的代码。 12345678910111213141516171819202122voidsyscall(void)&#123; int num; struct proc *p = myproc(); num = p-&gt;trapframe-&gt;a7; if(num &gt; 0 &amp;&amp; num &lt; NELEM(syscalls) &amp;&amp; syscalls[num]) &#123; p-&gt;trapframe-&gt;a0 = syscalls[num](); // if (p-&gt;tracemask &amp; (1 &lt;&lt; num)) &#123; printf("%d: syscall %s -&gt; %d\n",p-&gt;pid, syscalls_name[num], p-&gt;trapframe-&gt;a0); &#125; &#125; else &#123; printf("%d %s: unknown sys call %d\n", p-&gt;pid, p-&gt;name, num); p-&gt;trapframe-&gt;a0 = -1; &#125;&#125; num = p-&gt;trapframe-&gt;a7 会读取使用的系统调用对应的系统调用号。如果我们查看syscall.h，可以看到7对应的是exec系统调用。 123456789// System call numbers#define SYS_fork 1#define SYS_exit 2#define SYS_wait 3#define SYS_pipe 4#define SYS_read 5#define SYS_kill 6#define SYS_exec 7#define SYS_fstat 8 因此，这里实现的功能是告诉内核，某个用户应用程序执行了ECALL指令，并且想要调用exec系统调用。p-&gt;trapframe-&gt;a0 = syscallnum 这一行是实际执行系统调用。这里可以看出，num用来索引一个数组，这个数组是一个函数指针数组，可以想象的是syscalls[7]对应了exec的入口函数。 1234567891011static uint64 (*syscalls[])(void) = &#123;[SYS_fork] sys_fork,[SYS_exit] sys_exit,[SYS_wait] sys_wait,[SYS_pipe] sys_pipe,[SYS_read] sys_read,[SYS_kill] sys_kill,[SYS_exec] sys_exec,[SYS_fstat] sys_fstat,&#125;; sys_exec中的第一件事情是从用户空间读取参数，它会读取path，也就是要执行程序的文件名。然后从用户空间将参数拷贝到内核空间。initcode.S完成了通过exec调用init程序。init会为用户空间设置好一些东西，比如配置好console，调用fork，并在fork出的子进程中执行shell。 12345if(pid == 0)&#123; exec("sh", argv); printf("init: exec sh failed\n"); exit(1); &#125; 4. 添加系统调用sys_trace添加一个系统调用应该包含以下步骤： 在syscall.h中添加系统调用号 在系统调用入口函数sys_call.c增加sys_trace系统调用 添加一个entry到user/usys.pl。perl语言自动生成汇编语言usys.S，是用户态系统调用接口，首先把系统调用号压入a7寄存器，然后就直接ecall进入系统内核。而上文syscall函数就把a7寄存器的数字读出来调用对应的函数，所以这里就是系统调用用户态和内核态的切换接口。 123456#usys.S.global tracetrace: li a7, SYS_trace ecall ret 添加声明到user/user.h，让程序在编译的时候可以通过。 添加系统调用号如下： /kern/syscall.h 1234567891011121314151617181920212223// System call numbers#define SYS_fork 1#define SYS_exit 2#define SYS_wait 3#define SYS_pipe 4#define SYS_read 5#define SYS_kill 6#define SYS_exec 7#define SYS_fstat 8#define SYS_chdir 9#define SYS_dup 10#define SYS_getpid 11#define SYS_sbrk 12#define SYS_sleep 13#define SYS_uptime 14#define SYS_open 15#define SYS_write 16#define SYS_mknod 17#define SYS_unlink 18#define SYS_link 19#define SYS_mkdir 20#define SYS_close 21#define SYS_trace 22 syscall.c增加sys_trace系统调用 /kern/syscall.c 1234567891011121314151617181920212223242526// 添加sys_trace声明...extern uint64 sys_unlink(void);extern uint64 sys_wait(void);extern uint64 sys_write(void);extern uint64 sys_uptime(void);extern uint64 sys_trace(void);// 添加到syscalls数组中...static uint64 (*syscalls[])(void) = &#123;[SYS_fork] sys_fork,[SYS_exit] sys_exit,[SYS_wait] sys_wait,[SYS_pipe] sys_pipe,[SYS_read] sys_read,[SYS_kill] sys_kill,...[SYS_close] sys_close,[SYS_trace] sys_trace,&#125;;// 添加识别名,因为实验最终的实现效果需要打印系统调用名称，因此这里用一个数组存储系统调用的名称...char* syscalls_name[24] = &#123;"", "fork", "exit", "wait", "pipe", "read", "kill", "exec", "fstat", "chdir", "dup", "getpid", "sbrk", "sleep", "uptime", "open", "write", "mknod", "unlink", "link", "mkdir", "close", "trace"&#125;; 5. 实现跟踪打印系统调用 在sysproc.c中实现该系统调用处理函数 proc.h中为struct proc{}新增一个tracemask变量，用来保存当前进程要跟踪的系统调用号。 proc.c下的fork函数中实现子进程复制父进程tracemask功能。 syscall()系统调用入口分发函数，实现系统调用的跟踪。 添加$U/_trace到Makefile中的UPROGS变量里 实现系统调用处理函数 我们需要将用户态trace传入的待跟踪系统调用号绑定到当前进程的proc中，因此需要在struck proc中新增一个tracemask整型变量来保存用户态传入的系统调用号。 /kern/proc.h 123456789101112131415161718192021222324// Per-process statestruct proc &#123; struct spinlock lock; // p-&gt;lock must be held when using these: enum procstate state; // Process state struct proc *parent; // Parent process void *chan; // If non-zero, sleeping on chan int killed; // If non-zero, have been killed int xstate; // Exit status to be returned to parent's wait int pid; // Process ID // these are private to the process, so p-&gt;lock need not be held. uint64 kstack; // Virtual address of kernel stack uint64 sz; // Size of process memory (bytes) pagetable_t pagetable; // User page table struct trapframe *trapframe; // data page for trampoline.S struct context context; // swtch() here to run process struct file *ofile[NOFILE]; // Open files struct inode *cwd; // Current directory char name[16]; // Process name (debugging) int tracemask;&#125;; 通过函数argint()从a0寄存器中获取用户态传递的参数赋值给当前进程的tracemask，该参数是要跟踪的系统调用号。/kern/sysproc.c 12345678910uint64sys_trace(void)&#123; int n; if (argint(0, &amp;n) &lt; 0) return -1; myproc()-&gt;tracemask = n; return 0;&#125; 修改kernel/proc.c中的fork函数，添加子进程复制父进程mask的功能 /kern/proc.c 123456789101112131415161718192021intfork(void)&#123; int i, pid; struct proc *np; struct proc *p = myproc(); ... safestrcpy(np-&gt;name, p-&gt;name, sizeof(p-&gt;name)); // copy mask np-&gt;tracemask = p-&gt;tracemask; pid = np-&gt;pid; np-&gt;state = RUNNABLE; release(&amp;np-&gt;lock); return pid;&#125; syscall()函数中实现系统调用的跟踪 123456789101112131415161718192021voidsyscall(void)&#123; int num; struct proc *p = myproc(); num = p-&gt;trapframe-&gt;a7; if(num &gt; 0 &amp;&amp; num &lt; NELEM(syscalls) &amp;&amp; syscalls[num]) &#123; p-&gt;trapframe-&gt;a0 = syscalls[num](); //tracemask的32位代表32个系统调用号，满足第num位是1就会满足if条件，打印该系统调用 if (p-&gt;tracemask &amp; (1 &lt;&lt; num)) &#123; printf("%d: syscall %s -&gt; %d\n",p-&gt;pid, syscalls_name[num], p-&gt;trapframe-&gt;a0); &#125; &#125; else &#123; printf("%d %s: unknown sys call %d\n", p-&gt;pid, p-&gt;name, num); p-&gt;trapframe-&gt;a0 = -1; &#125;&#125; 添加$U/_trace到Makefile中的UPROGS变量里，运行结果如下]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux RCU机制]]></title>
    <url>%2FLinux%2FRCU%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[1. 简介RCU (Read-copy update)是2002年10月添加到Linux内核中的一种同步机制。作为数据同步的一种方式，在当前的Linux内核中发挥着重要的作用。 RCU主要针对的数据对象是链表，目的是提高遍历读取数据的效率，为了达到目的使用RCU机制读取数据的时候不对链表进行耗时的加锁操作。这样在同一时间可以有多个线程同时读取该链表，并且允许一个线程对链表进行修改（修改的时候，需要加锁）。RCU适用于需要频繁的读取数据，而相应修改数据并不多的情景，例如在文件系统中，经常需要查找定位目录，而对目录的修改相对来说并不多，这就是RCU发挥作用的最佳场景。 2. 实现功能RCU的实现主要解决了以下问题： 保证读取链表的完整性。新增或者删除一个节点，不至于导致遍历一个链表从中间断开。但是RCU并不保证一定能读到新增的节点或者不读到要被删除的节点。 在读取过程中，另外一个线程删除了一个节点。删除线程可以把这个节点从链表中移除，但它不能直接销毁这个节点，必须等到所有的读取线程读取完成以后，才进行销毁操作。RCU中把这个过程称为宽限期（Grace period）。 在读取过程中，另外一个线程插入了一个新节点，而读线程读到了这个节点，那么需要保证读到的这个节点是完整的。这里涉及到了发布-订阅机制（Publish-Subscribe Mechanism）。 2.1 数据完整性2.1.1 插入数据时保证读取数据的完整性如下图，在链表中加入一个节点new到A节点之前，所要做的第一步是将new的指针指向A节点，第二步才是将Head的指针指向new。这样做的目的是当插入操作完成第一步的时候，对于链表的读取并不产生影响，而执行完第二步的时候，读线程如果读到new节点，也可以继续遍历链表。如果把这个过程反过来，第一步head指向new，而这时一个线程读到new，由于new的指针指向的是Null，这样将导致读线程无法读取到A，B等后续节点。因此RCU并不能保证在插入数据时读线程一定能够读到新数据。 2.1.2 删除数据时保证读取数据的完整性删除节点A时，首先将Head的指针指向B，保持A的指针，然后删除程序将进入宽限期检测。由于A的内容并没有变更，读到A的线程仍然可以继续读取A的后续节点。A不能立即销毁，它必须等待宽限期结束后，才能进行相应销毁操作。由于Head的指针已经指向了B，当宽限期开始之后所有的后续读操作通过Head找到B，而A已经隐藏了，后续的读线程都不会读到它。这样就确保宽限期过后，删除A并不对系统造成影响。 2.1.3 更新数据时保证读取数据的完整性初始链表如下图所示，指针p指向的节点（5，6，7）是需要更新的节点，使用rcu更新节点代码如下： 12345671 q = kmalloc(sizeof(*p), GFP_KERNEL);2 *q = *p;3 q-&gt;b = 2;4 q-&gt;c = 3;5 list_replace_rcu(&amp;p-&gt;list, &amp;q-&gt;list);6 synchronize_rcu();7 kfree(p); 为了简化操作使用了单向链表，节点的红色外框表示该节点有被引用，节点中的值表示变量a,b,c的值。 代码第一行申请内存并创建一个q节点。 第2行将待更新节点p的数据复制到新节点q中。 第3，4行更新节点q中b和c变量的值，将5，6，7改成5，2，3。修改完成之后，写线程就可以将这个更新“发布”了（publish），对于读线程来说就“可见”了。 第5行进行替换，这样新节点q对读线程最终是可见的。如下所示，现在有了链表的两个路径。先前的读线程可能会看到5、6、7元素，但新的读线程会看到5、2、3元素。但是任何给定的读线程都保证读到完整的链表，而不是某个中间状态。 第6行synchronize_rcu()函数返回之后，一个grace period（宽限期）已经过去了，所以所有在list_replace_rcu()函数之前开始的读操作都已经完成了。任何读取5、6、7元素的读线程都被保证已经退出了它们的RCU读侧临界区，因此被禁止继续持有节点p的引用。，如下面的5、6、7元素周围的黑色细边框所示。就读线程而言，又回到了单一路径的链表同时更新了节点的数据。 kfree（）在第7行完成后，列表将显示如下： 2.2 发布订阅机制RCU的一个最关键的特性在于，它能够保证数据能安全的被多个线程同时读取，即便数据在同时更新。例如有全局指针gp，指向一段新的已分配内存并进行初始化。 123456789101112131415struct foo &#123; int a; int b; int c;&#125;;struct foo *gp = NULL;/* . . . */p = kmalloc(sizeof(*p), GFP_KERNEL);p-&gt;a = 1; // 1p-&gt;b = 2; // 2p-&gt;c = 3; // 3gp = p; // 4 由于编译器对代码进行优化，对于多CPU的机器来说，经常可能gp = p这个操作会发生在1,2或者3步之前，也就是说p还没被初始化完全就被赋值给了gp。rcu提供了一个具有发布含义的封装函数rcu_assign_ pointer() ,其封装了内存屏障功能，使用如下方式赋值。 12345p-&gt;a = 1; // 1p-&gt;b = 2; // 2p-&gt;c = 3; // 3rcu_assign_pointer(gp, p); 这个函数能够发布（创建）一个新的结构体，保证从编译器和CPU层面上gp被赋值前，p指向的字段能够赋值完成。我们看看这个函数的具体实现(Linux kernel 4.11.4)： 1234567891011#define rcu_assign_pointer(p, v) \(&#123; \ uintptr_t _r_a_p__v = (uintptr_t)(v); \ \ if (__builtin_constant_p(v) &amp;&amp; (_r_a_p__v) == (uintptr_t)NULL) \ WRITE_ONCE((p), (typeof(p))(_r_a_p__v)); \ else \ smp_store_release(&amp;p, RCU_INITIALIZER((typeof(p))_r_a_p__v)); \ _r_a_p__v; \&#125;) 该段代码做了两件事： 在必要时插入一个内存屏障； 关闭编译器在赋值时的非顺序编译优化，保证赋值时已经初始化了。 保证赋值顺序执行后，还需要保证读的顺序性。有如下代码： 12345p = gp;if (p != NULL) &#123; do_something_with(p-&gt;a, p-&gt;b, p-&gt;c);&#125; 以上代码在一般的处理器架构没有问题，但在 DEC Alpha CPU机器上，编译器的 value-speculation 优化选项据说可能会“猜测” p1 的值，然后重排指令，fp-&gt;a，fp-&gt;b，fp-&gt;c会在p = gp还没执行的时候就预先判断运行，可能导致传入dosomething 的一部分属于旧的gbl_ foo，而另外的属于新的。这样导致运行结果的错误。为了避免该类问题，RCU提供了原生接口rcu_dereference()来解决这个问题， rcu_dereference() 的实现，最终效果就是把一个受RCU保护的指针赋值给另一个，代码如下： 1234567rcu_read_lock();p = rcu_dereference(gp);if (p != NULL) &#123; do_something_with(p-&gt;a, p-&gt;b, p-&gt;c);&#125;rcu_read_unlock(); 综上，rcu_assign_pointer是发布，而rcu_dereference是订阅。RCU还提供了一些更高级的API接口，如下： 2.3 RCU宽限期在RCU中，数据的删除和销毁需要一定的宽限期，主要是因为需要等待读线程的完成。如图所示： 有如下代码，两个线程同时运行 foo_ read和foo_update的时候，当foo_ read执行完赋值操作后，线程发生切换；此时另一个线程开始执行foo_update并执行完成。当foo_ read运行的进程切换回来后，运行dosomething 的时候，fp已经被删除，这将产生严重错误。 1234567891011121314151617181920212223242526struct foo &#123; int a; char b; long c; &#125;; DEFINE_SPINLOCK(foo_mutex); struct foo *gbl_foo; void foo_read (void)&#123; foo *fp = gbl_foo; // 如果发生进程切换 if ( fp != NULL ) dosomething(fp-&gt;a, fp-&gt;b , fp-&gt;c );&#125; void foo_update( foo* new_fp )&#123; spin_lock(&amp;foo_mutex); foo *old_fp = gbl_foo; gbl_foo = new_fp; spin_unlock(&amp;foo_mutex); kfee(old_fp);&#125; 所以，写线程（删除和销毁数据的线程）在删除数据后不能立马销毁这个数据，一定要等待所有在宽限期开始前已经开始的读线程结束，才可以进行销毁操作。这样做的原因是这些线程有可能读到了要删除的元素。图中宽限期左侧有三个reader在宽限期开始前已经开始了读取，必须等待他们结束，而最左侧的reader在开始宽限期之前就已经结束了读取，不需要考虑，其余在宽限期开始后才开始读取的reader不可能读取到旧的节点数据，因此也不需要考虑。 因此，RCU提供了一个接口函数synchronize_rcu()来同步在宽限期的读线程。只有宽限期中没有读线程了，这个函数才返回，也就是说这是一个阻塞函数。所以foo_update需要写成下面的形式才是安全的。 123456789void foo_update( foo* new_fp )&#123; spin_lock(&amp;foo_mutex); foo *old_fp = gbl_foo; gbl_foo = new_fp; spin_unlock(&amp;foo_mutex); synchronize_rcu(); kfee(old_fp);&#125; 3. 总结RCU的核心API如下： 1234567rcu_read_lock()rcu_read_unlock()synchronize_rcu()rcu_assign_pointer()rcu_dereference() 其中，rcu_read_lock()和rcu_read_unlock()用来保持一个读者的RCU临界区.在该临界区内不允许发生上下文切换，内核要根据“是否发生过切换”来判断读者是否已结束读操作。而下列的函数用于实现内存屏障的作用。 rcu_dereference()：读者调用它来获得一个被RCU保护的指针。rcu_assign_pointer()：写者使用该函数来为被RCU保护的指针分配一个新的值。 synchronize_rcu()：这是RCU的核心所在，它挂起写线程，等待读者都退出后释放老的数据。 参考链接：https://lwn.net/Articles/262464/https://www.cnblogs.com/schips/p/linux_cru.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[获取和设置内核参数-sysctl]]></title>
    <url>%2Funcategorized%2F%E8%8E%B7%E5%8F%96%E5%92%8C%E8%AE%BE%E7%BD%AE%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0-sysctl%2F</url>
    <content type="text"><![CDATA[1.前言sysctl是一种用户应用来设置和获得运行时内核的配置参数的一种有效方式，通过这种 方式，用户应用可以在内核运行的任何时刻来改变内核的配置参数，也可以在任何时候获得内核的配置参数，通常，内核的这些配置参数也出现在proc文件系统 的/proc/sys目录下，用户应用可以直接通过这个目录下的文件来实现内核配置的读写操作。 例如，用户可以通过使用sysctl -w修改内核参数，没有选项表示读内核配置参数，用户可以使用 sysctl -a 来读取所有的内核配置参数。 sysctl -w vm.block_dump = 1 设置在dmesg信息中打印进程写入块的数量。参数 vm.block_dump 实际被转换到对应的 proc 文件/proc/sys/vm/block_dump，等同于 echo 1 &gt; /proc/sys/vm/block_dump 2. 内核模块中使用sysctl内核模块在文件 sysctl-exam-kern.c 中实现，在该内核模块中，每一个 sysctl 条目对应一个 struct ctl_table 结构。Sysctl 条目也可以是目录，此时 mode 字段应当设置为 0555，否则通过 sysctl 系统调用将无法访问它下面的 sysctl 条目，child 则指向该目录条目下面的所有条目，对于在同一目录下的多个条目，不必一一注册，用户可以把它们组织成一个 struct ctl_table 类型的数组，然后一次注册就可以。 123456789101112131415// /include/linux/sysctl.hstruct ctl_table &#123; int ctl_name; /* Binary ID */ const char *procname; /* Text ID for /proc/sys, or zero 表示在proc/sys/下显示的文件名称 */ void *data;//表示对应于内核中的变量名称 int maxlen;// 表示条目允许的最大长度 mode_t mode;//条目在proc文件系统下的访问权限 struct ctl_table *child;//子条目 struct ctl_table *parent; /* Automatically set 父级条目*/ proc_handler *proc_handler; /* Callback for text formatting 回调函数，对于整型内核变量，应当设置为&amp;proc_dointvec，而对于字符串内核变量，则设置为 &amp;proc_dostring*/ ctl_handler *strategy; /* Callback function for all r/w */ void *extra1; void *extra2;&#125;; 2.1 注册register_sysctl_table注册sysctl条目使用函数register_sysctl_table，函数原型如下： struct ctl_table_header *register_sysctl_table(struct ctl_table *table) 参数为定义的struct ctl_table结构的sysctl条目或条目数组指针； 2.2 卸载unregister_sysctl_table当模块卸载时，需要使用函数unregister_sysctl_table，其原型： void unregister_sysctl_table(struct ctl_table_header * header) 其中struct ctl_table_header是通过函数register_sysctl_table注册时返回的结构体指针。 2.3 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364//sysctl-exam-kern.c#include &lt;linux/module.h&gt;#include &lt;linux/init.h&gt;#include &lt;linux/kernel.h&gt;#include &lt;linux/sysctl.h&gt;static int sysctl_kerexam_data = 1024;//处理函数static int kerexam_callback(ctl_table *table, int write, void __user *buffer, size_t *lenp, loff_t *ppos)&#123; int rc; int *data = table-&gt;data; printk(KERN_INFO "before value = %d\n", *data); rc = proc_dointvec(table, write, buffer, lenp, ppos); if (write) printk(KERN_INFO "write operation, current value = %d\n", *data); else printk(KERN_INFO "read operation, current value = %d\n", *data); return rc;&#125;static struct ctl_table kerexam_ctl_table[] = &#123; &#123; .procname = "kerexam", .data = &amp;sysctl_kerexam_data, .maxlen = sizeof(int), .mode = 0644, .proc_handler = kerexam_callback, &#125;, &#123; /* sub parameter */ &#125;,&#125;;static struct ctl_table_header *sysctl_header;static int __init sysctl_example_init(void)&#123; sysctl_header = register_sysctl_table(kerexam_ctl_table); if (sysctl_header == NULL) &#123; printk(KERN_INFO "ERR: register_sysctl_table!"); return -1; &#125; printk(KERN_INFO "sysctl register success.\n"); return 0;&#125;static void __exit sysctl_example_exit(void)&#123; unregister_sysctl_table(sysctl_header); printk(KERN_INFO "sysctl unregister success.\n");&#125;module_init(sysctl_example_init);module_exit(sysctl_example_exit);MODULE_LICENSE("GPL"); Makefile 12345678910111213141516171819BASEINCLUDE ?= /usr/src/2.6.32-21-genericoops-objs := sysctl-exam-kern.o KBUILD_CFLAGS +=-g -O0obj-m := sysctl-exam-kern.oall : $(MAKE) -C $(BASEINCLUDE) SUBDIRS=$(shell pwd) modules;install: $(MAKE) -C $(BASEINCLUDE) SUBDIRS=$(shell pwd) modules_install; clean: $(MAKE) -C $(BASEINCLUDE) SUBDIRS=$(shell pwd) clean; rm -f *.ko; 2.4 运行模块程序运行的内核版本是2.6.32-21-generic，要能够加载内核模块，需要开启系统的可加载模块支持Enable loadable module support，在内核源码根目录执行 make menuconfig 进行配置。 在insmod模块的时候遇到如下错误，no symbol version for module_layout。 原因是/usr/src/linux-source-[version]/目录下缺少文件Module.symvers，查看 /usr/src/linux-headers-[version]-generic，发现里面有 Module.symvers，将其直接cp到/usr/src/linux-source-[version]/，之后重新make并且insmod sysctl-exam-kern.ko成功。 使用函数register_sysctl_table注册完成后，可以在/proc/sys/目录下看到用户定义的参数条目kerexam。 使用sysctl kerexam查看kerexam条目的默认值。 使用sysctl -w kerexam=2048 设置kerexam条目的值。查看dmesg打印日志如下。 3.内核函数分析使用Ftrace跟踪sysctl kerexam命令，函数如下，vfs_read函数调用了proc文件系统的proc_sys_read，调用proc_sys_call_handler函数，该函数通过grab_header函数获取struct ctl_table_header，PROC_I(inode)-&gt;sysctl_entry;获取struct ctl_table，通过table-&gt;proc_handler调用自定义的处理函数kerexam_callback。 123456789101112131415161718192021220) | sys_read() &#123;0) 0.000 us | fget_light();0) | vfs_read() &#123;0) | rw_verify_area() &#123;0) | security_file_permission() &#123;0) 0.000 us | apparmor_file_permission();0) 0.000 us | &#125;0) 0.000 us | &#125;0) | proc_sys_read() &#123;0) | proc_sys_call_handler() &#123;0) | grab_header() &#123;0) | sysctl_head_grab() &#123;0) 0.000 us | _spin_lock();0) 0.000 us | &#125;0) 0.000 us | &#125;0) | sysctl_perm() &#123;0) | security_sysctl() &#123;0) 0.000 us | apparmor_sysctl();0) 0.000 us | &#125;0) 0.000 us | &#125;0) | kerexam_callback() &#123; 参数含义1. nr_requests/sys/block/sda/queue IO调度队列大小，254 2. queue_depth/sys/block/sda/device磁盘队列深度，32 3. read_ahead_kb这个块设备上的文件系统预读的最大kb数。 128 4. max_sectors_kbBlock 层将允许文件系统请求的最大 kb 数。此值为读写。此值必须小于或等于max_hw_sectors_kb值。1280 5. max_hw_sectors_kb单个数据传输中支持的最大 kb 数。4096 6. max_segments设备的最大段数。128 7. max_discard_segments1 8. max_integrity_segments0 当读取时，该文件显示由硬件控制器可以处理的块层设置的完整性段的最大限制。 9. max_segment_size65536设备的最大段大小。 10. schedulermq-deadline当读取时，该文件将显示此块设备的当前和可用的IO调度程序。当前活动的IO调度器将被括在[]括号中。将IO调度器名称写入此文件将把此块设备的控制权切换到新的IO调度器。请注意，如果IO调度器模块还没有出现在系统中，那么向该文件写入IO调度器名称将尝试加载该IO调度器模块。 11. hw_sector_size这是设备的硬件扇区大小，单位是字节。512 12. logical_block_size这是设备的逻辑块大小，以字节为单位。512 13. physical_block_size512这是设备的物理块大小，以字节为单位。 14. chunk_sectors0 根据块设备的类型，这具有不同的含义。对于 RAID 设备（dm-raid），chunk_sectors 表示以 512B 扇区为单位的大小RAID 卷条带段。 对于分区块设备，无论是主机感知或主机管理，chunk_sectors 表示区域的 512B 扇区大小设备的最后一个区域除外可能更小。 15. minimum_io_size512这是设备报告的最小优选IO大小。 16. optimal_io_size0 这是设备报告的最佳IO大小。 17. discard_granularity0 如果该设备报告了内部分配的大小(以字节为单位)。值为“0”表示设备不支持丢弃功能。 18. discard_max_hw_bytes0 Devices that support discard functionality may have internal limits onthe number of bytes that can be trimmed or unmapped in a single operation.The discard_max_bytes parameter is set by the device driver to the maximumnumber of bytes that can be discarded in a single operation. Discardrequests issued to the device must not exceed this limit. A discard_max_bytesvalue of 0 means that the device does not support discard functionality. 支持丢弃功能的设备可能对单个操作中可删除或未映射的字节数有内部限制。discard max bytes参数由设备驱动程序设置为单个操作中可以丢弃的最大字节数。发送给设备的丢弃请求不能超过这个限制。丢弃最大字节值为0表示设备不支持丢弃功能。 19. discard_max_bytes0 当丢弃最大hw字节是设备的硬件限制时，这个设置是软件限制。有些设备在发出大的丢弃时显示出较大的延迟，将此值设置得更低将使Linux发出更小的丢弃，并可能有助于减少由大的丢弃操作引起的延迟。 20. discard_zeroes_data0 21. write_same_max_bytes0 这是设备可以在单个写入中写入的字节数-相同命令。 值 ‘0’ 表示不支持 write-same设备。 22. write_zeroes_max_bytes0 23. rotational1 该文件用于统计设备是旋转类型还是非旋转类型。 24. zonednone这表明该设备是否是分区块设备，并且该设备的区域模型设备，如果它确实被分区。 zoned 表示的可能值是常规块设备为“无”，分区为“主机感知”或“主机管理”块设备。 主机感知和主机管理的分区块的特征ZBC（分区块命令）和 ZAC 中描述了设备（分区设备 ATA 命令集）标准。 这些标准还定义了“驱动器管理”区域模型。 但是，由于驱动器管理的分区块设备不支持 zone 命令，它们将被视为常规块设备和 zoned 将报告“无”。 25. nomerges0 这使用户能够禁用与 IO 相关的查找逻辑在块层合并请求。 默认情况下 (0) 所有合并都是启用。 当设置为 1 时，只会尝试简单的一击合并。 什么时候设置为 2 不会尝试合并算法（包括一击或多次复杂的树/哈希查找）。 26. rq_affinity1 27. iostats1 此文件用于控制（开/关）iostats 统计功能。 28. add_random1 这个参数允许关闭磁盘熵贡献。这个文件的默认值是’1’(开启)。 29. io_poll0 读取时，该文件显示轮询是启用(1)还是禁用(0)。向该文件写入’0’将禁用该设备的轮询。写入任何非零值都将启用此特性。 30. io_poll_delay-1如果启用轮询，这将控制哪种轮询执行。 它默认为 -1，这是经典轮询。 在这种模式下，CPU 会反复要求完成而不放弃任何时间。如果设置为 0，则使用混合轮询模式，内核将尝试对 IO 何时完成进行有根据的猜测。 基于此猜测，内核会让发出 IO 的进程休眠一段时间在进入经典轮询循环之前的时间。 这种模式可能是比纯经典轮询慢一点，但会更有效率。如果设置为大于 0 的值，内核将把进程发出IO 在进入经典之前休眠此微秒数轮询。 31. write_cachewrite through当读取时，该文件将显示设备是否启用了回写缓存。对于前一种情况，它将返回“回写”，对于后一种情况，它将返回“透写”。写入此文件可以改变设备的内核视图，但不会改变设备状态。这意味着将设置从“回写”切换为“透写”可能不安全，因为这也会消除内核发出的缓存刷新。 32. dax0 该文件表明设备是否支持直接访问(DAX)，由cpu可寻址存储使用，以绕过页面缓存。如果为真，则显示’1’，否则显示’0’。 33. wbt_lat_usec75000如果设备已注册回写限制，则此文件显示目标最小读取延迟。 如果在给定的时间内超过此延迟时间窗口（参见 wb_window_usec），然后写回节流将开始缩减写入。 向该文件写入值 ‘0’ 将禁用特征。 将值“-1”写入此文件会将值重置为默认设置。 相关代码： 读取或者设置sys目录下参数: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//sysfs.gotype Sysfs struct &#123; Option string&#125;func NewSysfs() *Sysfs &#123; return &amp;Sysfs&#123; Option: "/sys", &#125;&#125;func (this *Sysfs)Get(key string) string &#123; data,err :=ioutil.ReadFile(utils.Format("%s/%s",this.Option,key)) if err != nil &#123; fmt.Println("文件打开失败。") return "-1" &#125; pattern := regexp.MustCompile(".*\\[(.*)\\].*") searchObj := pattern.FindStringSubmatch(string(data)) if searchObj != nil &#123; return searchObj[1] &#125; return string(data)&#125;func (this *Sysfs)Set(key string,value interface&#123;&#125;) &#123; var format string switch value.(type)&#123; case string: format = "%s" break case int: format = "%d" break default: fmt.Println("pramater is vaild!") return &#125; fp,e := os.OpenFile(utils.Format("%s/%s",this.Option,key), os.O_RDWR|os.O_TRUNC, 0666) //打开文件 if e != nil &#123; fmt.Println("open file error!", e) &#125; _, err := io.WriteString(fp, fmt.Sprintf(format, value)) if err != nil &#123; fmt.Println("set parm error!", err) &#125;&#125; 读取采样后的参数空间： 12345678910111213141516171819202122232425//collect.gofunc ReadCsv() &#123; fileName := "./src/data/samples.csv" fs, err := os.Open(fileName) if err != nil &#123; log.Fatalf("can not open the file, err is %+v", err) &#125; defer fs.Close() r := csv.NewReader(fs) for &#123; row, err := r.Read() if err != nil &amp;&amp; err != io.EOF &#123; log.Fatalf("can not read, err is %+v", err) &#125; if err == io.EOF &#123; break &#125; fmt.Println(row) &#125;&#125; 从数据库查询参数的配置路径： 123456789101112131415161718192021222324252627282930313233343536//arg_dao.gotype Arg struct &#123; id int64 name string value string valType int8 path string&#125;func (this * Arg)toString() string&#123; return "id:"+string(this.id)+",name:"+this.name+",value:"+this.value+",valType:"+string(this.valType)+",path:"+this.path&#125;//查询操作func Query() &#123; var arg Arg rows, e := db.DB.Query("select id,name,value,val_type,path from args") if e == nil &#123; errors.New("query incur error") &#125; for rows.Next() &#123; e := rows.Scan(&amp;arg.id,&amp;arg.name, &amp;arg.value,&amp;arg.valType, &amp;arg.path ) if e != nil &#123; fmt.Println(arg.toString()) &#125; &#125; rows.Close() //db.DB.QueryRow("select * from arg where id=1").Scan(arg.age, arg.id, arg.name, arg.phone, arg.sex) // //stmt, e := db.DB.Prepare("select * from arg where id=?") //query, e := stmt.Query(1) //query.Scan()&#125; 参数数据表设计： 123456789DROP TABLE IF EXISTS `args`;CREATE TABLE `args` ( `id` int(0) NOT NULL AUTO_INCREMENT, `name` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL, `value` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL COMMENT '默认值', `val_type` int(0) NULL DEFAULT NULL COMMENT '离散取值（0）或者连续取值（1）', `path` varchar(255) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL COMMENT '设置参数的文件系统路径', PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;]]></content>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[1.前言决策树就是一棵树，一颗决策树包含一个根节点、若干个内部结点和若干个叶结点；叶结点对应于决策结果，其他每个结点则对应于一个属性测试；每个结点包含的样本集合根据属性测试的结果被划分到子结点中；根结点包含样本全集，从根结点到每个叶子结点的路径对应了一个判定测试序列。如下图所示，根据水果的属性（颜色、形状、直径）来判断水果的种类。 决策树可以同时接受分类数据，如颜色、形状，和数值数据，如直径，作为输入。决策树对于只包含几种可能结果的问题而言，处理起来非常高效，但是面对拥有大量可能结果的数据集时，算法效率就好降低。 2.决策树的建立决策树的训练算法有三种CART、ID3和C4.5。CART必须是二叉树，而ID3和C4.5，不一定是二叉树。决策树学习的关键在于如何选择最优的划分属性，所谓的最优划分属性。最优特征选择方法分为基尼不纯度、信息增益、信息增益率三种。在ID3算法中我们使用了信息增益来选择特征，信息增益大的优先选择。在C4.5算法中，采用了信息增益比来选择特征，以减少信息增益容易选择特征值多的特征的问题。但是无论是ID3还是C4.5,都是基于信息论的熵模型的，这里面会涉及大量的对数运算。CART分类树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益(比)是相反的。 2.1 CART训练算法Scikit-Learn使用的是分类与回归树（Classification And Regression Tree，简称CART）算法来训练决策树（也叫作“生长”树）。想法非常简单：首先，使用单个特征k和阈值tk（例如，花瓣长度≤2.45厘米）将训练集分成两个子集。k和阈值tk怎么选择？答案是产生出最纯子集（受其大小加权）的k和tk。算法尝试最小化的成本函数为。 左右子集的不纯度就可以采用基尼不纯度或者信息增益。 默认使用的是基尼不纯度来进行测量，但是，可以将参数criterion设置为”entropy”来选择信息熵作为不纯度的测量方式。熵的概念源于热力学，是一种分子混乱程度的度量：如果分子保持静止和良序，则熵接近于零。后来这个概念传播到各个领域，其中包括香农的信息理论，它衡量的是一条信息的平均信息内容：如果所有的信息都相同，则熵为零。在机器学习中，它也经常被用作一种不纯度的测量方式：如果数据集中仅包含一个类别的实例，其熵为零。 数据集D的纯度可用基尼值来度量 数据集的属性有3个，分别是有房情况，婚姻状况和年收入，其中有房情况和婚姻状况是离散的取值，而年收入是连续的取值。拖欠贷款者属于分类的结果。 对于有房情况这个属性，它是离散型数据，那么按照它划分后的Gini系数计算如下。 使用sklearn对iris数据集建立决策树代码如下： 1234567891011121314151617181920212223242526from sklearn import datasetsfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.tree import export_graphvizfrom sklearn import svmimport numpy as npif __name__ == '__main__': iris = datasets.load_iris() x = iris.data[:,2:] y = iris.target tree_clf = DecisionTreeClassifier() tree_clf.fit(x,y) export_graphviz( tree_clf, out_file="iris_tree.dot", feature_names=iris.feature_names[2:], class_names=iris.target_names, rounded=True, filled=True ) 结果如图：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[多核系统上引入多队列SSD]]></title>
    <url>%2FLinux%2F%E5%A4%9A%E6%A0%B8%E7%B3%BB%E7%BB%9F%E4%B8%8A%E5%BC%95%E5%85%A5%E5%A4%9A%E9%98%9F%E5%88%97SSD%2F</url>
    <content type="text"><![CDATA[1.摘要存储设备的IO性能已从之前的数百IOPS加速到今天的数十万IOPS，并预计在未来几年内达到数千万IOPS。这一急剧演变主要归功于NAND-FLASH（闪存）器件及其数据并行设计的引入。 使用传统的机械存储设备（HDD），IO的延迟和吞吐量就受到这种旋转式存储设备的物理特性影响。通常HDD通过旋转磁盘盘片进行顺序访问速度较快，而通过移动磁头的随机访问却很慢，一代又一代的IO密集型算法和系统就是基于这两个基本特征而设计。固态硬盘（SSD）的出现正在改变这两个存储的性能特征，因为SSD的顺序IO和随机IO之间的延迟差异很小。固态硬盘的IO延迟为数十微秒，而硬盘为数十毫秒。SSD磁盘中的大量内部数据并行度实现了许多并发IO操作，从而使单个设备能够实现接近一百万次每秒IO(IOPS)的随机访问，而传统的磁性硬盘上仅有数百次IOPS。 2. IO瓶颈现代存储设备的吞吐量现在通常受到其硬件(即，SATA/SAS或PCI-E)和软件接口的限制。硬件性能的如此快速飞跃暴露了以前未被注意到的软件级别的瓶颈，包括操作系统层和应用层。如今，在Linux环境下，单CPU内核可以支持80万IOPS左右的IO提交率。无论使用多少核来提交IO，操作系统块层都不能扩展到超过一百万IOPS。这对今天的固态硬盘来说可能够快了，但对明天的固态硬盘来说就不够快了。 由于目前操作系统中存在的性能瓶颈，一些应用程序和设备驱动程序已经选择绕过Linux块层来提高性能。此选择增加了驱动程序和硬件实现的复杂性。更具体地说，它在容易出错的驱动程序实现中增加了重复代码，并删除了通用操作系统存储层提供的通用功能，如IO调度和服务质量流量整形。因此不放弃块层，又能提高存储性能，成为重要问题。 3. Linux块层传统实现操作系统块层负责将IO请求从应用程序传送到存储设备。块层是一种粘合剂，一方面允许应用程序以统一的方式访问不同的存储设备，另一方面为存储设备和驱动程序提供来自所有应用程序的单一入口点。它是一个便捷库，可以对应用程序隐藏存储设备的复杂性和多样性，同时提供对应用程序有价值的公共服务。此外，数据块层实施IO公平性、IO错误处理、IO统计和IO调度，以提高性能并帮助保护最终用户免受其他应用程序或设备驱动程序的不良或恶意实施的影响。 应用程序通过内核系统调用提交IO，将其转换为称为块IO的数据结构。每个数据块IO包含IO地址、IO大小、IO形态(读或写)或IO类型(同步/异步)2等信息。然后，将其传输到libaio(用于异步IO)或直接传输到数据块层(用于将其提交到数据块层的同步IO)。一旦IO请求被提交，相应的数据块IO就被缓冲在临时区域中，该临时区域被实现为一个队列，表示为请求队列。 Linux块层支持可插拔IO调度器：NOOP、deadline和CFQ，它们都可以在此临时区域内操作IO。块层还提供了一种处理IO完成的机制：每次设备驱动程序中的IO完成时，该驱动程序都会调用堆栈来调用块层中的通用完成函数。然后，块层调用libaio库中的IO完成函数，或者从同步读或写系统调用返回，后者向应用程序提供IO完成信号。 在当前块层中，中转区由请求队列结构表示。每个块设备实例化一个这样的队列。所有块设备的访问都是统一的，应用程序不需要知道块层内的控制流模式。然而，这种针对每个设备的单一队列设计的结果是，块层不能支持跨设备的IO调度。 传统实现块层三个主要性能开销如下： 请求队列锁定。 块层通过IO请求队列实现同步访问独占资源。无论何时向请求队列插入数据块IO或从请求队列中删除数据块IO，都必须获取此锁。 scsi设备的请求处理函数scsi_request_fn如下。 123456789101112131415161718192021/* * Function: scsi_request_fn() * * Purpose: Main strategy routine for SCSI. * * Arguments: q - Pointer to actual queue. * * Returns: Nothing * * Lock status: IO request lock assumed to be held when called. */static void scsi_request_fn(struct request_queue *q) __releases(q-&gt;queue_lock) __acquires(q-&gt;queue_lock)&#123; struct scsi_device *sdev = q-&gt;queuedata; struct Scsi_Host *shost; struct scsi_cmnd *cmd; struct request *req; ...&#125; 进入函数时已经保证request_queue已经被lock。该函数名称后面多了releases，acquires两个宏定义，在大型项目中为了保证代码质量，会加入很多防御性编程代码，这样能够即时的把问题暴露出来。例如： 1234567// 函数本身是非线程安全的，需要在外边上锁保护void dothing_unsafe()&#123; ASSERT(lock == 1); // 确保函数调用时，锁是成功加上的。 ... // 执行函数逻辑 ASSERT(lock == 0); // 最后返回时，需要确保lock已被释放，否则说明代码逻辑存在bug&#125; 例子中dothing_unsafe中典型的防御性编程的代码就是通过assert调用，保证函数在调用时lock是持有状态的。但是assert本身调用会带来除函数逻辑以外的额外开销，因此会对性能造成影响。影响包括几个方面，一个是assert中的if判断有可能会对流水线并行造成不好的影响，这个影响可以通过gcc的__builtin_expect内置函数，提前告知编译器代码生成来规避。另外一个不好的影响是会增加可执行代码的大小，影响指令cache的局部性。 但其实更好的做法是，通过编译期的静态检查，将问题提前暴露出来，而不是留到运行期再发现问题。例如还是上述的例子，通过sparse的静态检查，可以这样写： 123456// 函数本身是非线程安全的，需要在外边上锁保护void dothing_unsafe() __releases(lock)&#123; ... // 执行函数逻辑&#125; 其中acquires(x) 和releases(x)，acquire(x) 和release(x) 必须配对使用，都和锁有关。 每当通过IO提交操作请求队列时，必须获取该锁。 当I/O提交时，数据块层进行优化，如 plugging蓄洪(在将I/O发送到硬件之前先让I/O累积，以提高缓存效率) IO重新排序和公平调度都必须获取请求队列锁，才能继续操作。 硬件中断 较高的IOPS数会导致成比例的高中断数。当今的大多数存储设备都是这样设计的，即一个内核(CPU0)负责处理所有硬件中断，并将它们作为软中断转发到其他内核。 因此，单个核可能花费相当多的时间来处理这些中断、上下文切换以及影响应用程序可能依赖的数据局部性的L1和L2高速缓存。然后，其他CPU核心也必须使用IPI（处理器间中断）来执行IO完成例程。因此，在许多情况下，仅完成一个IO就需要两次中断和上下文切换。 远程内存访问 当强制跨CPU核心(或NUMA体系结构中的跨套接字)进行远程内存访问时，请求队列锁争用会加剧。每当IO在与发出IO的内核不同的内核上完成时，就需要这样的远程内存访问。在这种情况下，获取请求队列上的锁以从请求队列移除块IO引起对存储在上次获取该锁的核的高速缓存中的锁状态的远程存储器访问，然后在两个核上将高速缓存线标记为共享。更新时，副本将从远程缓存中显式失效。如果多个核正在主动发出IO并因此竞争此锁，则与此锁关联的缓存线将在这些核之间持续反弹。 4.多队列通过使用具有不同功能的两级队列，将单个请求队列锁上的锁争用分布到多个队列，如图所示。 软件暂存队列。数据块IO请求现在维护在一个或多个请求队列的集合中，而不是将IO转移到单个软件队列中进行调度。可以配置这些分段队列，使得系统上的每个套接字或每个内核都有一个这样的队列。因此，在具有4个插槽和每个插槽6个核心的NUMA系统上，临时区域可能包含最少4个队列，最多24个队列。如果单个队列上的争用不是瓶颈，则请求队列的可变特性会减少锁的扩散。由于许多CPU体系结构为每个套接字(通常也是NUMA节点)提供了大型共享L3缓存，因此每个处理器套接字只有一个队列可以在不利于缓存的重复数据结构和锁争用之间进行很好的权衡。 硬件调度队列。IO进入分段队列后，我们引入了一个新的中间队列层，称为硬件分派队列。使用这些队列，计划分派的数据块IO不会直接发送到设备驱动程序，而是发送到硬件分派队列。硬件分派队列的数量通常与设备驱动程序支持的硬件上下文的数量相匹配。设备驱动程序可以选择支持消息信号中断标准MSI-X[25]所支持的1到2048个队列。因为在块层中不支持IO排序，所以任何软件队列都可以馈送任何硬件队列，而不需要维护全局排序。这允许硬件实现直接映射到NUMA节点或CPU的一个或多个队列，并提供从应用程序到硬件的快速IO路径，而无需访问任何其他节点上的远程内存。 5.多队列内核数据结构单队列架构发起IO传输的核心函数是blk_queue_bio()。Multi queue多队列核心IO传输函数是blk_mq_make_request()。Multi queue多队列架构引入了struct blk_mq_tag_set、struct blk_mq_tag、struct blk_mq_hw_ctx、struct blk_mq_ctx等数据结构。最初阅读这些代码时，感觉比单队列复杂多了，很容易绕晕。 1 struct blk_mq_ctx代表每个CPU独有的软件队列； 2 struct blk_mq_hw_ctx代表硬件队列，块设备至少有一个； 3 struct blk_mq_tag每个硬件队列结构struct blk_mq_hw_ctx对应一个； 4 struct blk_mq_tag主要是管理struct request(下文简称req)的分配。struct request大家应该都比较熟悉了，单队列时代就存在，IO传输的最后都要把bio转换成request； 5 struct blk_mq_tag_set包含了块设备的硬件配置信息，比如支持的硬件队列数nr_hw_queues、队列深度queue_depth等，在块设备驱动初始化时多处使用blk_mq_tag_set初始化其他成员； 每个CPU对应唯一的软件队列blk_mq_ctx，blk_mq_ctx对应唯一的硬件队列blk_mq_hw_ctx，blk_mq_hw_ctx对应唯一的blk_mq_tag，三者的关系在后续代码分析中多次出现。 以nvme驱动程序初始化代码为例，分析多请求队列的源码，从函数nvme_dev_add开始。nvme_dev_add()函数中设置blk_mq_tag_set结构的关键成员；分配设置每个硬件队列独有blk_mq_tag结构；分配并设置struct blk_mq_tag_set *set的set-&gt;mq_map[]数组，该数组下标是CPU的编号，数组成员是硬件队列的编号，这样就完成了CPU编号与硬件队列编号的映射。 12345678910111213141516171819202122232425262728293031323334353637383940/* * Return: error value if an error occurred setting up the queues or calling * Identify Device. 0 if these succeeded, even if adding some of the * namespaces failed. At the moment, these failures are silent. TBD which * failures should be reported. */static int nvme_dev_add(struct nvme_dev *dev)&#123; if (!dev-&gt;ctrl.tagset) &#123; //设置blk_mq_tag_set的blk_mq_ops为nvme_mq_ops dev-&gt;tagset.ops = &amp;nvme_mq_ops; ////设置硬件队列个数 dev-&gt;tagset.nr_hw_queues = dev-&gt;online_queues - 1; dev-&gt;tagset.timeout = NVME_IO_TIMEOUT; dev-&gt;tagset.numa_node = dev_to_node(dev-&gt;dev); ////设置队列深度 dev-&gt;tagset.queue_depth = min_t(int, dev-&gt;q_depth, BLK_MQ_MAX_DEPTH) - 1; dev-&gt;tagset.cmd_size = nvme_cmd_size(dev); dev-&gt;tagset.flags = BLK_MQ_F_SHOULD_MERGE; dev-&gt;tagset.driver_data = dev; //调用blk_mq_alloc_tag_set()，target是struct blk_mq_tag_set tagset if (blk_mq_alloc_tag_set(&amp;dev-&gt;tagset)) return 0; dev-&gt;ctrl.tagset = &amp;dev-&gt;tagset; nvme_dbbuf_set(dev); &#125; else &#123; blk_mq_update_nr_hw_queues(&amp;dev-&gt;tagset, dev-&gt;online_queues - 1); /* Free previously allocated queues that are no longer usable */ nvme_free_queues(dev, dev-&gt;online_queues); &#125; return 0;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/* * Alloc a tag set to be associated with one or more request queues. * May fail with EINVAL for various error conditions. May adjust the * requested depth down, if if it too large. In that case, the set * value will be stored in set-&gt;queue_depth. */int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)&#123; /* * If a crashdump is active, then we are potentially in a very * memory constrained environment. Limit us to 1 queue and * 64 tags to prevent using too much memory. */ if (is_kdump_kernel()) &#123; set-&gt;nr_hw_queues = 1; set-&gt;queue_depth = min(64U, set-&gt;queue_depth); &#125; /* * There is no use for more h/w queues than cpus. 硬件队列数大于CPU个数 */ if (set-&gt;nr_hw_queues &gt; nr_cpu_ids) set-&gt;nr_hw_queues = nr_cpu_ids; //按照CPU个数分配struct blk_mq_tag_set需要的struct blk_mq_tags指针数组，每个CPU都有一个blk_mq_tags set-&gt;tags = kzalloc_node(nr_cpu_ids * sizeof(struct blk_mq_tags *), GFP_KERNEL, set-&gt;numa_node); if (!set-&gt;tags) return -ENOMEM; ret = -ENOMEM; //分配mq_map[]指针数组，按照CPU的个数分配nr_cpu_ids个unsigned int类型数据，该数组成员对应一个CPU set-&gt;mq_map = kzalloc_node(sizeof(*set-&gt;mq_map) * nr_cpu_ids, GFP_KERNEL, set-&gt;numa_node); if (!set-&gt;mq_map) goto out_free_tags; //为每个set-&gt;mq_map[cpu]分配一个硬件队列编号。该数组下标是CPU的编号，数组成员是硬件队列的编号 ret = blk_mq_update_queue_map(set); if (ret) goto out_free_mq_map; /*分配每个硬件队列独有的blk_mq_tags结构，根据硬件队列的深度queue_depth分配对应个数的request存到 struct blk_mq_tags * tags-&gt;static_rqs[]，并设置blk_mq_tags结构的nr_reserved_tags、nr_tags等其他成员*/ ret = blk_mq_alloc_rq_maps(set); if (ret) goto out_free_mq_map;&#125; 如注释，关键是调用函数blk_mq_alloc_rq_maps()分配每个硬件队列独有的blk_mq_tags结构，并初始化其成员，该函数源码如下: 1234567891011121314151617181920212223242526272829303132333435363738/* * Allocate the request maps associated with this tag_set. Note that this * may reduce the depth asked for, if memory is tight. set-&gt;queue_depth * will be updated to reflect the allocated depth. */static int blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)&#123; unsigned int depth; int err; depth = set-&gt;queue_depth; do &#123; /*分配每个硬件队列独有的blk_mq_tags结构，根据硬件队列的深度queue_depth分配对应个数的request存到struct blk_mq_tags * tags-&gt;static_rqs[]，并设置blk_mq_tags结构的nr_reserved_tags、nr_tags等其他成员*/ err = __blk_mq_alloc_rq_maps(set); if (!err)// __blk_mq_alloc_rq_maps分配成功返回0，这里就直接break了，只循环一次 break; set-&gt;queue_depth &gt;&gt;= 1; if (set-&gt;queue_depth &lt; set-&gt;reserved_tags + BLK_MQ_TAG_MIN) &#123; err = -ENOMEM; break; &#125; &#125; while (set-&gt;queue_depth); if (!set-&gt;queue_depth || err) &#123; pr_err("blk-mq: failed to allocate request map\n"); return -ENOMEM; &#125; if (depth != set-&gt;queue_depth) pr_info("blk-mq: reduced tag depth (%u -&gt; %u)\n", depth, set-&gt;queue_depth); return 0;&#125; 1234567891011121314151617181920static int __blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)&#123; int i; for (i = 0; i &lt; set-&gt;nr_hw_queues; i++) /*分配每个硬件队列独有的blk_mq_tags结构，根据硬件队列的深度queue_depth分配对应个数的request存到struct blk_mq_tags * tags-&gt;static_rqs[]，并设置blk_mq_tags结构的nr_reserved_tags、nr_tags等其他成员*/ if (!__blk_mq_alloc_rq_map(set, i)) goto out_unwind; return 0;out_unwind: while (--i &gt;= 0) blk_mq_free_rq_map(set-&gt;tags[i]); return -ENOMEM;&#125; 123456789101112131415161718192021222324252627static bool __blk_mq_alloc_rq_map(struct blk_mq_tag_set *set, int hctx_idx)&#123; int ret = 0; /*分配并返回硬件队列专属的blk_mq_tags结构，分配设置其成员nr_reserved_tags、nr_tags、rqs、static_rqs。主要 是分配struct blk_mq_tags *tags的tags-&gt;rqs[]、tags-&gt;static_rqs[]这两个req指针数组。hctx_idx是硬件队列编号，每 一个硬件队列独有一个blk_mq_tags结构*/ set-&gt;tags[hctx_idx] = blk_mq_alloc_rq_map(set, hctx_idx, set-&gt;queue_depth, set-&gt;reserved_tags); if (!set-&gt;tags[hctx_idx]) return false; /*针对hctx_idx编号的硬件队列，分配set-&gt;queue_depth个req存于tags-&gt;static_rqs[i]。具体是分配N个page，将page 的内存一片片分割成req结构大小。然后tags-&gt;static_rqs[i]记录每一个req首地址，接着执行磁盘底层驱 动初始化函数,建立request与nvme队列的关系吧*/ ret = blk_mq_alloc_rqs(set, set-&gt;tags[hctx_idx], hctx_idx, set-&gt;queue_depth); if (!ret) return true; blk_mq_free_rq_map(set-&gt;tags[hctx_idx]); set-&gt;tags[hctx_idx] = NULL; return false;&#125; blk_mq_alloc_rq_map()函数只分配struct blk_mq_tags *tags的tags-&gt;static_rqs[]这个req指针数组，实际分配req是在blk_mq_alloc_rqs()函数，源码如下。 1234567891011121314151617181920212223242526272829303132333435363738394041struct blk_mq_tags *blk_mq_alloc_rq_map(struct blk_mq_tag_set *set, unsigned int hctx_idx, unsigned int nr_tags, unsigned int reserved_tags)&#123; struct blk_mq_tags *tags; int node; node = blk_mq_hw_queue_to_node(set-&gt;mq_map, hctx_idx); if (node == NUMA_NO_NODE) node = set-&gt;numa_node; //分配一个每个硬件队列结构独有的blk_mq_tags结构，设置其成员nr_reserved_tags和nr_tags，分配 //blk_mq_tags的bitmap_tags、breserved_tags结构 tags = blk_mq_init_tags(nr_tags, reserved_tags, node, BLK_MQ_FLAG_TO_ALLOC_POLICY(set-&gt;flags)); if (!tags) return NULL; //分配nr_tags个struct request *指针赋于tags-&gt;rqs[]，不是分配struct request结构 tags-&gt;rqs = kzalloc_node(nr_tags * sizeof(struct request *), GFP_NOIO | __GFP_NOWARN | __GFP_NORETRY, node); if (!tags-&gt;rqs) &#123; blk_mq_free_tags(tags); return NULL; &#125; //分配nr_tags个struct request *指针赋予tags-&gt;static_rqs[] tags-&gt;static_rqs = kzalloc_node(nr_tags * sizeof(struct request *), GFP_NOIO | __GFP_NOWARN | __GFP_NORETRY, node); if (!tags-&gt;static_rqs) &#123; kfree(tags-&gt;rqs); blk_mq_free_tags(tags); return NULL; &#125; return tags;&#125; 接着是初始化的后半部分，在 blk_mq_init_queue()中完成。该函数主要分配块设备的运行队列request_queue，接着分配每个CPU专属的软件队列并初始化，分配硬件队列并初始化，然后建立软件队列和硬件队列的映射。 1234567891011121314151617struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *set)&#123; struct request_queue *uninit_q, *q; //分配struct request_queue并初始化 uninit_q = blk_alloc_queue_node(GFP_KERNEL, set-&gt;numa_node); if (!uninit_q) return ERR_PTR(-ENOMEM); //分配每个CPU专属的软件队列，分配硬件队列，对二者做初始化，并建立软件队列和硬件队列联系 q = blk_mq_init_allocated_queue(set, uninit_q); if (IS_ERR(q)) blk_cleanup_queue(uninit_q); return q;&#125;EXPORT_SYMBOL(blk_mq_init_queue); blk_mq_init_queue函数整体来说，是创建request_queue运行队列并初始化其成员，分配每个CPU专属的软件队列，分配硬件队列，对二者做初始化，并建立软件队列和硬件队列联系。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set, struct request_queue *q)&#123; /* mark the queue as mq asap */ q-&gt;mq_ops = set-&gt;ops; q-&gt;poll_cb = blk_stat_alloc_callback(blk_mq_poll_stats_fn, blk_mq_poll_stats_bkt, BLK_MQ_POLL_STATS_BKTS, q); if (!q-&gt;poll_cb) goto err_exit; //为每个CPU分配一个软件队列struct blk_mq_ctx，软件队列结构在这里分配 q-&gt;queue_ctx = alloc_percpu(struct blk_mq_ctx); if (!q-&gt;queue_ctx) goto err_exit; /* init q-&gt;mq_kobj and sw queues' kobjects */ blk_mq_sysfs_init(q); //分配硬件队列，这看着也是每个CPU分配一个queue_hw_ctx指针 q-&gt;queue_hw_ctx = kzalloc_node(nr_cpu_ids * sizeof(*(q-&gt;queue_hw_ctx)), GFP_KERNEL, set-&gt;numa_node); if (!q-&gt;queue_hw_ctx) goto err_percpu; //赋值q-&gt;mq_map，这个数组保存了每个CPU对应的硬件队列编号 q-&gt;mq_map = set-&gt;mq_map; /* 1 循环分配每个硬件队列结构blk_mq_hw_ctx并初始化，即对每个struct blk_mq_hw_ctx *hctx硬件队列结构大部 分成员赋初值。重点是赋值hctx-&gt;tags=blk_mq_tags，即每个硬件队列唯一对应一个blk_mq_tags，blk_mq_tags来自 struct blk_mq_tag_set 的成员struct blk_mq_tags[hctx_idx]。然后分配hctx-&gt;ctxs软件队列指针数组，注意只是指针数 组! 2 为硬件队列结构hctx-&gt;sched_tags分配blk_mq_tags，这是调度算法的tags。接着根据为这个blk_mq_tags分配 q-&gt;nr_requests个request，存于tags-&gt;static_rqs[]，这是调度算法的blk_mq_tags的request! */ ... INIT_DELAYED_WORK(&amp;q-&gt;requeue_work, blk_mq_requeue_work); INIT_LIST_HEAD(&amp;q-&gt;requeue_list); spin_lock_init(&amp;q-&gt;requeue_lock); //设置rq的make_request_fn为blk_mq_make_request，request处理函数 blk_queue_make_request(q, blk_mq_make_request); /* * Do this after blk_queue_make_request() overrides it... nr_requests被设置为队列深度 */ q-&gt;nr_requests = set-&gt;queue_depth; /* * Default to classic polling */ q-&gt;poll_nsec = -1; if (set-&gt;ops-&gt;complete) blk_queue_softirq_done(q, set-&gt;ops-&gt;complete); /*依次取出每个CPU唯一的软件队列struct blk_mq_ctx *__ctx ，__ctx-&gt;cpu记录CPU编号，还根据CPU编号取出该CPU对应的硬件队列blk_mq_hw_ctx*/ blk_mq_init_cpu_queues(q, set-&gt;nr_hw_queues); //共享tag设置 blk_mq_add_queue_tag_set(set, q); /*1:根据CPU编号依次取出每一个软件队列，再根据CPU编号取出硬件队列struct blk_mq_hw_ctx *hctx，对硬件队 列结构的hctx-&gt;ctxs[]赋值软件队列结构 2:根据硬件队列数，依次从q-&gt;queue_hw_ctx[i]数组取出硬件队列结构体struct blk_mq_hw_ctx *hctx，然后对 hctx-&gt;tags赋值blk_mq_tags结构，前边的blk_mq_realloc_hw_ctxs()函数已经对hctx-&gt;tags赋值blk_mq_tags结构 */ blk_mq_map_swqueue(q); ...&#125;EXPORT_SYMBOL(blk_mq_init_allocated_queue); 简单总结来说，blk_mq_init_allocated_queue函数负责分配每个CPU专属的软件队列，分配硬件队列，对二者做初始化，分配，并建立软件队列和硬件队列联系。该函数中调用的blk_mq_realloc_hw_ctxs()、blk_mq_map_swqueue()是两个重点函数，下文列出了源码注释。 12345678910111213141516171819202122232425262728293031323334353637383940414243static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set, struct request_queue *q)&#123; int i, j; struct blk_mq_hw_ctx **hctxs = q-&gt;queue_hw_ctx; blk_mq_sysfs_unregister(q); /* protect against switching io scheduler */ mutex_lock(&amp;q-&gt;sysfs_lock); for (i = 0; i &lt; set-&gt;nr_hw_queues; i++) &#123; int node; if (hctxs[i]) continue; node = blk_mq_hw_queue_to_node(q-&gt;mq_map, i); //分配每一个硬件队列结构blk_mq_hw_ctx hctxs[i] = kzalloc_node(blk_mq_hw_ctx_size(set), GFP_KERNEL, node); ... atomic_set(&amp;hctxs[i]-&gt;nr_active, 0); hctxs[i]-&gt;numa_node = node; hctxs[i]-&gt;queue_num = i; /* 1 为分配的struct blk_mq_hw_ctx *hctx 硬件队列结构大部分成员赋初值。重点是赋值hctx-&gt;tags=blk_mq_tags， 即每个硬件队列唯一对应一个blk_mq_tags，blk_mq_tags来自struct blk_mq_tag_set 的成员struct blk_mq_tags[hctx_idx]。 然后分配hctx-&gt;ctxs软件队列指针数组，注意只是指针数组! 2 为硬件队列结构hctx-&gt;sched_tags分配blk_mq_tags，这是调度算法的tags。接着为这个blk_mq_tags分配 q-&gt;nr_requests个request，存于tags-&gt;static_rqs[]，这是调度算法的blk_mq_tags的request! */ if (blk_mq_init_hctx(q, set, hctxs[i], i)) &#123; ... &#125; ... //设置硬件队列数 q-&gt;nr_hw_queues = i; mutex_unlock(&amp;q-&gt;sysfs_lock); blk_mq_sysfs_register(q);&#125; blk_mq_realloc_hw_ctxs()函数很重要，分配每一个硬件队列具体的数据结构blk_mq_hw_ctx。然后主要为该结构的tags和sched_tags成员，分配赋值每个硬件队列必须的blk_mq_tags。之后进行IO传输前，要从hctx-&gt;tags-&gt; static_rqs[]或者hctx-&gt;sched_tags-&gt; static_rqs[]分配一个req。该函数中调用的blk_mq_init_hctx()主要是初始化blk_mq_hw_ctx硬件队列成员，分配调度算法hctx-&gt;sched_tags需要的blk_mq_tags。 12345678910111213141516171819202122232425262728293031323334353637383940414243static int blk_mq_init_hctx(struct request_queue *q, struct blk_mq_tag_set *set, struct blk_mq_hw_ctx *hctx, unsigned hctx_idx)&#123; ... //运行队列 hctx-&gt;queue = q; hctx-&gt;flags = set-&gt;flags &amp; ~BLK_MQ_F_TAG_SHARED; cpuhp_state_add_instance_nocalls(CPUHP_BLK_MQ_DEAD, &amp;hctx-&gt;cpuhp_dead); //赋值hctx-&gt;tags的blk_mq_tags，每个硬件队列对应一个blk_mq_tags，这个tags在__blk_mq_alloc_rq_map()中赋值 hctx-&gt;tags = set-&gt;tags[hctx_idx]; /* * Allocate space for all possible cpus to avoid allocation at * runtime */ //为每个CPU分配软件队列blk_mq_ctx指针，只是指针 hctx-&gt;ctxs = kmalloc_node(nr_cpu_ids * sizeof(void *), GFP_KERNEL, node); if (!hctx-&gt;ctxs) goto unregister_cpu_notifier; if (sbitmap_init_node(&amp;hctx-&gt;ctx_map, nr_cpu_ids, ilog2(8), GFP_KERNEL, node)) goto free_ctxs; hctx-&gt;nr_ctx = 0; if (set-&gt;ops-&gt;init_hctx &amp;&amp; set-&gt;ops-&gt;init_hctx(hctx, set-&gt;driver_data, hctx_idx)) goto free_bitmap; /*为硬件队列结构hctx-&gt;sched_tags分配blk_mq_tags，一个硬件队列一个blk_mq_tags，这是调度算法的blk_mq_tags， 与硬件队列专属的blk_mq_tags不一样。然后根据为这个blk_mq_tags分配q-&gt;nr_requests个request，存于 tags-&gt;static_rqs[]*/ if (blk_mq_sched_init_hctx(q, hctx, hctx_idx)) goto exit_hctx; ...&#125; blk_mq_map_swqueue()函数主要作用是，根据CPU编号取出硬件队列结构struct blk_mq_ctx *ctx和软件队列结构struct blk_mq_ctx *ctx，然后把软件队列结构赋值给硬件队列结构，即hctx-&gt;ctxs[hctx-&gt;nr_ctx++] = ctx，相当于完成硬件队列与软件队列的映射。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768static void blk_mq_map_swqueue(struct request_queue *q)&#123;... /* * Map software to hardware queues. * * If the cpu isn't present, the cpu is mapped to first hctx. */ /*根据CPU编号依次取出每一个软件队列，再根据CPU编号取出硬件队列struct blk_mq_hw_ctx *hctx，对硬件 队列结构的hctx-&gt;ctxs[]赋值软件队列结构blk_mq_ctx*/ for_each_present_cpu(i) &#123; //根据CPU编号取出硬件队列编号 hctx_idx = q-&gt;mq_map[i]; /* unmapped hw queue can be remapped after CPU topo changed */ if (!set-&gt;tags[hctx_idx] &amp;&amp; !__blk_mq_alloc_rq_map(set, hctx_idx)) &#123; /* * If tags initialization fail for some hctx, * that hctx won't be brought online. In this * case, remap the current ctx to hctx[0] which * is guaranteed to always have tags allocated */ q-&gt;mq_map[i] = 0; &#125; //根据CPU编号取出每个CPU对应的软件队列结构struct blk_mq_ctx *ctx ctx = per_cpu_ptr(q-&gt;queue_ctx, i); //根据CPU编号取出每个CPU对应的硬件队列struct blk_mq_hw_ctx *hctx hctx = blk_mq_map_queue(q, i); cpumask_set_cpu(i, hctx-&gt;cpumask); /*硬件队列关联的第几个软件队列。硬件队列每关联一个软件队列，都hctx-&gt;ctxs[hctx-&gt;nr_ctx++] = ctx，把 软件队列结构保存到hctx-&gt;ctxs[hctx-&gt;nr_ctx++]，即硬件队列结构的hctx-&gt;ctxs[]数组，而ctx-&gt;index_hw会先保存 hctx-&gt;nr_ctx*/ ctx-&gt;index_hw = hctx-&gt;nr_ctx; //软件队列结构以hctx-&gt;nr_ctx为下标保存到hctx-&gt;ctxs[] hctx-&gt;ctxs[hctx-&gt;nr_ctx++] = ctx; &#125; mutex_unlock(&amp;q-&gt;sysfs_lock); /*根据硬件队列数，依次从q-&gt;queue_hw_ctx[i]数组取出硬件队列结构体struct blk_mq_hw_ctx *hctx，然后对 hctx-&gt;tags赋值blk_mq_tags结构*/ queue_for_each_hw_ctx(q, hctx, i) &#123; /* * If no software queues are mapped to this hardware queue, * disable it and free the request entries. */ if (!hctx-&gt;nr_ctx) &#123; /* Never unmap queue 0. We need it as a * fallback in case of a new remap fails * allocation */ if (i &amp;&amp; set-&gt;tags[i]) blk_mq_free_map_and_requests(set, i); hctx-&gt;tags = NULL; continue; &#125; //i是硬件队列编号，这是根据硬件队列编号i从struct blk_mq_tag_set *set取出硬件队列专属的blk_mq_tags hctx-&gt;tags = set-&gt;tags[i]; WARN_ON(!hctx-&gt;tags); ... &#125;&#125; 每个CPU对应唯一的软件队列blk_mq_ctx，blk_mq_ctx对应唯一的硬件队列blk_mq_hw_ctx，blk_mq_hw_ctx对应唯一的blk_mq_tags。我们在进行发起bio请求后，需要从blk_mq_tags结构的相关成员分配一个tag(其实是一个数字)，再根据tag分配一个req，最后才能进行IO派发，磁盘数据传输。 6. request的分配与发送在多队列io中，blk_mq_make_request函数负责request的处理，该函数代码注释如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465static void blk_mq_make_request(struct request_queue *q, struct bio *bio)&#123; //遍历当前进程plug_list链表上的所有req，检查bio和req代表的磁盘范围是否挨着，挨着则把bio合并到req if (!is_flush_fua &amp;&amp; !blk_queue_nomerges(q) &amp;&amp;blk_attempt_plug_merge(q, bio, &amp;request_count, &amp;same_queue_rq)) return; //在IO调度器队列里查找是否有可以合并的req，找到则可以bio后项或前项合并到req，还会触发二次合并 if (blk_mq_sched_bio_merge(q, bio)) return; //依次遍历软件队列ctx-&gt;rq_list链表上的req，然后看req能否与bio前项或者后项合并 if (blk_mq_merge_bio(q, bio)) return; /*从硬件队列的blk_mq_tags结构体的tags-&gt;bitmap_tags或者tags-&gt;nr_reserved_tags分配一个空闲tag，然后req = tags-&gt;static_rqs[tag]从static_rqs[]分配一个req，再req-&gt;tag=tag。接着hctx-&gt;tags-&gt;rqs[rq-&gt;tag] = rq，一个req必须分配一个tag才能IO传输。分配失败则启动硬件IO数据派发，之后再尝试分配tag*/ rq = blk_mq_sched_get_request(q, bio, bio-&gt;bi_rw, &amp;data); if (unlikely(!rq)) return; plug = current-&gt;plug; if (unlikely(is_flush_fua)) &#123;//如果是flush fua请求 //赋值req扇区起始地址，req结束地址，rq-&gt;bio = rq-&gt;biotail=bio，并且统计磁盘使用率等数据 blk_mq_bio_to_request(rq, bio); //将request插入到flush队列 blk_insert_flush(rq); //启动req磁盘硬件队列异步派送 blk_mq_run_hw_queue(data.hctx, true); &#125;else if (plug &amp;&amp; q-&gt;nr_hw_queues == 1)&#123;// 如果进程使用plug链表plug IO，并且硬件队列数是1 //赋值req扇区起始地址，req结束地址，rq-&gt;bio = rq-&gt;biotail=bio，并且统计磁盘使用率等数据 blk_mq_bio_to_request(rq, bio); //只是先把req添加到plug-&gt;mq_list链表上，等后续再一次性把plug-&gt;mq_list链表req向块设备驱动派发 list_add_tail(&amp;rq-&gt;queuelist, &amp;plug-&gt;mq_list) &#125;else if (plug &amp;&amp; !blk_queue_nomerges(q)) &#123;//如果进程使用plug链表plug IO，并且是硬件多队列 //赋值req扇区起始地址，req结束地址，rq-&gt;bio = rq-&gt;biotail=bio，统计磁盘使用率等数据 blk_mq_bio_to_request(rq, bio); //将req直接派发到设备驱动，如果块设备驱动层繁忙也会执行blk_mq_run_hw_queue将req异步派发给驱动 blk_mq_try_issue_directly(data.hctx, same_queue_rq); &#125;else if ((q-&gt;nr_hw_queues &gt; 1 &amp;&amp; is_sync) || (!q-&gt;elevator &amp;&amp; !data.hctx-&gt;dispatch_busy)) &#123;//如果是硬件多队列的write sync操作或者不使用调度器且硬件队列不繁忙 //赋值req扇区起始地址，req结束地址，rq-&gt;bio = rq-&gt;biotail=bio，并且统计磁盘使用率等数据 blk_mq_bio_to_request(rq, bio); //将req直接派发到设备驱动，如果块设备驱动层繁忙也会执行blk_mq_run_hw_queue将req异步派发给驱动 blk_mq_try_issue_directly(data.hctx, rq); &#125;else if (q-&gt;elevator) &#123;//使用调度器 //赋值req扇区起始地址，req结束地址，rq-&gt;bio = rq-&gt;biotail=bio，并且统计磁盘使用率等数据 blk_mq_bio_to_request(rq, bio); //将req插入IO调度器队列，并执行blk_mq_run_hw_queue()将IO派发到块设备驱动 blk_mq_sched_insert_request(rq, false, true, true); &#125;else &#123; //赋值req扇区起始地址，req结束地址，rq-&gt;bio = rq-&gt;biotail=bio，并且统计磁盘使用率等数据 blk_mq_bio_to_request(rq, bio); //把req插入到软件队列ctx-&gt;rq_list链表 blk_mq_queue_io(data.hctx, data.ctx, rq); //启动硬件队列上的req异步派发到块设备驱动 blk_mq_run_hw_queue(data.hctx, true); &#125;&#125; 每个进程都在task_struct结构体中，维护一个blk_plug *plug;变量，blk_plug定义如下： 123456struct blk_plug &#123; struct list_head list; /* requests */ struct list_head mq_list; /* blk-mq requests */ struct list_head cb_list; /* md requires an unplug callback */&#125;; list：用于缓存请求的队列mq_list：缓存硬件队列数是1的进程请求，延缓向驱动发送请求cb_list：回调函数的链表 blk_mq_make_request()函数包含的核心函数较多，基本流程是，先尝试把bio合并到软件队列或plug队列或调度算法队列。 如果无法合并，则执行blk_mq_sched_get_request()分配tag和req。这里出现”了分配tag”。这是与单队列时代的一个明显区别。 这里先简单介绍一下，多队列IO传输，将bio转换成req(就是sturct request)，大体过程是这样的：先根据当前进程所在CPU，找到CPU对应的软件队列blk_mq_ctx(获取过程见blk_mq_get_ctx函数，每个CPU都有一个唯一的软件队列)，再根据软件队列blk_mq_ctx得到其映射的硬件队列blk_mq_hw_ctx(获取过程见blk_mq_map_queue函数，每个软件队列对应唯一的硬件队列)。硬件队列blk_mq_hw_ctx结构中有两个关键成员struct blk_mq_tags *tags(针对无调度算法)和struct blk_mq_tags *sched_tags(针对有调度算法)。 有了硬件队列结构blk_mq_hw_ctx，得到其成员struct blk_mq_tags *tags指向的blk_mq_tags结构。blk_mq_tags结构又有几个关键成员： struct sbitmap_queue bitmap_tags struct sbitmap_queue breserved_tags struct request **static_rqs unsigned int nr_reserved_tags static_rqs这个指针数组保存了nr_tags个req指针，实际的req结构在前文的__blk_mq_alloc_rq_map-&gt;blk_mq_alloc_rqs分配。 struct sbitmap_queue bitmap_tags和struct sbitmap_queue breserved_tags分析下来有点像ext4 文件系统的inode bitmap，一个bit位表示一个req的使用情况，1为已经分配，0为未分配。 struct sbitmap_queue breserved_tags管理static_rqs[0~ (nr_reserved_tags-1]]这nr_reserved_tags个req，struct sbitmap_queue bitmap_tags管理static_rqs[ nr_reserved_tags ~ nr_tags]这nr_tags- nr_reserved_tags个req。 分配tag和req的一般情况：从struct sbitmap_queue bitmap_tags分析出哪个req是空闲的，返回一个数字，这个数字就是tag，表示了static_rqs[] 中哪个位置的req是空闲的。实际情况tag+ nr_reserved_tags才能表示实际空闲的req在static_rqs[]中的下标。每一个req在派发给驱动时，必须得先分配一个tag。 7.利用bcc工具采集每个cpu核心下对应硬件队列可以存储的请求数blk_mq_map_queue函数的功能是获取软件队列对应的硬件队列，通过硬件队列结构体blk_mq_hw_ctx中的变量tags-&gt;nr_tags和nr_reserved_tags取和获取当前cpu下硬件队列存储的请求数。但是由于内核5.0以上，相关结构体已经改变，没有运行成功，需要在内核4.x上安装bcc运行。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from bcc import BPFfrom time import sleepbpf_text = """#include &lt;uapi/linux/ptrace.h&gt;#include &lt;linux/blkdev.h&gt;#include &lt;linux/blk-mq.h&gt;struct proc_key_t &#123; u32 cpu;&#125;;struct val_t &#123; u32 num;&#125;;BPF_HASH(commbyreq, struct proc_key_t, struct val_t);int trace_pid_start(struct pt_regs *ctx, struct request_queue *q, int cpu)&#123; struct proc_key_t key = &#123;&#125;; struct val_t val = &#123;&#125;; key.cpu = cpu; struct blk_mq_hw_ctx * hw = q-&gt;queue_hw_ctx[q-&gt;mq_map[cpu]]; struct blk_mq_tags * tags = hw-&gt;tags; val.num = tags-&gt;nr_tags+tags-&gt;nr_reserved_tags; commbyreq.update(&amp;key, &amp;val); return 0;&#125;"""# load BPF programb = BPF(text=bpf_text)b.attach_kprobe(event="blk_mq_map_queue", fn_name="trace_pid_start")print("Tracing... Hit Ctrl-C to end.")# trace until Ctrl-Cwhile 1: print("%-6s %-16s" % ("CPU", "NUM")) # by-PID output counts = b.get_table("commbyreq") for k, v in counts: print("%-6d %-3d " % (k.cpu, v.num)) counts.clear() 8. blk_mq_sched_get_request()分配tag和req该函数主要作用是：从硬件队列的blk_mq_tags结构体的tags-&gt;bitmap_tags或者tags-&gt;nr_reserved_tags分配一个空闲tag，然后req = tags-&gt;static_rqs[tag]从static_rqs[]分配一个req，再req-&gt;tag=tag。接着hctx-&gt;tags-&gt;rqs[rq-&gt;tag] = rq，一个req必须分配一个tag才能IO传输。分配失败则启动硬件IO数据派发，之后再尝试分配tag。如果留意的话，这就是上一节介绍的tag和req的分配过程，更详细的步骤看下边的__blk_mq_alloc_request函数。流程如下。 12345678910111213141516171819202122struct request *blk_mq_sched_get_request(struct request_queue *q, struct bio *bio, unsigned int op, struct blk_mq_alloc_data *data)&#123; //data-&gt;ctx 获取当前进程所属CPU的专有软件队列 data-&gt;ctx = blk_mq_get_ctx(q); //获取软件队列的硬件队列，CPU、软件队列、硬件队列是一一对应关系 data-&gt;hctx = blk_mq_map_queue(q, data-&gt;ctx-&gt;cpu); if (e) &#123;//使用调度器 //使用调度时设置BLK_MQ_REQ_INTERNAL标志 data-&gt;flags |= BLK_MQ_REQ_INTERNAL; rq = __blk_mq_alloc_request(data, op); &#125;else&#123; //无调度器 //同理 rq = __blk_mq_alloc_request(data, op); &#125; return rq;&#125; __blk_mq_alloc_request函数的大体过程是：从硬件队列的blk_mq_tags结构体的tags-&gt;bitmap_tags或者tags-&gt;nr_reserved_tags分配一个空闲tag，然后req = tags-&gt;static_rqs[tag]从static_rqs[]分配一个req，再req-&gt;tag=tag。接着hctx-&gt;tags-&gt;rqs[rq-&gt;tag] = rq，一个req必须分配一个tag才能IO传输。分配失败则启动硬件IO数据派发，之后再尝试分配tag。函数核心是执行blk_mq_get_tag()分配tag。 1234567891011121314151617181920212223242526272829303132333435struct request *__blk_mq_alloc_request(struct blk_mq_alloc_data *data, int rw)&#123; /*从硬件队列有关的blk_mq_tags结构体的static_rqs[]数组里得到空闲的request。获取失败则启动硬件IO数据派发，之后再尝试从blk_mq_tags结构体的static_rqs[]数组里得到空闲的request。注意，这里返回的是空闲的request在static_rqs[]数组的下标*/ tag = blk_mq_get_tag(data); if (tag != BLK_MQ_TAG_FAIL) //分配tag成功 &#123; //有调度器时返回硬件队列的hctx-&gt;sched_tags,无调度器时返回硬件队列的hctx-&gt;tags struct blk_mq_tags *tags = blk_mq_tags_from_data(data); //从tags-&gt;static_rqs[tag]得到空闲的req，tag是req在tags-&gt;static_rqs[ ]数组的下标 rq = tags-&gt;static_rqs[tag]; //这里真正分配得到本次传输使用的req if (data-&gt;flags &amp; BLK_MQ_REQ_INTERNAL) //用调度器时设置 &#123; rq-&gt;tag = -1; __rq_aux(rq, data-&gt;q)-&gt;internal_tag = tag;//这是req的tag &#125; else &#123; //赋值为空闲req在blk_mq_tags结构体的static_rqs[]数组的下标 rq-&gt;tag = tag; __rq_aux(rq, data-&gt;q)-&gt;internal_tag = -1; //这里边保存的req是刚从static_rqs[]得到的空闲的req data-&gt;hctx-&gt;tags-&gt;rqs[rq-&gt;tag] = rq; &#125; //对新分配的req进行初始化，赋值软件队列、req起始时间等 blk_mq_rq_ctx_init(data-&gt;q, data-&gt;ctx, rq, rw); return rq; &#125; return NULL;&#125; 9. request direct直接派发blk_mq_try_issue_directly() 将req direct直接派发给磁盘设备驱动。 1234567891011121314151617181920static void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx, struct request *rq)&#123; /*从硬件队列的blk_mq_tags结构体的tags-&gt;bitmap_tags或者tags-&gt;nr_reserved_tags分配一个空闲tag赋于rq-&gt;tag，然后hctx-&gt;tags-&gt;rqs[rq-&gt;tag] = rq，一个req必须分配一个tag才能IO传输。分配失败则启动硬件IO数据派发，之后再尝试分配tag，循环,直到分配req成功。然后调用磁盘驱动queue_rq接口函数向驱动派发req，启动磁盘数据传输。如果遇到磁盘驱动硬件忙，则释放req的tag，设置硬件队列忙*/ ret = __blk_mq_try_issue_directly(hctx, rq, false); //如果硬件队列忙，把req添加到硬件队列hctx-&gt;dispatch队列，间接启动req硬件派发 if (ret == BLK_MQ_RQ_QUEUE_BUSY || ret == BLK_MQ_RQ_QUEUE_DEV_BUSY) blk_mq_request_bypass_insert(rq, true); /*req磁盘数据传输完成了，增加ios、ticks、time_in_queue、io_ticks、flight、sectors扇区数等使用计数。依次取出req-&gt;bio链表上所有req对应的bio，一个一个更新bio结构体成员数据，执行bio的回调函数。还更新req-&gt;__data_len和req-&gt;buffer*/ else if (ret != BLK_MQ_RQ_QUEUE_OK) blk_mq_end_request(rq, ret);&#125; 核心是执行 __blk_mq_try_issue_directly()函数。 1234567891011121314static int __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx, struct request *rq, bool bypass_insert)&#123; /*从硬件队列的blk_mq_tags结构体的tags-&gt;bitmap_tags或者tags-&gt;nr_reserved_tags分配一个空闲tag赋于rq-&gt;tag，然后hctx-&gt;tags-&gt;rqs[rq-&gt;tag] = rq，一个req必须分配一个tag才能IO传输。分配失败则启动硬件IO数据派发，之后再尝试分配tag，循环。但是如果req已经有tag 了，会直接返回，不用再分配tag*/ blk_mq_get_driver_tag(rq, NULL, false)); /*调用磁盘驱动queue_rq接口函数，根据req设置command,把req添加到q-&gt;timeout_list，并且启动q-&gt;timeout定时器,把新的command复制到sq_cmds[]命令队列，这看着是req直接发给磁盘驱动进行数据传输了。如果遇到磁盘驱动硬件忙，则设置硬件队列忙，还释放req的tag。*/ return __blk_mq_issue_directly(hctx, rq);&#125; __blk_mq_try_issue_directly函数的详细流程：从硬件队列的blk_mq_tags结构体的tags-&gt;bitmap_tags或者tags-&gt;nr_reserved_tags分配一个空闲tag赋于rq-&gt;tag，然后hctx-&gt;tags-&gt;rqs[rq-&gt;tag] = rq，一个req必须分配一个tag才能IO传输。分配失败则启动硬件IO数据派发，之后再尝试分配tag，循环,直到分配req成功。然后调用磁盘驱动queue_rq接口函数向驱动派发req，启动磁盘数据传输。如果遇到磁盘驱动硬件忙，则释放req的tag,设置硬件队列忙。 其实核心就两点：1为req分配tag；2把req派发给磁盘驱动，启动磁盘数据传输。有人可能会问，不是在blk_mq_make_request-&gt;blk_mq_sched_get_request 中已经为req分配过tag了，为什么这里还要再分配？关于这一点，我的分析是，如果req已经分配过tag了，执行blk_mq_get_driver_tag函数(下一节讲)会直接返回。但是会存在这种情况，req在派发给磁盘驱动时，磁盘驱动硬件繁忙，派发失败，则会把req加入硬件队列hctx-&gt;dispatch链表，然后把req的tag释放掉，则req-&gt;tag=-1，等空闲时派发该req。好的，空闲时间来了，再次派发该req，此时就需要执行blk_mq_get_driver_tag为req重新分配一个tag。一个req在派发给驱动时，必须分配一个tag！ __blk_mq_issue_directly是直接将req派发给驱动的核心函数。 12345678910111213141516171819202122static int __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx, struct request *rq)&#123; //根据req设置磁盘驱动 command,把req添加到q-&gt;timeout_list，并且启动q-&gt;timeout,把command复制到nvmeq-&gt;sq_cmds[]队列等等 ret = q-&gt;mq_ops-&gt;queue_rq(hctx, &amp;bd); switch (ret) &#123; case BLK_MQ_RQ_QUEUE_OK://成功把req派发给磁盘硬件驱动 blk_mq_update_dispatch_busy(hctx, false);//设置硬件队列不忙，看着就hctx-&gt;dispatch_busy = ewma break; case BLK_MQ_RQ_QUEUE_BUSY: case BLK_MQ_RQ_QUEUE_DEV_BUSY://这是遇到磁盘硬件驱动繁忙，req没有派送给驱动 blk_mq_update_dispatch_busy(hctx, true);//设置硬件队列忙 //硬件队列繁忙，则从tags-&gt;bitmap_tags或者breserved_tags中按照req-&gt;tag这个tag编号释放tag __blk_mq_requeue_request(rq); break; default: //标记硬件队列不忙 blk_mq_update_dispatch_busy(hctx, false); break; &#125; return ret;&#125; 基本是调用磁盘驱动层的函数，将req有关的磁盘传输信息发送给驱动，然后会进行磁盘数据传输。如果遇到磁盘驱动硬件忙，则设置硬件队列忙，还释放req的tag。 blk_mq_get_driver_tag() 在req派发给驱动前分配tag。 12345678910111213141516171819202122232425262728bool blk_mq_get_driver_tag(struct request *rq, struct blk_mq_hw_ctx **hctx, bool wait)&#123; /*如果req对应的tag没有被释放，则直接返回完事，其实还有一种情况rq-&gt;tag被置-1。就是__blk_mq_alloc_request()函数分配过tag和req后，如果使用了调度器，则rq-&gt;tag = -1。这种情况，rq-&gt;tag != -1也成立，但是再直接执行blk_mq_get_driver_tag()分配tag也没啥意思呀，因为tag已经分配过了。所以感觉该函数主要还是针对req因磁盘硬件驱动繁忙无法派送，然后释放了tag，再派发时分配tag的情况。*/ if (rq-&gt;tag != -1) goto done; //判断tag是否预留的，是则加上BLK_MQ_REQ_RESERVED标志 if (blk_mq_tag_is_reserved(data.hctx-&gt;sched_tags, rq_aux(rq)-&gt;internal_tag)) data.flags |= BLK_MQ_REQ_RESERVED; /*从硬件队列的blk_mq_tags结构体的tags-&gt;bitmap_tags或者tags-&gt;nr_reserved_tags分配一个空闲tag赋于rq-&gt;tag，然后hctx-&gt;tags-&gt;rqs[rq-&gt;tag] = rq，一个req必须分配一个tag才能IO传输。分配失败则启动硬件IO数据派发，之后再尝试分配tag，循环。*/ rq-&gt;tag = blk_mq_get_tag(&amp;data); //对hctx-&gt;tags-&gt;rqs[rq-&gt;tag]赋值 data.hctx-&gt;tags-&gt;rqs[rq-&gt;tag] = rq; //之所以这里重新赋值，是因为blk_mq_get_tag中可能会休眠，等再次唤醒进程所在CPU就变了，就会重新获取一次硬件队列保存到data.hctx *hctx = data.hctx; //分配成功返回1 return rq-&gt;tag != -1;&#125; 从硬件队列的blk_mq_tags结构体的tags-&gt;bitmap_tags或者tags-&gt;nr_reserved_tags分配一个空闲tag赋于rq-&gt;tag，然后hctx-&gt;tags-&gt;rqs[rq-&gt;tag] = rq，一个req必须分配一个tag才能IO传输。分配失败则启动硬件IO数据派发，之后再尝试分配tag，循环。 可以发现，该函数本质还是调用前文介绍过的blk_mq_get_tag()去硬件队列的blk_mq_tags结为req分配一个tag。该函数只是分配tag，没有分配req。blk_mq_get_driver_tag()存在的意义是:req派发给磁盘驱动时，遇到磁盘硬件队列繁忙，无法派送，则释放掉tag，req加入硬件hctx-&gt;dispatch链表异步派发。等再次派发时，就会执行blk_mq_get_driver_tag()为req分配一个tag。 10. 软件队列ctx-&gt;rq_list、硬件hctx-&gt;dispatch、IO算调度法队列链表上的req派发blk_mq_try_issue_directly()类的req direct 派发是针对单个req的，blk_mq_run_hw_queue()是派发类软件队列ctx-&gt;rq_list、硬件hctx-&gt;dispatch链表、IO调度算法队列上的req的，这是二者最大的区别。 启动硬件队列上的req派发到块设备驱动。 123456789bool blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)//async为true表示异步传输，false表示同步&#123; //有req需要硬件传输 if (need_run) &#123; __blk_mq_delay_run_hw_queue(hctx, async, 0); return true; &#125; return false&#125; 123456789101112131415161718static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,//async为true则异步，false则同步传输unsigned long msecs)// msecs决定派发延时&#123; //同步传输 if (!async &amp;&amp; !(hctx-&gt;flags &amp; BLK_MQ_F_BLOCKING)) &#123; /*各种各样场景的req派发，hctx-&gt;dispatch硬件队列dispatch链表上的req派发;有deadline调度算法时红黑树或 者fifo调度队列上的req派发;无IO调度器时，硬件队列关联的所有软件队列ctx-&gt;rq_list上的req的派发等等。派发过程 应该都是调用blk_mq_dispatch_rq_list()，磁盘驱动硬件不忙直接启动req传输，繁忙的话则把剩余的req转移到 hctx-&gt;dispatch队列，然后启动异步传输*/ __blk_mq_run_hw_queue(hctx); return; &#125; /*启动异步传输，开启kblockd_workqueue内核线程workqueue，异步执行hctx-&gt;run_work对应的work函数 blk_mq_run_work_fn, 实际blk_mq_run_work_fn里执行的还是__blk_mq_run_hw_queue*/ kblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &amp;hctx-&gt;run_work,msecs_to_jiffies(msecs));&#125; 1234567891011121314static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)&#123; //硬件队列锁，这时如果是同一个硬件队列，就有锁抢占了 hctx_lock(hctx, &amp;srcu_idx); /*各种各样场景的req派发，hctx-&gt;dispatch硬件队列dispatch链表上的req派发;有deadline调度算法时红黑树或者fifo 调度队列上的req派发；无IO调度算法时，硬件队列关联的所有软件队列ctx-&gt;rq_list上的req的派发等等。派发 都是调用blk_mq_dispatch_rq_list()，磁盘驱动硬件不忙直接启动req传输，繁忙的话则把剩余的req转移到 hctx-&gt;dispatch队列，然后启动异步传输*/ blk_mq_sched_dispatch_requests(hctx); hctx_unlock(hctx, srcu_idx);&#125; 核心是调用blk_mq_sched_dispatch_requests ()函数。blk_mq_sched_dispatch_requests()函数派发各种队列的req。 123456789101112131415161718192021222324252627282930313233343536373839404142void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)&#123; LIST_HEAD(rq_list); if (!list_empty_careful(&amp;hctx-&gt;dispatch)) &#123; //把hctx-&gt;dispatch链表上的req转移到局部rq_list list_splice_init(&amp;hctx-&gt;dispatch, &amp;rq_list); &#125; //如果hctx-&gt;dispatch上有req要派发，hctx-&gt;dispatch链表上的req已经转移到rq_list if (!list_empty(&amp;rq_list)) &#123; //这里设置了hctx-&gt;state的BLK_MQ_S_SCHED_RESTART标志位 blk_mq_sched_mark_restart_hctx(hctx); /*rq_list上的req来自hctx-&gt;dispatch硬件派发队列，遍历list上的req，先给req在硬件队列hctx的blk_mq_tags 里分配一个空闲tag，就是建立req与硬件队列的联系吧，然后把req派发给块设备驱动。看着任一个req要启动硬 件传输，都要从blk_mq_tags结构里得到一个空闲的tag。如果磁盘驱动硬件繁忙，还要把list剩余的req转移到 hctx-&gt;dispatch，启动异步传输。下发给块设备驱动的req成功减失败总个数不为0返回true。否则false。*/ if (blk_mq_dispatch_rq_list(q, &amp;rq_list, false)) &#123; if (has_sched_dispatch) //有调度器则接着派发调度器队列上的req //派发调度器队列上的req blk_mq_do_dispatch_sched(hctx); else //派发硬件队列绑定的所有软件队列上的req blk_mq_do_dispatch_ctx(hctx); &#125; &#125; else if (has_sched_dispatch)&#123; blk_mq_do_dispatch_sched(hctx); //派发调度器队列上的req &#125; else if (hctx-&gt;dispatch_busy)&#123; blk_mq_do_dispatch_ctx(hctx); //派发硬件队列绑定的所有软件队列上的req &#125;else&#123; //把硬件队列hctx关联的软件队列上的ctx-&gt;rq_list链表上req转移到传入的rq_list链表 blk_mq_flush_busy_ctxs(hctx, &amp;rq_list); /*遍历rq_list上的req，先给req在硬件队列hctx的blk_mq_tags里分配一个空闲tag，然后把req派发给块设备 驱动。如果遇到块设备驱动层繁忙，则把req再加入hctx-&gt;dispatch异步派发*/ blk_mq_dispatch_rq_list(q, &amp;rq_list, false); &#125;&#125; 总结下来，主要是3种情况1 执行blk_mq_dispatch_rq_list()派发硬件队列hctx-&gt;dispatch链表上的req2执行blk_mq_do_dispatch_sched()派发调度器队列上的req。3执行blk_mq_do_dispatch_ctx ()函数派发软件队列ctx-&gt;rq_list链表上的req。 blk_mq_do_dispatch_sched() 和blk_mq_flush_busy_ctxs()最后也是指定的blk_mq_dispatch_rq_list()函数进行实际的派发。 10.1 blk_mq_do_dispatch_sched()派发调度器队列上的req这里以deadline调度算法为例。执行deadline算法派发函数，循环从fifo或者红黑树队列选择待派发给传输的req，然后给req在硬件队列hctx的blk_mq_tags里分配一个空闲tag，接着把req派发给块设备驱动。如果磁盘驱动硬件繁忙，则把req转移到hctx-&gt;dispatch队列，然后启动req异步传输。硬件队列繁忙或者算法队列没有req了则跳出循环返回。 12345678910111213141516static void blk_mq_do_dispatch_sched(struct blk_mq_hw_ctx *hctx)&#123; LIST_HEAD(rq_list); do &#123; /*执行deadline算法派发函数，从fifo或者红黑树队列选择待派发的req返回。然后设置新的next_rq，并把req从fifo队列和红黑树队列剔除，req来源有:上次派发设置的next_rq;read req派发过多而选择的write req;fifo 队列上超时要传输的req，统筹兼顾，有固定策略*/ rq = e-&gt;aux-&gt;ops.mq.dispatch_request(hctx);//dd_dispatch_request //把选择出来派发的req加入局部变量rq_list链表 list_add(&amp;rq-&gt;queuelist, &amp;rq_list); // blk_mq_dispatch_rq_list()才会调度算法队列上的req进行派发 &#125; while (blk_mq_dispatch_rq_list(q, &amp;rq_list, true))&#125; 10.2 blk_mq_do_dispatch_ctx ()派发软件队列ctx-&gt;rq_list链表上的req循环遍历hctx硬件队列关联的所有软件队列上ctx-&gt;rq_list链表的req，给req在硬件队列hctx的blk_mq_tags里分配一个空闲tag，然后把req派发给块设备驱动。如果遇到块设备驱动层繁忙，则把req再加入hctx-&gt;dispatch链表异步派发。 123456789101112131415161718static void blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)&#123; //依次遍历hctx硬件队列关联的所有软件队列 do &#123; //从软件队列ctx-&gt;rq_list链表取出req，然后从软件队列中剔除req。接着清除hctx-&gt;ctx_map中软件队列对应的标志位??????? rq = blk_mq_dequeue_from_ctx(hctx, ctx); if (!rq) &#123; break; &#125; //req加入到rq_list list_add(&amp;rq-&gt;queuelist, &amp;rq_list); //取出硬件队列关联的下一个软件队列 ctx = blk_mq_next_ctx(hctx, rq-&gt;mq_ctx); // blk_mq_dispatch_rq_list()中才完成对rq_list链表上的软件队列的req的派发 &#125; while (blk_mq_dispatch_rq_list(q, &amp;rq_list, true))&#125; 10.3 blk_mq_dispatch_rq_list()实际完成对各个队列上的req的派发list来自hctx-&gt;dispatch硬件派发队列、软件队列rq_list链表上、调度算法队列等req。遍历list上的req，先给req在硬件队列hctx的blk_mq_tags里分配一个空闲tag，然后调用磁盘驱动queue_rq函数派发req。任一个req要启动硬件传输前，都要从blk_mq_tags结构里得到一个空闲的tag。如果遇到磁盘驱动硬件繁忙，则要把这个派发失败的req再添加list链表，再把list链表上的所有req转移到hctx-&gt;dispatch队列，之后启动异步派发时再从hctx-&gt;dispatch链表上取出这些req派发。下发给驱动的req成功减失败总个数不为0返回true，其他返回false。 可以发现一个规律，最终肯定调用磁盘驱动的queue_rq函数才能把req派送给磁盘驱动，然后才能进行磁盘数据传输。但是该函数有三个返回值，一是BLK_MQ_RQ_QUEUE_OK，表示派送req成功；BLK_MQ_RQ_QUEUE_DEV_BUSY或者BLK_MQ_RQ_QUEUE_BUSY，表示磁盘驱动硬件繁忙，则无法派送req，需要把req再放入hctx-&gt;dispatch链表，之后进行异步派发。异步派发最后还是执行blk_mq_dispatch_rq_list()函数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list, bool got_budget)&#123;do &#123; //从list链表取出一个req rq = list_first_entry(list, struct request, queuelist); /*先根据rq-&gt;mq_ctx-&gt;cpu这个CPU编号从q-&gt;mq_map[cpu]找到硬件队列编号，再q-&gt;queue_hw_ctx[硬件队列编号]返回硬件队列唯一的blk_mq_hw_ctx结构体，每个CPU都对应了唯一的硬件队列*/ hctx = blk_mq_map_queue(rq-&gt;q, rq-&gt;mq_ctx-&gt;cpu); /*从硬件队列的blk_mq_tags结构体的tags-&gt;bitmap_tags或者tags-&gt;nr_reserved_tags分配一个空闲tag赋于rq-&gt;tag，然后hctx-&gt;tags-&gt;rqs[rq-&gt;tag] = rq，一个req必须分配一个tag才能IO传输。分配失败则启动硬件IO数据派发，之后再尝试分配tag 。分配成功返回true，一般情况都分配成功*/ if (!blk_mq_get_driver_tag(rq, NULL, false)) &#123; //获取tag失败，则要尝试开始休眠了，再尝试分配，函数返回时获取tag就成功了 if (!blk_mq_mark_tag_wait(&amp;hctx, rq)) &#123; blk_mq_put_dispatch_budget(hctx); //如果还是分配tag失败，但是硬件队列有共享tag标志 if (hctx-&gt;flags &amp; BLK_MQ_F_TAG_SHARED) no_tag = true;//设置no_tag标志位 //直接跳出循环，不再进行req派发 break; &#125; &#125; //从list链表剔除req list_del_init(&amp;rq-&gt;queuelist); //根据req设置nvme_command,把req添加到q-&gt;timeout_list，并且启动q-&gt;timeout,把新的cmd复制到nvmeq-&gt;sq_cmds[]队列。真正把req派发给驱动，启动硬件nvme硬件传输 ret = q-&gt;mq_ops-&gt;queue_rq(hctx, &amp;bd);//nvme_queue_rq switch (ret) &#123; case BLK_MQ_RQ_QUEUE_OK://传输完成，queued++表示传输完成的req queued++; break; case BLK_MQ_RQ_QUEUE_BUSY: case BLK_MQ_RQ_QUEUE_DEV_BUSY: if (!list_empty(list)) &#123; nxt = list_first_entry(list, struct request, queuelist); blk_mq_put_driver_tag(nxt); &#125; //磁盘驱动硬件繁忙，要把本次派送的req再添加到list链表 list_add(&amp;rq-&gt;queuelist, list); //tags-&gt;bitmap_tags中按照req-&gt;tag把req的tag编号释放掉,与blk_mq_get_driver_tag()获取tag相反 __blk_mq_requeue_request(rq); break; default: pr_err("blk-mq: bad return on queue: %d\n", ret); case BLK_MQ_RQ_QUEUE_ERROR: errors++;//下发给驱动时出错errors加1，这种情况一般不会有吧，除非磁盘硬件有问题了 rq-&gt;errors = -EIO; blk_mq_end_request(rq, rq-&gt;errors); break; &#125; //如果磁盘驱动硬件繁忙，break跳出do...while循环 if (ret == BLK_MQ_RQ_QUEUE_BUSY || ret == BLK_MQ_RQ_QUEUE_DEV_BUSY) break; &#125;while (!list_empty(list)); //list链表不空，说明磁盘驱动硬件繁忙，有部分req没有派送给驱动 if (!list_empty(list)) &#123; //这里是把list链表上没有派送给驱动的的req再移动到hctx-&gt;dispatch链表 list_splice_init(list, &amp;hctx-&gt;dispatch); /*因为硬件队列繁忙没有把hctx-&gt;dispatch上的req全部派送给驱动，则下边就再执行一次blk_mq_run_hw_queue()或者blk_mq_delay_run_hw_queue()，再进行一次异步派发，就那几招，一个套路*///测试hctx-&gt;state是否设置了BLK_MQ_S_SCHED_RESTART位，blk_mq_sched_dispatch_requests()就会设置这个标志位 needs_restart = blk_mq_sched_needs_restart(hctx); if (!needs_restart ||(no_tag &amp;&amp; list_empty_careful(&amp;hctx-&gt;dispatch_wait.task_list))) //再次调用blk_mq_run_hw_queue()启动异步req派发true表示允许异步 blk_mq_run_hw_queue(hctx, true); //如果设置了BLK_MQ_S_SCHED_RESTART标志位，并且磁盘驱动硬件繁忙导致了部分req没有来得及传输完 else if (needs_restart &amp;&amp; (ret == BLK_MQ_RQ_QUEUE_BUSY)) //调用blk_mq_delay_run_hw_queue，但这次是异步传输，即开启kblockd_workqueue内核线程派发req blk_mq_delay_run_hw_queue(hctx, BLK_MQ_RESOURCE_DELAY); //更新hctx-&gt;dispatch_busy，设置硬件队列繁忙 blk_mq_update_dispatch_busy(hctx, true); //返回false，说明硬件队列繁忙 return false; &#125; if (ret == BLK_MQ_RQ_QUEUE_BUSY || ret == BLK_MQ_RQ_QUEUE_DEV_BUSY) return false;//返回false表示硬件队列忙 /*queued表示成功派发给驱动的req个数，errors表示下发给驱动时出错的req个数，二者加起来不为0才返回非。下发给驱动的req成功减失败总个数不为0返回true*/ return (queued + errors) != 0;&#125; 11. request plug形式的派发blk_flush_plug_list()派发plug-&gt;mq_list链表上的req。 12345678void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)&#123; if (!list_empty(&amp;plug-&gt;mq_list)) blk_mq_flush_plug_list(plug, from_schedule); if (list_empty(&amp;plug-&gt;list)) return;&#125; 核心是执行blk_mq_flush_plug_list函数。 函数核心流程：每次循环，取出plug-&gt;mq_list上的req，添加到ctx_list局部链表。如果每两次取出的req都属于一个软件队列，只是把这些req添加到局部ctx_list链表，该函数最后执行blk_mq_sched_insert_requests把ctx_list链表上的req进行派发。如果前后两次取出的req不属于一个软件队列，则立即执行blk_mq_sched_insert_requests()将ctx_list链表已经保存的req进行派发，然后把本次循环取出的req继续添加到ctx_list局部链表。 简单来说，blk_mq_sched_insert_requests()只会派发同一个软件队列上的req。该函数req的派发，如果有调度器，则把req先插入到IO算法队列；如果无调度器，会尝试执行blk_mq_try_issue_list_directly直接将req派发给磁盘设备驱动。最后再执行blk_mq_run_hw_queue()把剩余的因各种原因未派发的req进行同步或异步派发。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule)&#123; LIST_HEAD(ctx_list);//ctx_list临时保存了当前进程plug-&gt;mq_list链表上的部分req unsigned int depth; //就是令list指向plug-&gt;mq_list的吧 list_splice_init(&amp;plug-&gt;mq_list, &amp;list); //对plug-&gt;mq_list链表上的req进行排序吧，排序规则基于req的扇区起始地址 list_sort(NULL, &amp;list, plug_ctx_cmp); //循环直到plug-&gt;mq_list链表上的req空 while (!list_empty(&amp;list)) &#123; //plug-&gt;mq_list取一个req rq = list_entry_rq(list.next); //从链表删除req list_del_init(&amp;rq-&gt;queuelist);/*this_ctx是上一个req的软件队列，rq-&gt;mq_ctx是当前req的软件队列。二者软件队列相等则if不成立，只是把req添加到局部ctx_list链表。如果二者软件队列不等，则执行if里边的blk_mq_sched_insert_requests把局部ctx_list链表上的req进行派送。然后把局部ctx_list链表清空，重复上述循环*/ if (rq-&gt;mq_ctx != this_ctx) &#123; if (this_ctx) &#123; //派发this_ctx链表上的req blk_mq_sched_insert_requests(this_q, this_ctx,&amp;ctx_list,from_schedule); //this_ctx赋值为req软件队列 this_ctx = rq-&gt;mq_ctx; this_q = rq-&gt;q; //遇到不同软件队列的req，depth清0 depth = 0; &#125; &#125; depth++; //把req添加到局部变量ctx_list链表，看着是向ctx_list插入一个req，depth深度就加1 list_add_tail(&amp;rq-&gt;queuelist, &amp;ctx_list); &#125; /*如果plug-&gt;mq_list上的req，rq-&gt;mq_ctx都指向同一个软件队列，前边的blk_mq_sched_insert_requests执行不了，则在这里执行一次，将ctx_list链表上的req进行派发。还有一种情况，是plug-&gt;mq_list链表上的最后一个req也只能在这里派发*/ if (this_ctx) &#123; //派发this_ctx链表上的req blk_mq_sched_insert_requests(this_q, this_ctx, &amp;ctx_list,from_schedule); &#125;&#125; 11.1 blk_mq_sched_insert_requests真正派发plug-&gt;mq_list链表上的req1234567891011121314151617181920212223242526272829void blk_mq_sched_insert_requests(struct request_queue *q, struct blk_mq_ctx *ctx, struct list_head *list, bool run_queue_async)&#123; if (e &amp;&amp; e-&gt;aux-&gt;ops.mq.insert_requests) //使用调度器 &#123; /*尝试将req合并到q-&gt;last_merg或者调度算法的hash队列的临近req。合并不了的话，把req插入到deadline 调度算法的红黑树和fifo队列，设置req在fifo队列的超时时间。还插入elv调度算法的hash队列*/ e-&gt;aux-&gt;ops.mq.insert_requests(hctx, list, false); &#125; else &#123; //硬件队列不能忙，没用IO调度器，不能异步处理，if才成立 if (!hctx-&gt;dispatch_busy &amp;&amp; !e &amp;&amp; !run_queue_async) &#123; //将list链表上的req进行直接派发 blk_mq_try_issue_list_directly(hctx, list); //如果list空，说明所有的req都派发磁盘驱动了，直接返回收工 if (list_empty(list)) return; &#125; // 到这里，说明list链表上还有剩余的req没有派发硬件队列传输，则需把list链表上的剩下的所 //有req插入到到软件队列ctx-&gt;rq_list链表 blk_mq_insert_requests(hctx, ctx, list); &#125; //再次启动硬件IO数据派发 blk_mq_run_hw_queue(hctx, run_queue_async);&#125; 如果有IO调度算法，则把list(来自plug-&gt;mq_list)链表上的req插入elv的hash队列，mq-deadline算法的还要插入红黑树和fifo队列。如果没有使用IO调度算法，则执行blk_mq_try_issue_list_directly函数，在该函数中：取出list链表的上的req，从硬件队列的blk_mq_tags结构体的tags-&gt;bitmap_tags或者tags-&gt;nr_reserved_tags分配一个空闲tag赋于req-&gt;tag，然后调用磁盘驱动queue_rq接口函数把req派发给驱动。如果遇到磁盘驱动硬件忙，则设置硬件队列忙，还释放req的tag。然后把这个失败派送的req插入hctx-&gt;dispatch链表，此时如果list链表空则执行blk_mq_run_hw_queue同步派发req(这个过程见blk_mq_request_bypass_insert)，接着就return返回了。 因为此时磁盘驱动硬件忙，不能再继续把list剩余的req再强制进行派发了，则执行blk_mq_insert_requests函数把这些剩余未派发的req插入到软件队列ctx-&gt;rq_list链表上，然后执行blk_mq_run_hw_queue再进行req同步或异步派发。下文重点介绍函数blk_mq_try_issue_list_directly。 1234567891011121314151617181920212223242526272829303132333435void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,struct list_head *list)&#123; //list临时保存了当前进程plug-&gt;mq_list链表上的部分req,遍历该链表上的req while (!list_empty(list)) &#123; //从plug-&gt;mq_list链表取出一个req struct request *rq = list_first_entry(list, struct request,queuelist); //从list链表剔除req list_del_init(&amp;rq-&gt;queuelist); //真正req派送在这里 ret = blk_mq_request_issue_directly(rq); /*如果ret为BLK_MQ_RQ_QUEUE_OK，说明只是把req派发给磁盘驱动。如果是BLK_MQ_RQ_QUEUE_BUSY或者BLK_MQ_RQ_QUEUE_DEV_BUSY，则说明遇到磁盘驱动硬件繁忙，直接break。如果req是其他值，说明这个req传输完成了，则执行blk_mq_end_request()进行IO统计*/ if (ret != BLK_MQ_RQ_QUEUE_OK) &#123; if (ret == BLK_MQ_RQ_QUEUE_BUSY ||ret == BLK_MQ_RQ_QUEUE_DEV_BUSY) &#123; //磁盘驱动硬件繁忙，把req添加到硬件队列hctx-&gt;dispatch队列，如果list链表空为true，则同步启动req硬件派发 blk_mq_request_bypass_insert(rq,list_empty(list)); //注意，磁盘驱动硬件的话，直接直接跳出循环，函数返回了 break; &#125; //该req磁盘数据传输完成了，增加ios、ticks、time_in_queue、io_ticks、flight、sectors扇区数等使用 //计数。依次取出req-&gt;bio链表上所有req对应的bio,一个一个更新bio结构体成员数据，执行bio的回调函数。 //还更新req-&gt;__data_len和req-&gt;buffer。 blk_mq_end_request(rq, ret); &#125; &#125;&#125; 依次遍历list(来自plug-&gt;mq_list)链表上的req，执行blk_mq_request_issue_directly()派发该req。在该函数中，从硬件队列的blk_mq_tags结构体的tags-&gt;bitmap_tags或者tags-&gt;nr_reserved_tags分配一个空闲tag赋于rq-&gt;tag，调用磁盘驱动queue_rq接口函数把req派发给驱动。如果遇到磁盘驱动硬件忙，则设置硬件队列忙，还释放req的tag，然后把这个派送失败的req插入hctx-&gt;dispatch链表，此时如果list链表空则执行blk_mq_run_hw_queue同步派发req，之后就从blk_mq_request_issue_directly函数返回。如果遇到req传输完成则执行blk_mq_end_request()统计IO使用率等数据并唤醒进程。 blk_mq_request_issue_directly函数呢，调用__blk_mq_try_issue_directly()执行具体的req派发工作。 12345678910111213int blk_mq_request_issue_directly(struct request *rq)&#123; //req所在的软件队列 struct blk_mq_ctx *ctx = rq-&gt;mq_ctx; //与ctx-&gt;cpu这个CPU编号对应的硬件队列 struct blk_mq_hw_ctx *hctx = blk_mq_map_queue(rq-&gt;q, ctx-&gt;cpu); //硬件队列加锁 hctx_lock(hctx, &amp;srcu_idx); ret = __blk_mq_try_issue_directly(hctx, rq, true); hctx_unlock(hctx, srcu_idx);&#125; 进程plug-&gt;mq_list链表上的req派送，一个个是先执行blk_mq_try_issue_list_directly直接将req派送给磁盘驱动进行数据传输。如果遇到磁盘驱动硬件繁忙，还是要把req加入hctx-&gt;dispatch链表。接着还要把plug-&gt;mq_list链表上剩余未派发的req加入软件队列ctx-&gt;rq_list链表上。最后执行blk_mq_run_hw_queue()再把hctx-&gt;dispatch链表和ctx-&gt;rq_list链表上的req进行同步或者异步派发。 12. 总结软件队列ctx-&gt;rq_list链表、硬件队列hctx-&gt;dispatch链表、IO调度算法队列的相关链表、req plug模式的plug-&gt;mq_list，blk_mq_try_issue_list_directly、__blk_mq_try_issue_directly direct 派发模式。 硬件队列hctx-&gt;dispatch链表，正常情况下它用不到。只有在req派发时，发现磁盘驱动硬件繁忙，暂时没时间处理该req，又不能不管，只能先把req添加到硬件队列hctx-&gt;dispatch链表。接着执行blk_mq_run_hw_queue类的函数，该函数会在磁盘驱动硬件空闲时，从hctx-&gt;dispatch链表取出刚才没来得及派发的req，再次尝试派发给磁盘设备驱动。 软件队列ctx-&gt;rq_list链表，向该链表插入req的情况是：发起bio请求过程的blk_mq_make_request()-&gt;blk_mq_queue_io()；执行blk_mq_sched_insert_requests派发req时：情况1，如果硬件队列繁忙或者使用了调度器或者异步派发，不能执行blk_mq_try_issue_list_directly()直接将req派发给设备驱动的情况下，那就执行blk_mq_insert_requests()将派发req的临时list链表上的req插入到软件队列ctx-&gt;rq_list链表；情况2，硬件队列空闲且没有使用调度器且同步派发，则执行blk_mq_try_issue_list_directly()将临时list链表上的req派发给磁盘设备驱动。但派发过程遇到了磁盘驱动硬件繁忙，只能被迫返回，接着还是执行blk_mq_insert_requests()将list链表上剩下未派发的req插入到ctx-&gt;rq_list链表。之后执行blk_mq_run_hw_queue类函数，在磁盘驱动硬件空闲时，从软件队列ctx-&gt;rq_list链表取req，再次尝试派发给设备驱动。 IO调度算法队列的相关链表，这是执行设置了IO调度算法的情况下，肯定要先把派发的req插入的IO算法队列相关链表进行处理。它的派发见blk_mq_make_request()和blk_mq_sched_insert_requests()中调用的blk_mq_sched_insert_request函数。 req plug模式的plug-&gt;mq_list链表。就是当前进程集聚了很多bio在plug-&gt;mq_list，然后一下次全部派发给磁盘设备驱动。它的发起函数是blk_flush_plug_list。基本原理是，先取出plug-&gt;mq_list链表上的req，如果设置了IO调度器，则把req插入到IO算法队列。否则，则先执行blk_mq_try_issue_list_directly()将这些req直接派发给磁盘设备驱动。如果无法直接派发给磁盘设备驱动，就先把req添加到软件队列ctx-&gt;rq_list链表，等稍后执行blk_mq_run_hw_queue类函数，再次尝试派发这些req。 原文链接：https://blog.csdn.net/hu1610552336/article/details/111464548]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[lmbench安装与使用]]></title>
    <url>%2Funcategorized%2Flmbench%2F</url>
    <content type="text"><![CDATA[1.概述lmbench是个用于评价系统综合性能的多平台开源benchmark，能够测试包括文档读写、内存操作、进程创建销毁开销、网络等性能，测试 方法简单。Lmbench是个多平台软件，因此能够对同级别的系统进行比较测试，反映不同系统的优劣势，通过选择不同的库函数我们就能够比较库函数的性能；更为重要的是，作为一个开源软件， lmbench提供一个测试框架，假如测试者对测试项目有更高的测试需要，能够通过少量的修改源代码达到目的。 2.安装配置lmbench从官网下载http://www.bitmover.com/lmbench/get_lmbench.html lmbench3并且解压。 szp@szp-pc:~/lmbench3$ ls -l 总用量 76 -r--r--r-- 1 szp szp 1779 8月 23 2005 ACKNOWLEDGEMENTS drwxrwxr-x 4 szp szp 4096 4月 26 16:17 bin -r--r--r-- 1 szp szp 3695 8月 23 2005 CHANGES -r--r--r-- 1 szp szp 17982 8月 23 2005 COPYING -r--r--r-- 1 szp szp 5728 8月 23 2005 COPYING-2 drwxrwxr-x 2 szp szp 4096 8月 23 2005 doc -r--r--r-- 1 szp szp 9935 8月 23 2005 hbench-REBUTTAL -r--r--r-- 1 szp szp 1607 8月 23 2005 Makefile -r--r--r-- 1 szp szp 561 8月 23 2005 README drwxrwxr-x 2 szp szp 4096 8月 23 2005 results drwxrwxr-x 2 szp szp 4096 8月 23 2005 scripts drwxrwxr-x 2 szp szp 4096 4月 26 16:23 src szp@szp-pc:~/lmbench3$ 进入目录下的src文件夹，执行make 命令。 szp@szp-pc:~/lmbench3/src$ make results make[1]: 进入目录“/home/szp/lmbench3/src” gcc -O -DRUSAGE -DHAVE_uint=1 -DHAVE_int64_t=1 -DHAVE_DRAND48 -DHAVE_SCHED_SETAFFINITY=1 -o ../bin/x86_64-linux-gnu/disk disk.c ../bin/x86_64-linux-gnu/lmbench.a -lm /usr/bin/ld: /tmp/cc3ZbIZp.o: in function `seekto&apos;: disk.c:(.text+0x37): undefined reference to `llseek&apos; collect2: error: ld returned 1 exit status make[1]: *** [Makefile:323：../bin/x86_64-linux-gnu/disk] 错误 1 make[1]: 离开目录“/home/szp/lmbench3/src” make: *** [Makefile:114：lmbench] 错误 2 出现错误，在disk.c程序中找不到llseek，原因是llseek接口函数在GCC 7.3.0编译器中被lseek64替代。将测试工具的disk.c源码文件中的llseek接口函数全部替换成lseek64，重新编译。接下来lmbench会让用户配置测试文件。可以选择缺省值，详细解释如下。 如果编译没有错误，就会出现一些选择提示以对测试进行一个配置并生成配置脚本，首先出现的如下： If you are running on an MP machine and you want to try runningmultiple copies of lmbench in parallel, you can specify how many here.Using this option will make the benchmark run 100x slower (sorry).NOTE: WARNING! This feature is experimental and many results areknown to be incorrect or random! MULTIPLE COPIES [default 1] 提示你同时运行多少个并行的测试，对应为结果中的 scal load 项接下来是选择作业调度控制方法，选 1 允许调度作业即可。如下： Options to control job placement1) Allow scheduler to place jobs2) Assign each benchmark process with any attendent child processes to its own processor3) Assign each benchmark process with any attendent child processes to its own processor, except that it will be as far as possible from other processes4) Assign each benchmark and attendent processes to their own processors5) Assign each benchmark and attendent processes to their own processors, except that they will be as far as possible from each other and other processes6) Custom placement: you assign each benchmark process with attendent child processes to processors7) Custom placement: you assign each benchmark and attendent processes to processorsNote: some benchmarks, such as bw_pipe, create attendent childprocesses for each benchmark process. For example, bw_pipeneeds a second process to send data down the pipe to be readby the benchmark process. If you have three copies of thebenchmark process running, then you actually have six processes;three attendent child processes sending data down the pipes andthree benchmark processes reading data and doing the measurements.Job placement selection: 1 接下来是指定内存，本次指定为 512M，如下。 Several benchmarks operate on a range of memory. This memory should besized such that it is at least 4 times as big as the external cache[s]on your system. It should be no more than 80% of your physical memory.The bigger the range, the more accurate the results, but larger sizestake somewhat longer to run the benchmark.MB [default 2814]512 选择运行的子集，默认为全部，本次使用默认值，如下。 lmbench measures a wide variety of system performance, and the full suiteof benchmarks can take a long time on some platforms. Consequently, weoffer the capability to run only predefined subsets of benchmarks, onefor operating system specific benchmarks and one for hardware specificbenchmarks. We also offer the option of running only selected benchmarkswhich is useful during operating system development.Please remember that if you intend to publish the results you either needto do a full run or one of the predefined OS or hardware subsets.SUBSET (ALL|HARWARE|OS|DEVELOPMENT) [default all] 然后测试开始，需要等待一些时间。 Confguration done, thanks.There is a mailing list for discussing lmbench hosted at BitMover.Send mail to majordomo@bitmover.com to join the list.Using config in CONFIG.user3-VT3456-8614CMB Latency measurements Calculating file system latency Local networking Bandwidth measurements 3.查看测试结果执行make see命令可以查看测试结果。 szp@szp-pc:~/lmbench3$ make see cd results &amp;&amp; make summary percent 2&gt;/dev/null | more make[1]: 进入目录“/home/szp/lmbench3/results” L M B E N C H 3 . 0 S U M M A R Y ------------------------------------ (Alpha software, do not distribute) Basic system parameters ------------------------------------------------------------------------------ Host OS Description Mhz tlb cache mem scal pages line par load bytes --------- ------------- ----------------------- ---- ----- ----- ------ ---- szp-pc Linux 5.8.0-4 x86_64-linux-gnu 64 1 Basic integer operations - times in nanoseconds - smaller is better ------------------------------------------------------------------- Host OS intgr intgr intgr intgr intgr bit add mul div mod --------- ------------- ------ ------ ------ ------ ------ szp-pc Linux 5.8.0-4 0.2200 0.9300 6.1200 4.3200 Basic float operations - times in nanoseconds - smaller is better ----------------------------------------------------------------- Host OS float float float float add mul div bogo --------- ------------- ------ ------ ------ ------ szp-pc Linux 5.8.0-4 0.9300 0.9300 3.2000 1.2000 Basic double operations - times in nanoseconds - smaller is better ------------------------------------------------------------------ Host OS double double double double add mul div bogo --------- ------------- ------ ------ ------ ------ szp-pc Linux 5.8.0-4 0.9300 1.2300 4.0600 1.3800 Memory latencies in nanoseconds - smaller is better (WARNING - may not be correct, check graphs) ------------------------------------------------------------------------------ Host OS Mhz L1 $ L2 $ Main mem Rand mem Guesses --------- ------------- --- ---- ---- -------- -------- ------- user2-VT3 Linux 3.8.0-c 1598 2.5060 12.5 56.5 143.7 make[1]: 离开目录“/home/szp/lmbench3/results” 主要参数说明： Memory latencies（内存操作延时） L1：L1 缓存操作延时 L2：L2 缓存操作延时 Main Mem：系统内存连续操作延时 Rand Mem：系统内存随机访问操作延时如果想继续使用上次配置的结果，使用make rerun命令。此外，lmbench测试程序运行的时间较长，需要耐心等待。]]></content>
  </entry>
  <entry>
    <title><![CDATA[pageCache和bufferCache]]></title>
    <url>%2FLinux%2FpageCache%E5%92%8CbufferCache%2F</url>
    <content type="text"><![CDATA[1.前言PageCache又称页高速缓存，页高速缓存是由内存中的物理页面组成，对应的是磁盘上的物理块，可以动态扩大缩小。Linux支持的文件大小可以达到TB级别，访问较大文件时高速缓存中会存储属于该文件大量的页，因此采用struct address_space对页高速缓存进行管理，同时设计了基树结构来加速对缓存中页的查找。 早期的Linux内核版本中，同时存在PageCache和BufferCache，前者用于缓存对文件操作的内容，后者用于缓存直接对块设备操作的内容。page cache按照文件的逻辑页进行缓冲，buffer cache按照文件的物理块进行缓冲。在有文件系统的情况下，对文件操作，那么数据会缓存到page cache，如果直接采用dd等工具对磁盘进行读写，那么数据会缓存到buffer cache。由于这两种缓存处于半独立的状态，缺乏集成导致整体性能下降和缺乏灵活性。在内核版本2.4之后，对Page Cache、Buffer Cache的实现进行了融合，融合后的Buffer Cache不再以独立的形式存在，Buffer Cache的内容，直接存在于Page Cache中，同时，保留了对Buffer Cache的描述符单元：buffer_head。 2. address_space结构体address_space负责管理页面高速缓存，索引节点（inode）的i_mapping字段总是指向索引节点的数据页所有者的address_space对象。address_space对象的host字段指向其所有者的索引节点对象。 123456789101112131415161718struct address_space &#123; struct inode *host; /* owner: inode, block_device 指向拥有该对象的索引节点的指针*/ struct radix_tree_root page_tree; /* radix tree of all pages 表示拥有者页的基数radix tree 的根*/ spinlock_t tree_lock; /* and lock protecting it 保护基树的自旋锁 */ atomic_t i_mmap_writable;/* count VM_SHARED mappings 地址空间中共享内存映射的个数*/ struct rb_root i_mmap; /* tree of private and shared mappings radix优先搜索树的根 */ struct rw_semaphore i_mmap_rwsem; /* protect tree, count, list */ /* Protected by tree_lock together with the radix tree */ unsigned long nrpages; /* number of total pages */ unsigned long nrshadows; /* number of shadow entries */ pgoff_t writeback_index;/* writeback starts here */ const struct address_space_operations *a_ops; /* methods */ unsigned long flags; /* error bits/gfp mask */ spinlock_t private_lock; /* for use by the address_space */ struct list_head private_list; /* ditto */ void *private_data; /* ditto */&#125; 内核可以通过基树索引结构快速判断所需要的页是否在页高速缓存中。当查找所需要的页时，内核把页索引转换为基树的路径，并快速找到页描述符所在位置。如果找到，内核可以从基树获得页描述符，并且确定所找到的页是否是脏页，以及其数据是否正在使用。一个address_space代表一个基树，用page_tree来关联树根节点。 基树的叶子节点为page结构体，中间节点为代表基树节点的radix_tree_node。基树每个节点可以有多到64个指针指向其他节点或页描述符。每个节点由radix_tree_node数据结构表示， slots是包含64个指针的数组，count是记录节点中非空指针数量的计数器，tags是二维的标志数组。树根由radix_tree_root数据结构表示，gfp_mask指定新节点请求内存时所用的标志，rnode指向与树中第一层节点相应的数据结构radix_tree_node。 若基树深度为1，则只能表示从0至63的索引，则页索引（上文提高的index字段）的低6位进行解析，从而对应成radix_tree_node结构中的slot下标，找到对应的页；若基树深度为2，则页索引的低12位分成05,611两个字段进行解析。分别找到第一层slot字段和第二层slot字段的值。 3. bufferCachebufferCache存放在叫做“缓冲区页”的专门页中，并且使用称为bufferHead的缓冲区头部来描述一个页中的数据块在磁盘中的位置。 1234567891011121314151617181920212223struct buffer_head &#123; //缓冲区状态标志 unsigned long b_state; /* buffer state bitmap (see above) */ //指向缓冲区页的链表中的下一个元素的指针 struct buffer_head *b_this_page;/* circular list of page's buffers */ //指向拥有该块的缓冲区页的指针。 struct page *b_page; /* the page this bh is mapped to */ //与块设备相关的块号（逻辑块号） sector_t b_blocknr; /* start block number */ //块大小 size_t b_size; /* size of mapping */ //块在缓冲区页内的位置 char *b_data; /* pointer to data within the page */ //指向块设备描述符的指针 struct block_device *b_bdev; //I/O完成方法 bh_end_io_t *b_end_io; /* I/O completion */ void *b_private; /* reserved for b_end_io */ struct list_head b_assoc_buffers; /* associated with another mapping */ struct address_space *b_assoc_map; /* mapping this buffer is associated with */ atomic_t b_count; /* users using this buffer_head */&#125;; 融合后的buffer和cache如下图所示。 一个具体的文件在打开后，内核会在内存中为之建立一个struct inode结构（该inode结构也会在对应的file结构体中引用），其中的i_mapping域指向一个address_space结构。这样，一个文件就对应一个address_space结构，一个 address_space与一个偏移量能够确定一个page cache 或swap cache中的一个页面。 因此，当要寻址某个数据时，很容易根据给定的文件及数据在文件内的偏移量而找到相应的页面。address_space_operations 就是用来操作该文件映射到内存的页面，比如把内存中的修改写回文件、从文件中读入数据到页面缓冲等。 file结构体和inode结构体中都有一个address_space结构体指针，实际上，file-&gt;f_mapping是从对应inode-&gt;i_mapping而来,inode-&gt;i_mapping-&gt;a_ops是由对应的文件系统类型在生成这个inode时赋予的。 address_space_operations 包含以下方法。 writepage 写操作，从页写到所有者的磁盘映像 readpage 读操作，从所有者的磁盘映像读到页 sync_page 如果对所有者页进行的操作已准备好，则立刻开始I/O数据的传输 writepages 把指定数量的所有者脏页写回磁盘。 set_page_dirty 把所有者的页设置为脏页 readpages 从磁盘中读所有者页的链表 prepare_write 为写操作做准备（由磁盘文件系统使用） commit_write 完成写操作（由磁盘文件系统使用） bmap 从文件块索引中获取逻辑块号 invalidatepage 是所有者的页无效（截断文件时使用） releasepage 由日志文件系统使用以准备释放页 direct_IO 所有者页的直接I/O传输 （绕过了页高速缓存 page cache） 操作普通文件和操作设备文件过程如下图所示。 4. Cache 预读文件Cache可以看做是内存管理系统与文件系统之间的联系纽带。因此，文件Cache管理是操作系统的一个重要组成部分，它的性能直接影响着文件系统和内存管理系统的性能。大多数磁盘I/O读写都是顺序的，且普通文件在磁盘上的存储都是占用连续的扇区。这样读写文件时，就可以减少磁头的移动次数，提升读写性能。当程序读一个文件时，它通常从第一字节到最后一个字节顺序访问。因此，同一个文件中磁盘上多个相邻的扇区通常会被读进程都访问。 预读（read ahead）就是在数据真正被访问之前，从普通文件或块设备文件中读取多个连续的文件页面到内存中。多数情况下，内核的预读机制可以明显提高磁盘性能，因为减少了磁盘控制器处理的命令数，每个命令读取多个相邻扇区。此外，预读机制还提高了系统响应时间。 当然，在进程大多数的访问是随机读时，预读是对系统有害的，因为它浪费了内核Cache 空间。当内核确定最近常用的I/O访问不是顺序的时，就会减少或关闭预读。 预读(read-ahead)算法预测即将访问的页面，并提前把它们批量的读入缓存。 它的主要功能和任务包括: 批量：把小I/O聚集为大I/O，以改善磁盘的利用率，提升系统的吞吐量。 提前：对应用程序隐藏磁盘的I/O延迟，以加快程序运行。 预测：这是预读算法的核心任务。 前两个功能的达成都有赖于准确的预测能力。当前包括Linux、FreeBSD和Solaris等主流操作系统都遵循了一个简单有效的原则: 把读模式分为随机读和顺序读两大类，并只对顺序读进行预读。这一原则相对保守，但是可以保证很高的预读命中率，同时有效率/覆盖率也很好。因为顺序读是最简单而普遍的，而随机读在内核来说也确实是难以预测的。 以dd程序读一个裸设备为例，每次读1K数据，连续读16次： szp@szp-pc:~$ sudo dd if=/dev/sdb of=/dev/zero bs=1K count=16 [sudo] szp 的密码： 记录了16+0 的读入 记录了16+0 的写出 16384 bytes (16 kB, 16 KiB) copied, 0.000382103 s, 42.9 MB/s blktrace工具记录情况如下： szp@szp-pc:~$ sudo blktrace -d /dev/sdb -o - | blkparse -i - 8,16 1 1 0.000000000 2453 Q RA 0 + 32 [dd] 8,16 1 2 0.000006462 2453 G RA 0 + 32 [dd] 8,16 1 3 0.000007073 2453 P N [dd] 8,16 1 4 0.000008215 2453 U N [dd] 1 8,16 1 5 0.000008967 2453 I RA 0 + 32 [dd] 8,16 1 6 0.000013205 2453 D RA 0 + 32 [dd] 8,16 1 7 0.000094959 2453 C RA 0 + 32 [0] 8,16 1 8 0.000124926 2453 Q RA 32 + 64 [dd] 8,16 1 9 0.000125988 2453 G RA 32 + 64 [dd] 8,16 1 10 0.000126169 2453 P N [dd] 8,16 1 11 0.000126399 2453 U N [dd] 1 8,16 1 12 0.000126639 2453 I RA 32 + 64 [dd] 8,16 1 13 0.000127681 2453 D RA 32 + 64 [dd] 8,16 1 14 0.000189909 2453 C RA 32 + 64 [0] 第一个字段：8,16 这个字段是设备号 major device ID和minor device ID。 第二个字段：1 表示CPU 第三个字段：14 序列号 第四个字段：0.000189909 Time Stamp是时间偏移 第五个字段：PID 本次IO对应的进程ID 第六个字段：Event，这个字段非常重要，反映了IO进行到了哪一步 第七个字段：R表示 Read， W是Write，D表示block，B表示Barrier Operation 第八个字段：223490+56，表示的是起始block number 和 number of blocks，即我们常说的Offset 和 Size 第九个字段： 进程名 简单介绍blktrace工具，blktrace 结合btt可以统计一个IO是在调度队列停留的时间长，还是在硬件上消耗的时间长，利用这个工具可以协助分析和优化问题，可以更好的追踪IO的过程。 其中第六个字段非常有用：每一个字母都代表了IO请求所经历的某个阶段。 Q – 即将生成IO请求（bio）| G – IO请求（request）生成| I – IO请求进入IO Scheduler队列| D – IO请求进入driver| C – IO请求执行完毕 上面的输出可以简单解析为： Q ：dd程序派发的第一个bio进入通用块层，该bio大小为32个扇区，包括了dd总共需要读取的16k数据。 G ：获得一个request，用第一个bio初始化。 [P-C] : 第一个bio在通用块层后续的处理。 Q : dd程序派发的第二个bio进入通用块层，该bio大小为64个扇区，该bio为预读触发的请求。 G : 同样获得一个request，用第二个bio初始化。读操作同步进行，不会与前一个请求产生合并。 [P-C] : 第二个在通用块层的处理。 读操作是同步的，所以触发读请求的是dd进程本身。dd进程发起了16次读操作，总共读取16K数据，但是预读机制只向底层发送了两次读请求，分别为0+32(16K), 32+64(32K)，总共预读了16 + 32 = 48K数据，并保存到cache中，多预读的数据可以为后续的读操作服务。 内核判断两次读访问是顺序的标准是： 请求的第一个页面与上次访问的最后一个页面是相邻的。 访问一个给定的文件，预读算法使用两个页面集：当前窗口（current window）和前进窗口（ahead window）。 当前窗口（current window）包括了进程已请求的页面或内核提前读且在页面cache中的页面。（当前窗口中的页面未必是最新的，因为可能仍有I/O数据传输正在进行。） 前进窗口（ahead window）包含的页面是紧邻当前窗口（current window）中内核提前读的页面。前进窗口中的页面没有被进程请求，但内核假设进程迟早会访问这些页面。当内核判断出一个顺序访问和初始页面属于当前窗口时，就检查前进窗口是否已经建立起来。若未建立，内核建立一个新的前进窗口，并且为对应的文件页面触发读操作。 在理想状况下，进程正在访问的页面都在当前窗口中，而前进窗口中的文件页面正在传输。当进程访问的页面在前进窗口中时，前进窗口变为当前窗口。 5.预读数据结构与函数5.1 file_ra_state数据结构预读算法使用的主要数据结构是file_ra_state，每个文件对象都有一个f_ra域。其定义在文件include/linux/fs.h中。 123456789101112131415161718/* * Track a single file's readahead state */struct file_ra_state &#123; pgoff_t start; /* where readahead started 当前窗口的第一个页面*/ unsigned int size; /* # of readahead pages 当前窗口的页面数量，值为-1表示预读临时关闭，0表示当前窗口为空*/ unsigned int async_size; /* do asynchronous readahead when there are only # of pages ahead 异步预读页面数量*/ unsigned int ra_pages; /* Maximum readahead window 预读窗口最大页面数量。0表示预读暂时关闭。 */ unsigned int mmap_miss; /* Cache miss stat for mmap accesses 预读失效计数 */ loff_t prev_pos; /* Cache last read() position Cache中最近一次读位置*/&#125;; ra_pages表示当前窗口的最大页面数，也就是针对该文件的最大预读页面数；其初始值由该文件所在块设备上的backing_dev_info描述符中。进程可以通过系统调用 posix_fadvise（）来改变已打开文件的ra_pages值来调优预读算法。 5.2 generic_file_buffered_read读文件函数路径如下： 123456789read() -&gt;sys_read() -&gt;vfs_read() -&gt;__vfs_read() -&gt;ext4_file_read_iter() -&gt;generic_file_read_iter() -&gt;generic_file_buffered_read() -&gt;generic_file_buffered_read() -&gt;page_cache_async_readahead() 内核处理用户进程的读数据请求时，使用最多的是调用page_cache_sync_readahead（）和page_cache_async_readahead（）函数来执行预读。 1234567891011121314151617181920212223242526272829303132333435static ssize_t generic_file_buffered_read(struct kiocb *iocb, struct iov_iter *iter, ssize_t written)&#123; struct file *filp = iocb-&gt;ki_filp; struct address_space *mapping = filp-&gt;f_mapping; struct inode *inode = mapping-&gt;host; struct file_ra_state *ra = &amp;filp-&gt;f_ra; loff_t *ppos = &amp;iocb-&gt;ki_pos; ... //检查页是否已经包含在页缓存中 page = find_get_page(mapping, index); if (!page) &#123; if (iocb-&gt;ki_flags &amp; IOCB_NOWAIT) goto would_block; page_cache_sync_readahead(mapping, ra, filp, index, last_index - index); //执行完同步预读后，再次检查是否已在页缓存 page = find_get_page(mapping, index); if (unlikely(page == NULL)) goto no_cached_page; &#125; if (PageReadahead(page)) &#123;//检测页标志是否设置了PG_readahead，启动异步预读 page_cache_async_readahead(mapping, ra, filp, page, index, last_index - index); &#125; if (!PageUptodate(page)) &#123;//检查数据是不是最新的 if (iocb-&gt;ki_flags &amp; IOCB_NOWAIT) &#123; put_page(page); goto would_block; &#125; ...&#125; do_generic_file_read（）首先调用find_get_page（）检查页是否已经包含在页缓存中。如果没有则调用page_cache_sync_readahead（）发出一个同步预读请求。预读机制很大程度上能够保证数据已经进入缓存，因此再次调用find_get_page（）查找该页。这次仍然有一定失败的概率，那么就跳转到标号no_cached_page处直接进行读取操作。检测页标志是否设置了PG_readahead，如果设置了该标志就调用page_cache_async_readahead（）启动一个异步预读操作，这与前面的同步预读操作不同，这里并不等待预读操作的结束。虽然页在缓存中了，但是其数据不一定是最新的，这里通过PageUptodate(page)来检查。如果数据不是最新的，则调用函数mapping-&gt;a_ops-&gt;readpage（）进行再次数据读取。 5.3 page_cache_sync_readahead（）page_cache_sync_readahead（）函数就是同步预读一些页面到内存中。page_cache_sync_readahead（）它重新装满当前窗口和前进窗口，并根据预读命中率来更新窗口大小。函数有5个参数： mapping：文件拥有者的addresss_space对象 ra：包含此页面的文件file_ra_state描述符 filp：文件对象 offset：页面在文件内的偏量 req_size：完成当前读操作需要的页面数 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * page_cache_sync_readahead - generic file readahead * @mapping: address_space which holds the pagecache and I/O vectors * @ra: file_ra_state which holds the readahead state * @filp: passed on to -&gt;readpage() and -&gt;readpages() * @offset: start offset into @mapping, in pagecache page-sized units * @req_size: hint: total size of the read which the caller is performing in * pagecache pages * * page_cache_sync_readahead() should be called when a cache miss happened: * it will submit the read. The readahead logic may decide to piggyback more * pages onto the read request if access patterns suggest it will improve * performance. 同步预读一些页面到内存中。 mapping：文件拥有者的addresss_space对象 ra：包含此页面的文件file_ra_state描述符 filp：文件对象 offset：页面在文件内的偏量 req_size：完成当前读操作需要的页面数 */void page_cache_sync_readahead(struct address_space *mapping, struct file_ra_state *ra, struct file *filp, pgoff_t offset, unsigned long req_size)&#123; /* no read-ahead */ if (!ra-&gt;ra_pages) return; /* be dumb 当文件模式设置FMODE_RANDOM时，表示文件预期为随机访问,这种情形比较少见。*/ if (filp &amp;&amp; (filp-&gt;f_mode &amp; FMODE_RANDOM)) &#123; force_page_cache_readahead(mapping, filp, offset, req_size); return; &#125; /* do read-ahead */ ondemand_readahead(mapping, ra, filp, false, offset, req_size);&#125;EXPORT_SYMBOL_GPL(page_cache_sync_readahead); 当文件模式设置FMODE_RANDOM时，表示文件预期为随机访问。这种情形比较少见，这里不关注。函数变成对ondemand_readahead（）封装。 page_cache_async_readahead（）源码也在mm/readahead.c文件中，异步提前读取 多个页面到内存中。 函数共6个参数，比page_cache_sync_readahead（）多一个参数page。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * page_cache_async_readahead - file readahead for marked pages * @mapping: address_space which holds the pagecache and I/O vectors * @ra: file_ra_state which holds the readahead state * @filp: passed on to -&gt;readpage() and -&gt;readpages() * @page: the page at @offset which has the PG_readahead flag set * @offset: start offset into @mapping, in pagecache page-sized units * @req_size: hint: total size of the read which the caller is performing in * pagecache pages * * page_cache_async_readahead() should be called when a page is used which * has the PG_readahead flag; this is a marker to suggest that the application * has used up enough of the readahead window that we should start pulling in * more pages. 异步提前读取多个页面到内存中。 */voidpage_cache_async_readahead(struct address_space *mapping, struct file_ra_state *ra, struct file *filp, struct page *page, pgoff_t offset, unsigned long req_size)&#123; /* no read-ahead 是否需要预读*/ if (!ra-&gt;ra_pages) return; /* 页面处于回写状态 * Same bit is used for PG_readahead and PG_reclaim. */ if (PageWriteback(page)) return; ClearPageReadahead(page);//清除页面预读标志PG_ read ahed。 /* * Defer asynchronous read-ahead on IO congestion.检查当前磁盘I/O是否处于拥塞状态 */ if (inode_read_congested(mapping-&gt;host)) return; /* do read-ahead */ ondemand_readahead(mapping, ra, filp, true, offset, req_size);&#125;EXPORT_SYMBOL_GPL(page_cache_async_readahead); 若不需要预读或者页面处于回写状态，就直接返回。通过前面的检查后，就清除页面PG_ read ahed。在执行预读前，还要检查当前磁盘I/O是否处于拥塞状态，若处于拥塞就不能再进行预读。接下来就调用ondemand_readahead（）真正执行预读。 5.5 ondemand_readahead（）预读算法ondemand_readahead（）函数实现在文件mm/readahead.c。该函数主要根据file_ra_state描述符中的成员变量来执行一些动作。 （1）首先判断如果是从文件头开始读取的，初始化预读信息。默认设置预读为4个page。 （2）如果不是文件头，则判断是否连续的读取请求，如果是则扩大预读数量。一般等于上次预读数量x2。 （3）否则就是随机的读取，不适用预读，只读取sys_read请求的数量。 （4）然后调用ra_submit提交读取请求。 1234567891011121314151617181920212223242526272829303132333435static unsigned longondemand_readahead(struct address_space *mapping, struct file_ra_state *ra, struct file *filp, bool hit_readahead_marker, pgoff_t offset, unsigned long req_size)&#123; ...initial_readahead: ra-&gt;start = offset; //计算初始预读窗口大小 ra-&gt;size = get_init_ra_size(req_size, max_pages); ra-&gt;async_size = ra-&gt;size &gt; req_size ? ra-&gt;size - req_size : ra-&gt;size;readit: /* * Will this read hit the readahead marker made by itself? * If so, trigger the readahead marker hit now, and merge * the resulted next readahead window into the current one. * Take care of maximum IO pages as above. */ if (offset == ra-&gt;start &amp;&amp; ra-&gt;size == ra-&gt;async_size) &#123; //计算下一个预读窗口大小。 add_pages = get_next_ra_size(ra, max_pages); if (ra-&gt;size + add_pages &lt;= max_pages) &#123; ra-&gt;async_size = add_pages; ra-&gt;size += add_pages; &#125; else &#123; ra-&gt;size = max_pages; ra-&gt;async_size = max_pages &gt;&gt; 1; &#125; &#125; return ra_submit(ra, mapping, filp);&#125; get_init_ra_size（）计算初始预读窗口大小，get_next_ra_size（）计算下一个预读窗口大小。 当进程第一次访问文件并且请求的第一个页面在文件内的偏移量是0时， ondemand _readahead（）函数会认为进程会进行顺序访问文件。 于是函数从第一个页面开始创建新的当前窗口。当前窗口的初始值大小一般是2的幂次方，通常与进程第一次读取的页面数有关：请求的页面数越多，当前窗口越大，最大值保存在ra-&gt;ra_pages中。 相反地，进程第一次访问文件，但请求的页面在文件内的偏移量不是0时，内核就认为进程不会进行顺序访问。这样暂时关闭预读功能（设置ra-&gt;size的值为-1）。 然而内核重新发现顺序访问文件时，就会启用预读，创建新的当前窗口。若前进窗口（ahead window）不存在，当函数意识到进程在当前窗口进行顺序访问时，就会创建新的前进窗口。前进窗口的起始页面通常是紧邻当前窗口的最后一个页面。前进窗口的大小与当前窗口大小有关。一旦函数发现文件访问不是顺序的（根据前一次的访问），当前窗口和前进窗口就会被清空且预读功能被暂时关闭。当发现顺序访问时，就会重新启用预读。 ra_submit（）仅是对__do_page_cache_readahead（）的封装。 5.6 __do_page_cache_readahead()__do_page_cache_readahead（）有4个参数： mapping：文件拥有者的addresss_space对象 filp：文件对象 offset：页面在文件内的偏移量 nr_to_read：完成当前读操作需要的页面数 lookahead_size：异步预读大小 1234567891011121314151617181920212223242526272829303132333435363738394041424344int __do_page_cache_readahead(struct address_space *mapping, struct file *filp, pgoff_t offset, unsigned long nr_to_read, unsigned long lookahead_size)&#123; struct inode *inode = mapping-&gt;host; struct page *page; unsigned long end_index; /* The last page we want to read */ LIST_HEAD(page_pool); int page_idx; int ret = 0; loff_t isize = i_size_read(inode); gfp_t gfp_mask = readahead_gfp_mask(mapping); if (isize == 0) goto out; end_index = ((isize - 1) &gt;&gt; PAGE_SHIFT); /* * Preallocate as many pages as we will need.再次检查页面是否已经被其他进程读进内存，如果没有申请页面 */ for (page_idx = 0; page_idx &lt; nr_to_read; page_idx++) &#123; pgoff_t page_offset = offset + page_idx; if (page_offset &gt; end_index) break; rcu_read_lock(); page = radix_tree_lookup(&amp;mapping-&gt;page_tree, page_offset); rcu_read_unlock(); if (page &amp;&amp; !radix_tree_exceptional_entry(page)) continue; page = __page_cache_alloc(gfp_mask); if (!page) break; page-&gt;index = page_offset; list_add(&amp;page-&gt;lru, &amp;page_pool); //当分配到第nr_to_read ‐ lookahead_size个页面时，就设置该页面标志PG_readahead，以让下次进行异步预读。 if (page_idx == nr_to_read - lookahead_size) SetPageReadahead(page); ret++; &#125; 在从磁盘上读数据前，首先预分配一些内存页面，用来存放读取的文件数据。在预读过程中，可能有其他进程已经将某些页面读进内存，检因此在此检查页面是否已经在Cache中。若页面Cache中没有所请求的页面，则分配内存页面，并将页面加入到页面池中。 当分配到第nr_to_read ‐ lookahead_size个页面时，就设置该页面标志PG_readahead，以让下次进行异步预读。 页面准备好后，调用read_pages（）执行I/O操作，从磁盘读取文件数据。 5.7 readpagesext4文件系统的address_space_operations对象中的readpages方法实现为 ext4_readpages（）。若readpages方法没有定义，则readpage方法来每次读取一页。从方法名字，我们很容易看出两者的区别，readpages是一次可以读取多个页面，readpage 是每次只读取一个页面。两个方法实现上差别不大，我们以ext4文件系统为例，只考虑readpages方法。 ext4_readpages（）在文件fs/ext4/inode.c中。 12345678910111213static intext4_readpages(struct file *file, struct address_space *mapping, struct list_head *pages, unsigned nr_pages)&#123; struct inode *inode = mapping-&gt;host; /* If the file has inline data, no need to do readpages. */ if (ext4_has_inline_data(inode)) return 0; return ext4_mpage_readpages(mapping, pages, NULL, nr_pages);&#125; 整个函数调用流程如下图所示。 参考内容：https://blog.csdn.net/jinking01/article/details/107221428https://www.usenix.org/legacy/publications/library/proceedings/usenix2000/freenix/full_papers/silvers/silvers_html/index.htmlhttp://www.ilinuxkernel.com/files/Linux.Kernel.Cache.pdf]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSAPP cache lab(一)]]></title>
    <url>%2Funcategorized%2Fcsapp-cache-lab%2F</url>
    <content type="text"><![CDATA[1. CacheCache的内部组织结构如下图所示。Cache共有S组，每组E行，每行包括一个有效位(valid bit)，一个标签和B比特数据。当E=1时，称为直接映射，当E &gt; 1时，称为E路组相连。 内存地址由标签（tag）、组索引（set index）、块偏移（block offset）三个部分组成。假设指令长度为m，则t=m-b-s，Cache存储的数据总量为S*B*E。 缓存未命中的原因有以下三种： Cold (compulsory) miss 。首次访问cache必然导致未命中。 Conflict miss 。缓存还有大量的空余空间，但是多个数据对象被映射到相同的缓存块组，导致数据对象被交替换出。 Capacity miss。需要缓存的数据块超过了缓存的存储空间。 2. Cache 模拟程序程序要求： 不存储内存数据。 不使用块偏移量-地址中的b位不会被置位。 缓存模拟器需要在运行中指定的不同s，b，E工作。 使用LRU算法，从缓存中逐出最近最少使用的块，以便为下一个块腾出空间。 2.1 测试数据测试文件中的数据记载着每一次对内存的操作，前面的字母代表操作类型，统一的格式是: [空格][操作类型][空格][内存地址][逗号][大小] 其中如果第一个不是空格而是I，则代表加载，没有实际意义。 操作类型有以下三种： L：读取，从内存中读取S：存储，向内存中存储M：修改，这涉及一次读取，一次存储操作 地址指的是一个 64 位的 16 进制内存地址；大小表示该操作内存访问的字节数其中I指令无空格，M/S/L指令前有1个空格。 123456789101112131415I 04ead900,3I 04ead903,3I 04ead906,5I 04ead838,3I 04ead83b,3I 04ead83e,5 L 1ffefff968,8I 04ead843,3I 04ead846,3I 04ead849,5 L 1ffefff960,8I 04ead84e,3I 04ead851,3...... 实验指导提供了一个样例可执行文件csim-ref，我们要做的就是写出一个和它功能一样的程序。 执行一个示例，结果如下。 除此之外，实验指导还提供三个函数帮助完成实验，分别是getopt、fscanf、malloc 。 123456789101112131415161718//getoptint main(intargc, char** argv)&#123; int opt,x,y; /* looping over arguments */ while(-1 != (opt = getopt(argc, argv, "x:y:")))&#123; /* determine which argument it’s processing */ switch(opt) &#123; case 'x': x = atoi(optarg); break; case 'y': y = atoi(optarg); break; default: printf("wrong argument\n"); break; &#125; &#125; &#125; getopt用于获取命令行参数，fscanf 读入测试文件内容，malloc 分配空间给cache。 3. 代码实现3.1 Cache基本单元定义Cache基本单元为struct cache_line,不要求存储内存数据，所以没有设置块偏移量。 123456typedef struct&#123; unsigned long tag; /* 主存标记 */ int valid; /* 有效位 */ clock_t time_stamp; /* 时间戳实现LRU算法 */&#125;cache_line; 3.2 解析命令行参数123456789101112131415161718192021222324252627282930313233 while((input=getopt(argc,argv,"s:E:b:t:vh")) != -1) &#123; has_opt = 1; switch(input)&#123; case 's':s = atoi(optarg);break; case 'E':E = atoi(optarg);break; case 'b':b = atoi(optarg);break; case 't':t = optarg;break; case 'v':v = 1;break; case 'h':print_usage();exit(0); default:print_usage();exit(-1); &#125; &#125; if(!has_opt)&#123; printf("./csim: Missing required command line argument\n"); print_usage(); return 0; &#125; 3.3 缓存处理 使用位移解析内存地址中的组号和标签。 使用hit_or_miss函数读取缓存的数据，返回命中或者没有命中。 如果没有命中使用函数put_in_cache将该内存块放入缓存，并检查是否产生eviction。 如果缓存中有空间，将内存块直接放入，如果没有，使用LRU算法进行置换，并且标识产生了eviction。 统计所有的miss数量和eviction数量。 完整实验代码如下。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228#include "stdio.h"#include "stdlib.h"#include "string.h"#include "unistd.h"#include "getopt.h"#include "time.h"#include "cachelab.h"typedef struct&#123; unsigned long tag; /* 主存标记 */ int valid; /* 有效位 */ clock_t time_stamp; /* 时间戳实现LRU算法 */&#125;cache_line;cache_line** initiate(int set_index_bits, int associativity)&#123; /* 初始化 cache_line 二维数组 cache 并分配空间 */ int i; int sets = 1 &lt;&lt; set_index_bits; unsigned int size; cache_line** cache; cache = (cache_line** ) malloc(sizeof(cache_line) * sets); for(i=0;i&lt;sets;i++) &#123; size = sizeof(cache_line) * associativity; cache[i] = (cache_line* ) malloc(size); /* memset 初始化内容全为0*/ memset(cache[i], 0, size); &#125; return cache;&#125;int clean(cache_line** c, int set_index_bits)&#123; /* cache_line 二维数组的清理工作 */ int i; int sets = 1 &lt;&lt; set_index_bits; for(i=0;i&lt;sets;i++) &#123; free(c[i]); &#125; free(c); return 0;&#125;int hit_or_miss(cache_line* line, int line_length, unsigned long add_tag)&#123; /* hit: 1, miss: 0 */ int i, result = 0; for(i=0;i&lt;line_length;i++) &#123; if((add_tag == line[i].tag) &amp;&amp; (line[i].valid == 1)) &#123; result = 1; line[i].time_stamp = clock(); /* 如果 hit 的话,更新时间戳 */ break; &#125; &#125; return result;&#125;int put_in_cache(cache_line* line, int line_length, unsigned long add_tag)&#123; /* no eviction: 0, eviction: 1 */ int i, index = 0, result = 0; clock_t temp_stamp = line[0].time_stamp; /* 有空位就放入并结束程序 */ for(i=0;i&lt;line_length;i++) &#123; if(line[i].valid == 0) &#123; line[i].tag = add_tag; line[i].valid = 1; line[i].time_stamp = clock(); return result; &#125; &#125; /* 没有空位,那就比较时间戳,使用LRU算法 */ for(i=0;i&lt;line_length;i++) &#123; if(temp_stamp &gt; line[i].time_stamp) &#123; temp_stamp = line[i].time_stamp; index = i; &#125; &#125; result = 1; line[index].tag = add_tag; line[index].time_stamp = clock(); return result;&#125;void print_verbose(char* pre, char type, int hit_miss, int eviction)&#123; /* 命令行带 -v 的话的详细数据输出函数 */ char* h = hit_miss?" hit":" miss"; char* e = eviction?" eviction":""; char* format; if(type == 'M') &#123; format = "%s%s%s\n"; strcat(pre, format); printf(pre, h, e, " hit"); &#125; else &#123; format = "%s%s\n"; strcat(pre, format); printf(pre, h, e); &#125;&#125;int cache_access(cache_line** cache_sets, int s, int E, int b, int v, int bytes, int* hits, int* misses, int* evictions, unsigned long addr, char type)&#123; /* 缓存模拟核心逻辑 */ int hit_miss = 0, evictions_or_not = 0; char pre[20]; /* 索引地址位可以用位运算*/ unsigned long tag = addr &gt;&gt; (b + s), sets = ((addr &lt;&lt; (64 - b - s)) &gt;&gt; (64 -s)); cache_line* set = cache_sets[sets]; /* 尝试读取 */ hit_miss = hit_or_miss(set, E, tag); /* 如果没命中的话就把这个主存块放入缓存,观察是否有 eviction */ if(!hit_miss) &#123; evictions_or_not = put_in_cache(set, E, tag); &#125; /* 如果命令行带 -v 的话,打印详细信息 */ if(v) &#123; sprintf(pre, "%c %lx,%d", type, addr, bytes); print_verbose(pre, type, hit_miss, evictions_or_not); &#125; /* 统计工作 */ *hits += hit_miss; if(type=='M') &#123; *hits += 1; &#125; *misses += !hit_miss; *evictions += evictions_or_not; return 0;&#125;/* 当使用 ./csim -h 或错误的参数或没有参数时打印这个帮助信息 */void print_usage()&#123; printf("Usage: ./csim [-hv] -s &lt;num&gt; -E &lt;num&gt; -b &lt;num&gt; -t &lt;file&gt;\n"); printf("Options\n"); printf(" -h Print this help message.\n"); printf(" -v Optional verbose flag.\n"); printf(" -s &lt;num&gt;: Number of set index bits.\n"); printf(" -E &lt;num&gt;: Number of lines per set.\n"); printf(" -b &lt;num&gt;: Number of block offset bits.\n"); printf(" -t &lt;file&gt;: Trace file.\n"); printf("\n"); printf("Exampes:\n"); printf(" linux&gt; ./csim -s 4 -E 1 -b 4 -t traces/yi.trace\n"); printf(" linux&gt; ./csim -v -s 8 -E 2 -b 4 -t traces/yi.trace\n");&#125;int main(int argc, char** argv)&#123; unsigned long address; int s, E, b, bytes, has_opt = 0, hits = 0, misses = 0, evictions = 0, v = 0; char* t; char input, type; FILE* fp; cache_line** cache; /* 解析命令行参数 */ while((input=getopt(argc,argv,"s:E:b:t:vh")) != -1) &#123; has_opt = 1; switch(input)&#123; case 's': s = atoi(optarg); break; case 'E': E = atoi(optarg); break; case 'b': b = atoi(optarg); break; case 't': t = optarg; break; case 'v': v = 1; break; case 'h': print_usage(); exit(0); default: print_usage(); exit(-1); &#125; &#125; if(!has_opt)&#123; printf("./csim: Missing required command line argument\n"); print_usage(); return 0; &#125; /* 根据参数初始化二维数组 */ cache = initiate(s, E); /* 读入文件并解析 */ fp = fopen(t, "r"); if(fp == NULL) &#123; printf("%s: No such file or directory\n", t); exit(1); &#125; else &#123; while(fscanf(fp, " %c %lx,%d", &amp;type, &amp;address, &amp;bytes) != EOF) &#123; /* 'I' 类型的指令读取我们不关心 */ if(type == 'I') &#123; continue; &#125; else &#123; /* 得到详细参数,进入缓存模拟核心逻辑 */ cache_access(cache, s, E, b, v, bytes, &amp;hits, &amp;misses, &amp;evictions, address, type); &#125; &#125; fclose(fp); &#125; printSummary(hits, misses, evictions); /* 记得 free 掉给二维数组分配的堆空间 */ clean(cache, s); return 0;&#125; 测试程序运行效果。]]></content>
  </entry>
  <entry>
    <title><![CDATA[文件系统日志]]></title>
    <url>%2Funcategorized%2F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[1. 前言文件系统可能在任何两次写入之间崩溃或者断电，在一些必须更新两个磁盘上结构的特定操作场景下，如果其中一个首先到达磁盘，此时发生了崩溃，磁盘上的结构就会处于不一致的状态。崩溃后，系统重启并尝试重新挂载文件系统，如何确保磁盘上的结构保持在合理的状态，成为所有文件系统需要解决的问题。 古老的解决方案是通过文件系统检查程序（file system checker）,即fsck。该方案的思想是并不妨碍或者阻止磁盘不一致状态的产生，而是在崩溃后重启时检查并修复不一致状态。fsck是UNIX系统下的工具，用于查找并修复磁盘的不一致状态。fsck会通过检查超级块的合理性、扫描inode等一系列操作来确保当前磁盘的一致性。构建有效工作的fsck通常需要复杂的文件系统知识，而且对于大容量磁盘，扫描整个磁盘可能需要数小时。 更加通用的一种解决方案是日志记录（journaling,write-ahead logging）,该解决方案的思想是在磁盘中开辟一块空间用于记录日志，通过每次写入时增加一点记录写入内容的开销，阻止不一致状态的产生。许多现代文件系统如ext3、ext4、NTFS都使用该种解决方案。其中ext4确保数据一致性主要分为以下几步： 数据写入。首先将数据写入磁盘中的最终位置。 日志元数据写入。将本次写入事务的开始数据块和元数据写入日志。 日志提交。将事物提交数据块写入日志。 元数据写入磁盘。将元数据更新的内容写入磁盘最终位置。 释放。在日志的超级块中将事务标记为空闲。 其中先写入被指对象（本次写入的数据），然后写入指针对象（inode元数据），可以保证inode中的数据块地址永远不会指向垃圾数据。 2. 日志记录块设备（jbd）jbd是Linux内核中的通用块设备日志记录层，它独立于文件系统，ext3，ext4和OCFS2都在使用JBD，其中jbd2随着ext4文件系统在2006诞生。在Linux内核版本4.14中，jbd2共包含六个源代码文件，位于目录/fs/jbd2/下。 journal journal 在英文中有“日志”之意，在jbd中journal既是磁盘上日志空间的代表，又起到管理内存中为日志机制而创建的handle、transaction等数据结构的作用，可以说是整个机制的代表。 transaction jbd为了提高效率，将若干个handle组成一个事务，用transaction来表示。对日志读写来说，都是以transaction为单位的。在处理日志数据时，transaction具有原子性，即恢复时，如果一个transaction是完整的，其中包含的数据就可用于文件系统的恢复，否则，忽略不完整的transaction。 commit 所谓提交，就是把内存中transaction中的磁盘缓冲区中的数据写到磁盘的日志空间上。注意，jbd是将缓冲区中的数据另外写一份，写到日志上，原来的kernel将缓冲区写回磁盘的过程并没有改变。在内存中，transaction是可以有若干个的，而不是只有一个。transaction可分为三种，一种是已经commit到磁盘日志中的，它们正在进行checkpoint操作；第二种是正在将数据提交到日志的transaction；第三种是正在运行的transaction。正在运行的transaction管理随后发生的handle，并在适当时间commit到磁盘日志中。注意正在运行的transaction最多只可能有一个，也可能没有，如果没有，则handle提出请求时，则会按需要创建一个正在运行的transaction。 checkpoint 当一个transaction已经commit，那么，是不是在内存中它就没有用了呢？好像是这样，因为其中的数据已经写到磁盘日志中了。但是实际上不是这样的。主要原因是磁盘日志是个有限的空间，比如说100MB，如果一直提交transaction，很快就会占满，所以日志空间必须复用。其实与日志提交的同时，kernel也在按照自己以前的方式将数据写回磁盘。试想，如果一个transaction中包含的所有磁盘缓冲区的数据都已写回到磁盘的原来的位置上（不是日志中，而是在磁盘的原来的物理块上），那么，该transaction就没有用了，可以被删除了，该transaction在磁盘日志中的空间就可以被回收，进而重复利用了。 revoke 假设有一个缓冲区，对应着一个磁盘块，内核多次修改该缓冲区，于是磁盘日志中就会有该缓冲区的若干个版本的数据。假设此时要从文件中删除该磁盘块，那么，一旦包含该删除操作的transaction提交，那么，再恢复时，已经存放在磁盘日志中的该磁盘块的若干个版本的数据就不必再恢复了，因为到头来还是要删除的。revoke就是这样一种加速恢复速度的方法。当本transaction包含删除磁盘块操作时，就会在磁盘日志中写一个revoke块，该块中包含&lt;被revoked的块号blocknr，提交的transaction的ID&gt;，表示恢复时，凡是transaction ID小于等于ID的所有写磁盘块blocknr的操作都可以取消了，不必进行了。 recover 加入日志机制后，一旦系统崩溃，重新挂载分区时，就会检查该分区上的日志是否需要恢复。如果需要，则依次将日志空间的数据写回磁盘原始位置，则文件系统又重新处于一致状态了。 3. 核心代码打开journal.c文件，可以看到module_init(journal_init); module_exit(journal_exit);函数，应该是基于内核模块编写，来看它的初始化函数journal_init和卸载函数journal_exit。 123456789101112131415161718192021222324252627static int __init journal_init(void)&#123; int ret; //检查日志超级块的大小是否为1024，超级块中记录日志系统的静态属性信息和动态信息。 BUILD_BUG_ON(sizeof(struct journal_superblock_s) != 1024); //初始化缓存 ret = journal_init_caches(); if (ret == 0) &#123; //在proc/fs下创建jbd2目录 jbd2_create_jbd_stats_proc_entry(); &#125; else &#123; jbd2_journal_destroy_caches(); &#125; return ret;&#125;static void __exit journal_exit(void)&#123;#ifdef CONFIG_JBD2_DEBUG int n = atomic_read(&amp;nr_journal_heads); if (n) printk(KERN_ERR "JBD2: leaked %d journal_heads!\n", n);#endif jbd2_remove_jbd_stats_proc_entry(); jbd2_journal_destroy_caches();&#125; BUILD_BUG_ON宏定义检查了日志超级块（struct journal_superblock_s）的大小是否为1024，还不清楚什么情况下会不是1024。日志超级块保存了日志系统的静态属性信息和动态信息。 123456789101112131415161718192021222324252627282930/* * The journal superblock. All fields are in big-endian byte order. */typedef struct journal_superblock_s&#123;/* 0x0000 */ journal_header_t s_header;/* 0x000C */ /* Static information describing the journal */ __be32 s_blocksize; /* journal device blocksize journal所在设备的块大小 */ __be32 s_maxlen; /* total blocks in journal file 日志的长度，即包含多少个块*/ __be32 s_first; /* first block of log information 日志中的开始块号，初始化时置为1，非物理块号*//* 0x0018 */ /* Dynamic information describing the current state of the log */ __be32 s_sequence; /* first commit ID expected in log 日志中最旧的一个事务的ID*/ __be32 s_start; /* blocknr of start of log 日志开始的块号，与s_sequence组成环形结构，重复使用日志空间*//* 0x0020 */ /* Error value, as set by jbd2_journal_abort(). */ __be32 s_errno;.../* 0x0100 */ __u8 s_users[16*48]; /* ids of all fs'es sharing the log *//* 0x0400 */&#125; journal_superblock_t; 接下来journal_init_caches();初始化了日志系统需要的缓存。成功后，使用jbd2_create_jbd_stats_proc_entry();函数在proc文件系统下创建了jbds文件夹。这里使用了条件编译，在proc文件系统存在的情况下才使用proc_mkdir创建文件夹，否则执行空的while循环。 12345678910111213141516171819#ifdef CONFIG_PROC_FS#define JBD2_STATS_PROC_NAME "fs/jbd2"static void __init jbd2_create_jbd_stats_proc_entry(void)&#123; proc_jbd2_stats = proc_mkdir(JBD2_STATS_PROC_NAME, NULL);&#125;static void __exit jbd2_remove_jbd_stats_proc_entry(void)&#123; if (proc_jbd2_stats) remove_proc_entry(JBD2_STATS_PROC_NAME, NULL);&#125;#else#define jbd2_create_jbd_stats_proc_entry() do &#123;&#125; while (0)#define jbd2_remove_jbd_stats_proc_entry() do &#123;&#125; while (0) 4. 日志模式ext4支持三种日志模式，划分的依据是选择元数据块还是数据块写入日志，以及何时写入日志。 日志（journal） 文件系统所有数据块和元数据块的改变都记入日志。 这种模式减少了丢失每个文件所作修改的机会，但是它需要很多额外的磁盘访问。例如，当一个新文件被创建时，它的所有数据块都必须复制一份作为日志记录。这是最安全和最慢的ext4日志模式。 预定（ordered） 只对文件系统元数据块的改变才记入日志，这样可以确保文件系统的一致性，但是不能保证文件内容的一致性。然而，ext4文件系统把元数据块和相关的数据块进行分组，以便在元数据块写入日志之前写入数据块。这样，就可以减少文件内数据损坏的机会；例如，确保增大文件的任何写访问都完全受日志的保护。这是缺省的ext4日志模式。 写回（writeback） 只有对文件系统元数据的改变才记入日志，不对数据块进行任何特殊处理。这是在其他日志文件系统发现的方法，也是最快的模式。在挂载ext4文件系统时，可通过data=journal等修改日志模式。 5. 日志系统初始化ext4通过内核函数ext4_fill_super (fs/ext4/super.c)对每个分区的超级块进行初始化。 123456789/* * The first inode we look at is the journal inode. Don't try * root first: it may be modified in the journal! */if (!test_opt(sb, NOLOAD) &amp;&amp; ext4_has_feature_journal(sb)) &#123; err = ext4_load_journal(sb, es, journal_devnum); if (err) goto failed_mount3a;&#125; ext4_fill_super函数调用了ext4_load_journal函数装载日志系统，装载的方式分为目录和分区，ext4可以使用这两种方式来存储日志。 1234567891011121314151617181920212223242526static int ext4_load_journal(struct super_block *sb, struct ext4_super_block *es, unsigned long journal_devnum)&#123; journal_t *journal; unsigned int journal_inum = le32_to_cpu(es-&gt;s_journal_inum); dev_t journal_dev; int err = 0; int really_read_only; BUG_ON(!ext4_has_feature_journal(sb)); ... if (journal_inum) &#123; //采用目录 if (!(journal = ext4_get_journal(sb, journal_inum))) return -EINVAL; &#125; else &#123; //采用分区 if (!(journal = ext4_get_dev_journal(sb, journal_dev))) return -EINVAL; &#125; ... return 0;&#125; 函数ext4_get_journal用于初始化日志目录。 1234567891011121314151617181920212223static journal_t *ext4_get_journal(struct super_block *sb, unsigned int journal_inum)&#123; struct inode *journal_inode; journal_t *journal; BUG_ON(!ext4_has_feature_journal(sb)); journal_inode = ext4_get_journal_inode(sb, journal_inum); if (!journal_inode) return NULL; //初始化日志inode journal = jbd2_journal_init_inode(journal_inode); if (!journal) &#123; ext4_msg(sb, KERN_ERR, "Could not load journal inode"); iput(journal_inode); return NULL; &#125; journal-&gt;j_private = sb; ext4_init_journal_params(sb, journal); return journal;&#125; 其中函数jbd2_journal_init_inode用于初始化日志inode，函数被定义在fs/jbd2/journal.c。 1234567891011121314journal_t *jbd2_journal_init_inode(struct inode *inode)&#123; journal_t *journal; char *p; unsigned long long blocknr; blocknr = bmap(inode, 0); ... //初始化/proc/fs/jbd2目录中的文件 jbd2_stats_proc_init(journal); return journal;&#125; 调用jbd2_stats_proc_init函数初始化proc文件系统下jbd2目录中的文件。 123456789//初始化proc文件系统下/proc/fs/jbd2目录中的文件static void jbd2_stats_proc_init(journal_t *journal)&#123; journal-&gt;j_proc_entry = proc_mkdir(journal-&gt;j_devname, proc_jbd2_stats); if (journal-&gt;j_proc_entry) &#123; proc_create_data("info", S_IRUGO, journal-&gt;j_proc_entry, &amp;jbd2_seq_info_fops, journal); &#125;&#125; 使用proc_create_data创建了一个info文件，并且赋值了文件操作函数集jbd2_seq_info_fops给info文件。 jbd2_seq_info_fops定义了对文件的操作函数，如下： 12345678static const struct file_operations jbd2_seq_info_fops = &#123; .owner = THIS_MODULE, .open = jbd2_seq_info_open, .read = seq_read, .llseek = seq_lseek, .release = jbd2_seq_info_release,&#125;; 查看jbd2_seq_info_open函数， 1234567891011121314151617181920212223242526272829303132static int jbd2_seq_info_open(struct inode *inode, struct file *file)&#123; journal_t *journal = PDE_DATA(inode); struct jbd2_stats_proc_session *s; int rc, size; s = kmalloc(sizeof(*s), GFP_KERNEL); if (s == NULL) return -ENOMEM; size = sizeof(struct transaction_stats_s); s-&gt;stats = kmalloc(size, GFP_KERNEL); if (s-&gt;stats == NULL) &#123; kfree(s); return -ENOMEM; &#125; spin_lock(&amp;journal-&gt;j_history_lock); memcpy(s-&gt;stats, &amp;journal-&gt;j_stats, size); s-&gt;journal = journal; spin_unlock(&amp;journal-&gt;j_history_lock); rc = seq_open(file, &amp;jbd2_seq_info_ops); if (rc == 0) &#123; struct seq_file *m = file-&gt;private_data; m-&gt;private = s; &#125; else &#123; kfree(s-&gt;stats); kfree(s); &#125; return rc;&#125; 打开操作函数seq_open(file, &amp;jbd2_seq_info_ops)会初始化file结构体，然后将jbd2_seq_info_ops操作函数集赋值给file。 1234567static const struct seq_operations jbd2_seq_info_ops = &#123; .start = jbd2_seq_info_start, .next = jbd2_seq_info_next, .stop = jbd2_seq_info_stop, .show = jbd2_seq_info_show,&#125;; jbd2_seq_info_show（journal.c/833)这个函数的主要功能就是把日志的统计信息写到打开文件的private_data指向的seq_file里。 12345678910111213141516171819202122232425262728293031static int jbd2_seq_info_show(struct seq_file *seq, void *v)&#123; struct jbd2_stats_proc_session *s = seq-&gt;private; if (v != SEQ_START_TOKEN) return 0; seq_printf(seq, "%lu transaction, each upto %u blocks\n", s-&gt;stats-&gt;ts_tid, s-&gt;journal-&gt;j_max_transaction_buffers); if (s-&gt;stats-&gt;ts_tid == 0) return 0; seq_printf(seq, "average: \n %ums waiting for transaction\n", jiffies_to_msecs(s-&gt;stats-&gt;u.run.rs_wait / s-&gt;stats-&gt;ts_tid)); seq_printf(seq, " %ums running transaction\n", jiffies_to_msecs(s-&gt;stats-&gt;u.run.rs_running / s-&gt;stats-&gt;ts_tid)); seq_printf(seq, " %ums transaction was being locked\n", jiffies_to_msecs(s-&gt;stats-&gt;u.run.rs_locked / s-&gt;stats-&gt;ts_tid)); seq_printf(seq, " %ums flushing data (in ordered mode)\n", jiffies_to_msecs(s-&gt;stats-&gt;u.run.rs_flushing / s-&gt;stats-&gt;ts_tid)); seq_printf(seq, " %ums logging transaction\n", jiffies_to_msecs(s-&gt;stats-&gt;u.run.rs_logging / s-&gt;stats-&gt;ts_tid)); seq_printf(seq, " %lluus average transaction commit time\n", div_u64(s-&gt;journal-&gt;j_average_commit_time, 1000)); seq_printf(seq, " %lu handles per transaction\n", s-&gt;stats-&gt;u.run.rs_handle_count / s-&gt;stats-&gt;ts_tid); seq_printf(seq, " %lu blocks per transaction\n", s-&gt;stats-&gt;u.run.rs_blocks / s-&gt;stats-&gt;ts_tid); seq_printf(seq, " %lu logged blocks per transaction\n", s-&gt;stats-&gt;u.run.rs_blocks_logged / s-&gt;stats-&gt;ts_tid); return 0;&#125; 我们可以在proc下查看写入的信息如下： 那么文件系统如何通过cat读出这些信息的呢？ 上面提到的jbd2_seq_info_fops 文件操作集中有seq_read函数,使用cat读取的时候会调用该函数。 12345678static const struct file_operations jbd2_seq_info_fops = &#123; .owner = THIS_MODULE, .open = jbd2_seq_info_open, .read = seq_read, .llseek = seq_lseek, .release = jbd2_seq_info_release,&#125;; 这个函数从一个序列文件读出数据到用户空间。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161/* 一个序列文件读出数据到用户空间 * @file: 要读的文件指针，这里是/proc/fs/sda5-8/info * @buf: 用户空间的指针 * @size: 最大要读取的字节数，有可能少于这个数，函数返回值是实际读取数。 * @ppos: 从文件的什么地方开始读。 */ssize_t seq_read(struct file *file, char __user *buf, size_t size, loff_t *ppos)&#123; // 获得文件所包含的序列文件指针。 struct seq_file *m = (struct seq_file *)file-&gt;private_data; size_t copied = 0; loff_t pos; size_t n; void *p; int err = 0; mutex_lock(&amp;m-&gt;lock); // 在每个文件内有一个值记录着文件当前读取位置，如果与传进来的值不相等，就设成传进近来的值。 // 并把文件的位置调整到*ppos, seq_file与一般的文件不一样，改变当然的位置要重新生成一次文件 /* Don't assume *ppos is where we left it */ if (unlikely(*ppos != m-&gt;read_pos)) &#123; m-&gt;read_pos = *ppos; while ((err = traverse(m, *ppos)) == -EAGAIN) ; if (err) &#123; /* With prejudice... */ m-&gt;read_pos = 0; m-&gt;version = 0; m-&gt;index = 0; m-&gt;count = 0; goto Done; &#125; &#125; /* * seq_file-&gt;op-&gt;..m_start/m_stop/m_next may do special actions * or optimisations based on the file-&gt;f_version, so we want to * pass the file-&gt;f_version to those methods. * * seq_file-&gt;version is just copy of f_version, and seq_file * methods can treat it simply as file version. * It is copied in first and copied out after all operations. * It is convenient to have it as part of structure to avoid the * need of passing another argument to all the seq_file methods. */ m-&gt;version = file-&gt;f_version; // 如果文件还没有数据缓冲区，那就得先分配。 /* grab buffer if we didn't have one */ if (!m-&gt;buf) &#123; m-&gt;buf = kmalloc(m-&gt;size = PAGE_SIZE, GFP_KERNEL); if (!m-&gt;buf) goto Enomem; &#125; // 如果有数据还没刷到缓冲区，那就先flush到buf里，然后再拷贝到用户空间的缓冲区。 /* if not empty - flush it first */ if (m-&gt;count) &#123; n = min(m-&gt;count, size); err = copy_to_user(buf, m-&gt;buf + m-&gt;from, n); if (err) goto Efault; m-&gt;count -= n; m-&gt;from += n; size -= n; buf += n; copied += n; if (!m-&gt;count) m-&gt;index++; if (!size) goto Done; &#125; // 如果没有数据了，那就生成数据，其中m-&gt;op-&gt;show就是jbd2_seq_info_show函数， /* we need at least one record in buffer */ pos = m-&gt;index; p = m-&gt;op-&gt;start(m, &amp;pos); while (1) &#123; err = PTR_ERR(p); if (!p || IS_ERR(p)) break; err = m-&gt;op-&gt;show(m, p); if (err &lt; 0) break; if (unlikely(err)) m-&gt;count = 0; if (unlikely(!m-&gt;count)) &#123; p = m-&gt;op-&gt;next(m, p, &amp;pos); m-&gt;index = pos; continue; &#125; if (m-&gt;count &lt; m-&gt;size) goto Fill; m-&gt;op-&gt;stop(m, p); kfree(m-&gt;buf); m-&gt;buf = kmalloc(m-&gt;size &lt;&lt;= 1, GFP_KERNEL); if (!m-&gt;buf) goto Enomem; m-&gt;count = 0; m-&gt;version = 0; pos = m-&gt;index; p = m-&gt;op-&gt;start(m, &amp;pos); &#125; m-&gt;op-&gt;stop(m, p); m-&gt;count = 0; goto Done;Fill: /* they want more? let's try to get some more */ while (m-&gt;count &lt; size) &#123; size_t offs = m-&gt;count; loff_t next = pos; p = m-&gt;op-&gt;next(m, p, &amp;next); if (!p || IS_ERR(p)) &#123; err = PTR_ERR(p); break; &#125; err = m-&gt;op-&gt;show(m, p); if (m-&gt;count == m-&gt;size || err) &#123; m-&gt;count = offs; if (likely(err &lt;= 0)) break; &#125; pos = next; &#125; m-&gt;op-&gt;stop(m, p); n = min(m-&gt;count, size); err = copy_to_user(buf, m-&gt;buf, n); if (err) goto Efault; copied += n; m-&gt;count -= n; if (m-&gt;count) m-&gt;from = n; else pos++; m-&gt;index = pos;Done: if (!copied) copied = err; else &#123; *ppos += copied; m-&gt;read_pos += copied; &#125; file-&gt;f_version = m-&gt;version; mutex_unlock(&amp;m-&gt;lock); return copied;Enomem: err = -ENOMEM; goto Done;Efault: err = -EFAULT; goto Done;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[文件系统测试工具filebench]]></title>
    <url>%2Funcategorized%2F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7filebench%2F</url>
    <content type="text"><![CDATA[1.filebench安装首先检测是否安装了gcc： gcc –version 如果已经安装则执行： sudo apt-get install flex bison Ubuntu下安装filebench，首先下载安装包。 szp@szp-pc:~$ wget https://phoenixnap.dl.sourceforge.net/project/filebench/1.5-alpha3/filebench-1.5-alpha3.tar.gz 将filebench解压到/usr/local目录下。 szp@szp-pc:~$ sudo tar -zxf filebench-1.5-alpha3.tar.gz -C /usr/local 进入解压后的目录下， szp@szp-pc:~$ cd /usr/local/ 依次执行以下命令进行安装。 szp@szp-pc:/usr/local/filebench-1.5-alpha3$ sudo ./configure szp@szp-pc:/usr/local/filebench-1.5-alpha3$ sudo make szp@szp-pc:/usr/local/filebench-1.5-alpha3$ sudo make install 为了不影响filebench自带的负载文件，复制一份workloads下自带的工作负载文件，然后根据自身需要进行修改。 szp@szp-pc:~$ cp -r /usr/local/share/filebench/workloads/ ./workloads 使用WML语言进行编写，教程如下：workload model language。 2.filebench模拟web服务器测试使用 sudo filebench -f webserver.f 运行webserver测试。 直接运行会出现无法停止的问题，修改webserver.f，加入退出模式。 12// 设置退出模式，支持[ timeout | alldone | firstdone ]set mode quit firstdone 然后有可能出现以下错误： szp@szp-pc:~/workloads$ sudo filebench -f webserver.f Filebench Version 1.5-alpha3 0.002: Allocated 173MB of shared memory 0.027: Web-server Version 3.1 personality successfully loaded 0.027: Populating and pre-allocating filesets 0.032: logfiles populated: 1 files, avg. dir. width = 2, avg. dir. depth = 0.0, 0 leafdirs, 0.002MB total size 0.032: Removing logfiles tree (if exists) 0.039: Pre-allocating directories in logfiles tree 0.039: Pre-allocating files in logfiles tree 0.040: bigfileset populated: 10 files, avg. dir. width = 2, avg. dir. depth = 3.3, 0 leafdirs, 0.132MB total size 0.040: Removing bigfileset tree (if exists) 0.046: Pre-allocating directories in bigfileset tree 0.046: Pre-allocating files in bigfileset tree 0.047: Waiting for pre-allocation to finish (in case of a parallel pre-allocation) 0.047: Population and pre-allocation of filesets completed 0.049: Starting 1 filereader instances 9.063: Waiting for pid 8269 thread filereaderthread-1 10.063: Waiting for pid 8269 thread filereaderthread-1 11.064: Running... 11.064: Unexpected Process termination Code 3, Errno 0 around line 77 12.065: Run took 1 seconds... 12.065: Run took 1 seconds... 原因是系统打开了ASLR。ASLR(Address Space Layout Randomization)在2005年被引入到Linux的内核 kernel 2.6.12 中，当然早在2004年就以patch的形式被引入。随着内存地址的随机化，使得响应的应用变得随机。这意味着同一应用多次执行所使用内存空间完全不同，也意味着简单的缓冲区溢出攻击无法达到目的。 使用root权限以及如下命令关闭ASLR。 1echo 0 &gt; /proc/sys/kernel/randomize_va_space 重新执行filebench程序： root@szp-pc:/home/szp/workloads# filebench -f webserver.f Filebench Version 1.5-alpha3 0.002: Allocated 173MB of shared memory 0.018: Web-server Version 3.1 personality successfully loaded 0.019: Populating and pre-allocating filesets 0.021: logfiles populated: 1 files, avg. dir. width = 2, avg. dir. depth = 0.0, 0 leafdirs, 0.002MB total size 0.021: Removing logfiles tree (if exists) 0.025: Pre-allocating directories in logfiles tree 0.025: Pre-allocating files in logfiles tree 0.026: bigfileset populated: 10 files, avg. dir. width = 2, avg. dir. depth = 3.3, 0 leafdirs, 0.132MB total size 0.026: Removing bigfileset tree (if exists) 0.030: Pre-allocating directories in bigfileset tree 0.031: Pre-allocating files in bigfileset tree 0.033: Waiting for pre-allocation to finish (in case of a parallel pre-allocation) 0.033: Population and pre-allocation of filesets completed 0.035: Starting 1 filereader instances 1.052: Running... 61.333: Run took 60 seconds... 61.335: Per-Operation Breakdown appendlog 160003ops 2654ops/s 20.7mb/s 0.1ms/op [0.00ms - 151.46ms] closefile10 160003ops 2654ops/s 0.0mb/s 0.0ms/op [0.00ms - 0.19ms] readfile10 160003ops 2654ops/s 16.0mb/s 0.0ms/op [0.00ms - 0.52ms] openfile10 160003ops 2654ops/s 0.0mb/s 0.0ms/op [0.00ms - 1.19ms] closefile9 160003ops 2654ops/s 0.0mb/s 0.0ms/op [0.00ms - 0.20ms] readfile9 160003ops 2654ops/s 13.9mb/s 0.0ms/op [0.00ms - 0.33ms] openfile9 160003ops 2654ops/s 0.0mb/s 0.0ms/op [0.00ms - 0.84ms] closefile8 160003ops 2654ops/s 0.0mb/s 0.0ms/op [0.00ms - 0.30ms] readfile8 160003ops 2654ops/s 30.3mb/s 0.0ms/op [0.00ms - 0.49ms] openfile8 160003ops 2654ops/s 0.0mb/s 0.0ms/op [0.00ms - 0.88ms] closefile7 160003ops 2654ops/s 0.0mb/s 0.0ms/op [0.00ms - 0.28ms] readfile7 160003ops 2654ops/s 28.2mb/s 0.0ms/op [0.00ms - 0.71ms] openfile7 160003ops 2654ops/s 0.0mb/s 0.0ms/op [0.00ms - 0.85ms] closefile6 160003ops 2654ops/s 0.0mb/s 0.0ms/op [0.00ms - 0.18ms] readfile6 160003ops 2654ops/s 125.5mb/s 0.0ms/op [0.00ms - 0.68ms] openfile6 160003ops 2654ops/s 0.0mb/s 0.0ms/op [0.00ms - 1.47ms] closefile5 160003ops 2654ops/s 0.0mb/s 0.0ms/op [0.00ms - 0.10ms] readfile5 160003ops 2654ops/s 9.5mb/s 0.0ms/op [0.00ms - 0.25ms] openfile5 160003ops 2654ops/s 0.0mb/s 0.0ms/op [0.00ms - 0.77ms] closefile4 160003ops 2654ops/s 0.0mb/s 0.0ms/op [0.00ms - 0.18ms] readfile4 160003ops 2654ops/s 58.3mb/s 0.0ms/op [0.00ms - 0.31ms] openfile4 160003ops 2654ops/s 0.0mb/s 0.0ms/op [0.00ms - 1.12ms] closefile3 160003ops 2654ops/s 0.0mb/s 0.0ms/op [0.00ms - 0.13ms] readfile3 160003ops 2654ops/s 30.2mb/s 0.0ms/op [0.00ms - 0.20ms] openfile3 160003ops 2654ops/s 0.0mb/s 0.0ms/op [0.00ms - 1.24ms] closefile2 160003ops 2654ops/s 0.0mb/s 0.0ms/op [0.00ms - 0.17ms] readfile2 160003ops 2654ops/s 26.7mb/s 0.0ms/op [0.00ms - 0.48ms] openfile2 160003ops 2654ops/s 0.0mb/s 0.0ms/op [0.00ms - 2.47ms] closefile1 160003ops 2654ops/s 0.0mb/s 0.0ms/op [0.00ms - 0.18ms] readfile1 160003ops 2654ops/s 12.0mb/s 0.0ms/op [0.00ms - 0.25ms] openfile1 160003ops 2654ops/s 0.0mb/s 0.0ms/op [0.00ms - 0.76ms] 61.335: IO Summary: 4960093 ops 82283.522 ops/s 26543/2654 rd/wr 371.3mb/s 0.0ms/op 61.335: Shutting down processes 结果分析： flowop name - 支持的flowop（测试流程）有很多 所有threads的ops 所有threads的ops / run time 所有threads的READ/WRITE带宽 所有threads的每个op的平均latency 测试中op的最小和最大latency 3.测试程序解释1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# 自定义变量set $dir=/home/szp/testset $nfiles=10set $meandirwidth=2set $filesize=cvar(type=cvar-gamma,parameters=mean:16384;gamma:1.5)set $nthreads=1set $iosize=1mset $meanappendsize=16k# 设置退出模式，支持[ timeout | alldone | firstdone ]set mode quit firstdone# fileset：定义一组测试中用的files# name=bigfileset：必须指定 - fileset的名称，后面flowop中用到# path=$dir：必须指定 - 创建测试文件的目录# size=$meanfilesize：可选，关键字也可以为filesize，默认为1KB - 测试文件的size# entries=$nfiles：可选，默认位1024 - fileset中的file个数# dirwidth=$meandirwidth：可选，默认为0 - 每个目录中创建的file个数define fileset name=bigfileset,path=$dir,size=$filesize,entries=$nfiles,dirwidth=$meandirwidth,prealloc=100,readonlydefine fileset name=logfiles,path=$dir,size=$filesize,entries=1,dirwidth=$meandirwidth,prealloc# process：定义处理过程# name=filecreate：必须指定 - 处理过程的名称# instances=1：可选，默认为1 - 处理过程的进程数define process name=filereader,instances=1&#123;# thread：process中的一个thread# name=filecreatethread：必须指定 - 处理线程的名称# memsize=10m：必须指定 - 线程启动后初始化为0的内存大小，用于read/write flowop# instances=$nthreads：可选，默认为1 - 创建的线程数 thread name=filereaderthread,memsize=10m,instances=$nthreads &#123; # flowop：定义处理流程中的每一步 # createfile/writewholefile/closefile：flowop的关键字，每个代表不同的操作 # name=$name：flowop的名称 # filesetname=bigfileset：指定op操作的fileset # fd=1：指定file descriptor的值，在应用允许文件被多次open的场景中有用 # iosize=$iosize：指定读写的iosize flowop openfile name=openfile1,filesetname=bigfileset,fd=1 flowop readwholefile name=readfile1,fd=1,iosize=$iosize flowop closefile name=closefile1,fd=1 flowop openfile name=openfile2,filesetname=bigfileset,fd=1 flowop readwholefile name=readfile2,fd=1,iosize=$iosize flowop closefile name=closefile2,fd=1 flowop openfile name=openfile3,filesetname=bigfileset,fd=1 flowop readwholefile name=readfile3,fd=1,iosize=$iosize flowop closefile name=closefile3,fd=1 flowop openfile name=openfile4,filesetname=bigfileset,fd=1 flowop readwholefile name=readfile4,fd=1,iosize=$iosize flowop closefile name=closefile4,fd=1 flowop openfile name=openfile5,filesetname=bigfileset,fd=1 flowop readwholefile name=readfile5,fd=1,iosize=$iosize flowop closefile name=closefile5,fd=1 flowop openfile name=openfile6,filesetname=bigfileset,fd=1 flowop readwholefile name=readfile6,fd=1,iosize=$iosize flowop closefile name=closefile6,fd=1 flowop openfile name=openfile7,filesetname=bigfileset,fd=1 flowop readwholefile name=readfile7,fd=1,iosize=$iosize flowop closefile name=closefile7,fd=1 flowop openfile name=openfile8,filesetname=bigfileset,fd=1 flowop readwholefile name=readfile8,fd=1,iosize=$iosize flowop closefile name=closefile8,fd=1 flowop openfile name=openfile9,filesetname=bigfileset,fd=1 flowop readwholefile name=readfile9,fd=1,iosize=$iosize flowop closefile name=closefile9,fd=1 flowop openfile name=openfile10,filesetname=bigfileset,fd=1 flowop readwholefile name=readfile10,fd=1,iosize=$iosize flowop closefile name=closefile10,fd=1 flowop appendfilerand name=appendlog,filesetname=logfiles,iosize=$meanappendsize,fd=2 &#125;&#125;# 开始运行filebench测试# 格式：run [&lt;runtime&gt;]，&lt;runtime&gt;不指定的话，默认为60secho "Web-server Version 3.1 personality successfully loaded"run 60]]></content>
  </entry>
  <entry>
    <title><![CDATA[Gin入门教程]]></title>
    <url>%2FGolang%2FGin%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Gin是一个golang的微框架，封装比较优雅，API友好，源码注释比较明确，具有快速灵活，容错方便等特点。对于golang而言，web框架的依赖要远比Python，Java之类的要小。自身的net/http足够简单，性能也非常不错。借助框架开发，不仅可以省去很多常用的封装带来的时间，也有助于团队的编码风格和形成规范。在使用Gin之前应当安装好了go环境，建议版本1.9+。参考Gin中文手册。 1. Gin框架环境搭建由于国内使用go get网速很慢，首先应当设置代理。 设置代理： go env -w GO111MODULE=ongo env -w GOPROXY=https://goproxy.cn,direct 下载gin: go get -u github.com/gin-gonic/gin 如下图所示： 2. 初始化Web项目使用go mod init 域名/模块名初始化gin项目。 12345mkdir myTestcd myTestgo mod init szp2016.github.io/myTestgo get github.com/gin-gonic/gin 执行完之后可以在go.mod文件中看到项目所需依赖已经添加。]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
        <tag>Gin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发]]></title>
    <url>%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2F%E5%B9%B6%E5%8F%91%2F</url>
    <content type="text"><![CDATA[1. 前言操作系统是第一个并发程序，是在拥有多线程的进程中应该研究的问题。每个线程都是一个进程的执行流，代表着程序做事，但是当线程访问内存的时候，对于他们来说有些内存节点是与其他线程共享的，如果不协调线程之间的内存访问，程序将无法按着预期工作。如何解决呢？操作系统使用锁或者条件变量这样的原语，来支持多线程应用程序，如果不非常小心的访问自己的内存，将会发生许多奇怪而可怕的事情，接下来就看看会发生什么。 2. 共享变量加一发生什么事了有如下代码，使用pthread_create函数创建16个线程，它们都对全局变量sum进行加一操作，每个线程操作1000次。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;pthread.h&gt;#include &lt;errno.h&gt;#include &lt;unistd.h&gt;//定义线程数量#define PTHREAD_NUM 16//定义全局变量unsigned long sum = 0;//子线程执行的函数，对全局变量sum做加一操作10000次void *thread(void *arg)&#123; for(int i=0; i &lt; 10000; i++) sum += 1;&#125;int main(void)&#123; printf("before ...sum = %lu\n", sum);//定义线程标识符，线程id，与线程交互。 pthread_t pthread[PTHREAD_NUM]; int ret; void *retval[PTHREAD_NUM]; for (int i = 0; i &lt; PTHREAD_NUM; ++i) &#123; /* 创建新线程，可能会立即执行，也可能处于就绪状态，等待执行。 第一个参数：pthread_t结构类型指针，传进去并初始化。 第二个参数：pthread_attr_t 结构类型指针，用于指定该线程的属性，例如栈大小、调度优先级。 第三个参数：函数指针，线程应该在哪个函数中运行。 第四个参数：传递给线程开始执行的函数的参数。 */ ret = pthread_create(&amp;pthread[i],NULL,thread,NULL); if (ret != 0) &#123; perror("cause:"); printf("creat pthread %d failed.\n", i+1); &#125; &#125; for (int i = 0; i &lt; PTHREAD_NUM; ++i) &#123; /* 主程序等待某个线程完成。 第一个参数：pthread_t类型，指定要等待的线程。 第二个参数：希望得到的返回值。 本程序的thread函数没有返回值。 */ pthread_join(pthread[i],&amp;retval[i]); &#125; printf("after.....sum = %lu\n", sum ); return 0;&#125; 按照我们的期望，如果程序“正常”执行的话，很容易可以得到结果sum应该是160000。编译程序： gcc thread.c -g -o thread -lpthread szp@szp-pc:~/code/sync$ gcc thread.c -g -o thread -lpthread szp@szp-pc:~/code/sync$ ./thread before ...sum = 0 after.....sum = 137320 szp@szp-pc:~/code/sync$ ./thread before ...sum = 0 after.....sum = 117388 szp@szp-pc:~/code/sync$ ./thread before ...sum = 0 after.....sum = 151319 szp@szp-pc:~/code/sync$ ./thread before ...sum = 0 after.....sum = 94043 szp@szp-pc:~/code/sync$ 运行程序，并没有得到预期的结果，而且每次结果都在变化，情况逐渐变得失控，程序产生了不确定的结果。要想理解这个问题，必须了解更新全局变量sum的汇编代码。 对可执行文件thread进行反汇编： objdump -d thread 找到thread函数的汇编代码。 00000000000011c9 &lt;thread&gt;: 11c9: f3 0f 1e fa endbr64 11cd: 55 push %rbp 11ce: 48 89 e5 mov %rsp,%rbp 11d1: 48 89 7d e8 mov %rdi,-0x18(%rbp) 11d5: c7 45 fc 00 00 00 00 movl $0x0,-0x4(%rbp) 11dc: eb 16 jmp 11f4 &lt;thread+0x2b&gt; 11de: 48 8b 05 33 2e 00 00 mov 0x2e33(%rip),%rax # 4018 &lt;sum&gt; 11e5: 48 83 c0 01 add $0x1,%rax 11e9: 48 89 05 28 2e 00 00 mov %rax,0x2e28(%rip) # 4018 &lt;sum&gt; 11f0: 83 45 fc 01 addl $0x1,-0x4(%rbp) 11f4: 81 7d fc 0f 27 00 00 cmpl $0x270f,-0x4(%rbp) 11fb: 7e e1 jle 11de &lt;thread+0x15&gt; 11fd: 90 nop 11fe: 5d pop %rbp 11ff: c3 retq 我们来看这一行代码sum += 1;在被编译之后成了什么样子,从中截取代码如下。 11de: 48 8b 05 33 2e 00 00 mov 0x2e33(%rip),%rax # 4018 &lt;sum&gt; 11e5: 48 83 c0 01 add $0x1,%rax 11e9: 48 89 05 28 2e 00 00 mov %rax,0x2e28(%rip) # 4018 &lt;sum&gt; 11f0: 83 45 fc 01 addl $0x1,-0x4(%rbp) 全局变量sum的地址在注释中给出了就是0x4018，需要主要的是全局的变量的地址计算，是通过rip（愿死者安息）寄存器中存储的偏移量+下一条指令的地址计算出来的。例如上面取出sum的值存入%rax寄存器，sum地址=0x11e5+0x2e33=0x4018，add命令对%rax寄存器的值加1（0x1）。最后%rax的值被放回内存中相同的地址，计算方式是0x11f0+0x2e28=0x4018。 2.1 多个线程并发执行，共享变量的加1操作到底会带来什么问题？试想一个线程进入这个代码区域，要对sum增加一个值，它将sum的值（假设此时是100）从内存中读入它的%rax寄存器，然后执行下一条指令add，对%rax寄存器的值加1，这是该线程%rax寄存器存储的sum值为51。当它要继续执行下面的写回sum值的指令时，发生了时钟中断，操作系统将当前线程的状态（%rax寄存器，PC）保存到线程的TCB中。然后接下来另一个线程被选中，同样也进入了这一块代码，取sum值放入自己的%rax，现在值是50，对其值加1，现在%rax值为51，最后写回内存，一气呵成，完成了操作。目前全局变量sum的值为51了。现在又发生了线程切换，上面的线程一又获得了执行权，它将从TCB中恢复刚才执行到的状态，它的%rax中的值为51，接下来执行写回命令，sum的值被重新赋值为51。好了，现在两个线程对sum执行了加1操作，sum的值应当是52才对，但是在刚才的情况下，sum的值是51。 这只是其中一种情况，事实上程序产生不确定结果的原因也真是源于此。这种情况被称为竞态条件，运气不好就会产生错误的结果。可能产生竞态条件的这段代码就叫做临界区。临界区就是访问共享资源的代码片段。我们想要实现的理想的代码运行顺序就是所谓的互斥，保证如果一个线程正处于临界区，其他线程将被阻止进入临界区。互斥最终实现的结果就是线程的同步执行，即让程序按照某种既定的顺序执行。 3. 什么是原子操作？Linux下如何进行原子操作？上面的问题就是不能一步就完成要做的事情，从而产生了不合时宜的中断。指令集不能提供一个指令完成对内存中的一个数值增加1，当中断发生的时候，要么没有执行，要么执行完成了，不存在中间状态。原子的意思是“作为一个单元”，全部执行或者不执行，有时将多个行为组合为单个原子动作称为事务，常见于数据库中。事实上，硬件提供了一些有用的指令，在这些指令上构建通用的集合，同步原语。通过这些原语，加上操作系统的帮助就能够同步的访问临界区。 gcc从4.1.2开始提供了sync_*系列的build-in函数，用于提供加减和逻辑运算的原子操作。 __sync_fetch_and_add系列一共有十二个函数，有加/减/与/或/异或/等函数的原子性操作函数，sync_fetch_and_add，顾名思义，先fetch，然后自加，返回的是自加以前的值。以count = 4为例，调用__sync_fetch_and_add(&amp;count,1)之后，返回值是4，然后，count变成了5。 4. 什么是临界区和临界资源？临界资源：系统中一次只允许一个进程(线程)访问的资源称为临界资源。一旦分配给进程，不能强制剥夺，通常是一个变量或者数据结构。临界区：并发执行的进程中, 访问(读取或修改)临界资源必须互斥执行的程序段叫临界区。 5. Linux C中如何保护临界区POSIX通过锁来提供互斥进入临界区的函数，最基本的一对函数是： int pthread_mutex_lock(pthread_mutex_t *mutex);int pthread_mutex_unlock(pthread_mutex_t *mutex); 利用锁来保护临界区，使用方法大致如下：pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;pthread_mutex_lock(&amp;lock);x = x + 1; // 临界区pthread_mutex_unlock(&amp;lock); 如果在调用pthread_mutex_lock的时候，没有其他进程持有该锁，线程将获取该锁，并进入临界区；如果另一个进程已经获取了该锁，那么尝试获取该锁的线程不会从该函数调用返回，而是会进入睡眠，在获取锁的函数内等待，直到获得该锁。 另外在使用锁（锁简单来讲就是一个变量）之前，应该对其初始化，常见两种方式对初始化： 使用PTHREAD_MUTEX_INITIALIZER宏定义 使用pthread_mutex_init(&amp;lock, NULL);函数 接下来就可以对上面的程序进行修改，对临界区sum=sum+1加锁保护。修改thread函数如下： 12345678910111213141516171819pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;unsigned long sum = 0;void *thread(void *arg)&#123; for(int i=0; i &lt; 10000; i++)&#123; int rc = pthread_mutex_lock(&amp;lock); assert(rc==0); sum += 1; pthread_mutex_unlock(&amp;lock); &#125; &#125; 运行结果与预期相同了： szp@szp-pc:~/code/sync$ gcc thread.c -o thread -lpthread szp@szp-pc:~/code/sync$ ./thread before ...sum = 0 after.....sum = 160000 szp@szp-pc:~/code/sync$ ./thread before ...sum = 0 after.....sum = 160000]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[open系统调用（二）]]></title>
    <url>%2FLinux%2Fopen%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1. 前言前面说到通过调用函数path_openat以解析文件路径，path_openat中包装了两个重要的函数path_init和link_path_walk，本文就从这两个函数开始，继续打开文件的旅程。 2. path_opennat再次看一下path_opennat函数，其中的path_init和link_path_walk通常连在一起调用，二者合在一起就可以根据给定的文件路径名称在内存中找到或者建立代表着目标文件或者目录的dentry结构和inode结构。 12345678910111213141516171819202122static struct file * path_openat(struct nameidata * nd, ... // 路径初始化，确定查找的起始目录，初始化结构体 nameidata 的成员 path。 s = path_init(nd, flags); // 调用函数 link_path_walk 解析文件路径的每个分量，最后一个分量除外。 // 调用函数 do_last，解析文件路径的最后一个分量，并且打开文件。 while (! (error = link_path_walk(s, nd)) &amp;&amp; (error = do_last(nd, file, op, &amp;opened)) &gt; 0) &#123; nd-&gt;flags &amp;= ~(LOOKUP_OPEN | LOOKUP_CREATE | LOOKUP_EXCL); // 如果最后一个分量是符号链接，调用 trailing_symlink 函数进行处理 // 读取符号链接文件的数据，新的文件路径是符号链接链接文件的数据，然后继续 while // 循环，解析新的文件路径。 s = trailing_symlink(nd); if (IS_ERR(s)) &#123; error = PTR_ERR(s); break; &#125; &#125;...&#125; 3.path_lookup在内核版本2.6，已经使用path_lookup函数代替了path_init函数，实现的功能都是一样的，做查找前的准备工作，初始化nd，nd存储的是查找结果，nd就是nameidata结构体。具体实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041//做查找前的准备工作，初始化nd，nd存储的是查找结果。int path_lookup(const char *name, unsigned int flags, struct nameidata *nd)&#123; nd-&gt;last_type = LAST_ROOT; /* if there are only slashes... */ nd-&gt;flags = flags; read_lock(&amp;current-&gt;fs-&gt;lock); if (*name=='/') &#123;//如果是从根目录开始的绝对路径 //当前进程没有使用chroot()系统调用设置替换根目录，altroot为0 if (current-&gt;fs-&gt;altroot &amp;&amp; !(nd-&gt;flags &amp; LOOKUP_NOALT)) &#123; /*flags中的LOOKUP_NOALT标志位通常为0，这些标志位都是对寻找目标的指示。 既然替换过了根目录，那就赋值，替换后的根目录。 */ nd-&gt;mnt = mntget(current-&gt;fs-&gt;altrootmnt); nd-&gt;dentry = dget(current-&gt;fs-&gt;altroot); read_unlock(&amp;current-&gt;fs-&gt;lock); if (__emul_lookup_dentry(name,nd)) return 0; read_lock(&amp;current-&gt;fs-&gt;lock); &#125; //没有替换根目录，直接赋值当前进程的根目录 nd-&gt;mnt = mntget(current-&gt;fs-&gt;rootmnt); nd-&gt;dentry = dget(current-&gt;fs-&gt;root); &#125; else&#123;//从进程当前目录开始的相对路径 //赋值指向的vfsmount nd-&gt;mnt = mntget(current-&gt;fs-&gt;pwdmnt); /*dentry初始化为当前目录，也就是搜索的起点，即从根目录到当前目录的那部分不用查找了。fs_struct-&gt;pwd存放当前目录dentry。 dget函数将该dentry引用计数+1后返回。 */ nd-&gt;dentry = dget(current-&gt;fs-&gt;pwd); &#125; read_unlock(&amp;current-&gt;fs-&gt;lock); current-&gt;total_link_count = 0; return link_path_walk(name, nd);&#125; 函数最后调用link_path_walk进行实际的搜索工作。 4.link_path_walkpath_lookup函数执行成功后，就会在nameidata结构体的成员dentry中指向搜索路径的起点，接下来就是link_path_walk函数顺着路径进行搜索了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222int link_path_walk(const char * name, struct nameidata *nd)&#123; struct path next; struct inode *inode; int err; unsigned int lookup_flags = nd-&gt;flags; //如果是根目录，跳过‘/’ while (*name=='/') name++; //如果路径只包含‘/’,搜索完成，返回。 if (!*name) goto return_reval; //获取dentry对应的inode inode = nd-&gt;dentry-&gt;d_inode; //link_count计数器，记录查找链的长度，当达到某个值时，就终止搜索，防止陷入循环。 if (current-&gt;link_count)//计数值不是0，则说明顺着符号链接，进入了另一个设备的文件系统，递归调用了本函数。 //查找标志设置为1，表示顺着符号链接，找到终点。 lookup_flags = LOOKUP_FOLLOW; /* At this point we know we have a real path component. */ for(;;) &#123; unsigned long hash; //存放当前节点的信息 struct qstr this; unsigned int c; //检查是否拥有中间目录的权限，需要有执行权限MAY_EXEC，这里先使用permission函数的青春版进行检查，不通过，再使用原函数做更多检查 err = exec_permission_lite(inode, nd); if (err == -EAGAIN) &#123; err = permission(inode, MAY_EXEC, nd); &#125; if (err) break; this.name = name; c = *(const unsigned char *)name; hash = init_name_hash(); //逐个字符的计算出，当前节点名称的哈希值，遇到/或者‘\0’退出。 do &#123; name++; hash = partial_name_hash(c, hash); c = *(const unsigned char *)name; &#125; while (c &amp;&amp; (c != '/')); this.len = name - (const char *) this.name; this.hash = end_name_hash(hash); /* remove trailing slashes? */ if (!c)//当前节点的最后一个字符是‘\0’,即为当前路径中的最后一节 goto last_component; //当前节点名称的最后一个字符是/ while (*++name == '/'); //当前节点已经是最后一个，只不过后面加了‘/’ if (!*name) goto last_with_slashes; /* * "." and ".." are special - ".." especially so because it has * to be able to know about the current root directory and * parent relationships. */ /*到这里节点一定是中间节点或者起始节点，并且是个目录*/ if (this.name[0] == '.') switch (this.len) &#123; //如果目录的第一个字符是‘.’，当前节点长度只能为1或者2 default: break; case 2: if (this.name[1] != '.')//如果是2，第二个字符也是‘.’ break; //查找当前目录的父目录 follow_dotdot(&amp;nd-&gt;mnt, &amp;nd-&gt;dentry); inode = nd-&gt;dentry-&gt;d_inode; /* fallthrough */ case 1: //回到for循环开始，继续下一个节点 continue; &#125; /* * See if the low-level filesystem might want * to use its own hash.. */ if (nd-&gt;dentry-&gt;d_op &amp;&amp; nd-&gt;dentry-&gt;d_op-&gt;d_hash) &#123; err = nd-&gt;dentry-&gt;d_op-&gt;d_hash(nd-&gt;dentry, &amp;this); if (err &lt; 0) break; &#125; //准备工作完成，开始搜索 nd-&gt;flags |= LOOKUP_CONTINUE; /* This does the actual lookups.. */ //找到或者建立的所需的dentry结构 err = do_lookup(nd, &amp;this, &amp;next); if (err) break; /* Check mountpoints.. */ follow_mount(&amp;next.mnt, &amp;next.dentry); err = -ENOENT; inode = next.dentry-&gt;d_inode; if (!inode) goto out_dput; err = -ENOTDIR; if (!inode-&gt;i_op) goto out_dput; if (inode-&gt;i_op-&gt;follow_link) &#123;//判断当前节点是不是一个链接 mntget(next.mnt); err = do_follow_link(next.dentry, nd); dput(next.dentry); mntput(next.mnt); if (err) goto return_err; err = -ENOENT; inode = nd-&gt;dentry-&gt;d_inode; if (!inode) break; err = -ENOTDIR; if (!inode-&gt;i_op) break; &#125; else &#123; dput(nd-&gt;dentry); nd-&gt;mnt = next.mnt; nd-&gt;dentry = next.dentry; &#125; err = -ENOTDIR; if (!inode-&gt;i_op-&gt;lookup) break; continue; /* here ends the main loop */last_with_slashes://路径的终点是个目录，如果节点是个链接，将下面两个标志位置1 lookup_flags |= LOOKUP_FOLLOW | LOOKUP_DIRECTORY;last_component://路径终点节点 nd-&gt;flags &amp;= ~LOOKUP_CONTINUE; if (lookup_flags &amp; LOOKUP_PARENT) goto lookup_parent; if (this.name[0] == '.') switch (this.len) &#123; default: break; case 2: if (this.name[1] != '.') break; follow_dotdot(&amp;nd-&gt;mnt, &amp;nd-&gt;dentry); inode = nd-&gt;dentry-&gt;d_inode; /* fallthrough */ case 1: goto return_reval; &#125; if (nd-&gt;dentry-&gt;d_op &amp;&amp; nd-&gt;dentry-&gt;d_op-&gt;d_hash) &#123; err = nd-&gt;dentry-&gt;d_op-&gt;d_hash(nd-&gt;dentry, &amp;this); if (err &lt; 0) break; &#125; err = do_lookup(nd, &amp;this, &amp;next); if (err) break; follow_mount(&amp;next.mnt, &amp;next.dentry); inode = next.dentry-&gt;d_inode; if ((lookup_flags &amp; LOOKUP_FOLLOW) &amp;&amp; inode &amp;&amp; inode-&gt;i_op &amp;&amp; inode-&gt;i_op-&gt;follow_link) &#123; mntget(next.mnt); err = do_follow_link(next.dentry, nd); dput(next.dentry); mntput(next.mnt); if (err) goto return_err; inode = nd-&gt;dentry-&gt;d_inode; &#125; else &#123; dput(nd-&gt;dentry); nd-&gt;mnt = next.mnt; nd-&gt;dentry = next.dentry; &#125; err = -ENOENT; if (!inode) break; if (lookup_flags &amp; LOOKUP_DIRECTORY) &#123; err = -ENOTDIR; if (!inode-&gt;i_op || !inode-&gt;i_op-&gt;lookup) break; &#125; goto return_base;lookup_parent: nd-&gt;last = this; nd-&gt;last_type = LAST_NORM; if (this.name[0] != '.') goto return_base; if (this.len == 1) nd-&gt;last_type = LAST_DOT; else if (this.len == 2 &amp;&amp; this.name[1] == '.') nd-&gt;last_type = LAST_DOTDOT; else goto return_base;return_reval: /* * We bypassed the ordinary revalidation routines. * We may need to check the cached dentry for staleness. */ if (nd-&gt;dentry &amp;&amp; nd-&gt;dentry-&gt;d_sb &amp;&amp; (nd-&gt;dentry-&gt;d_sb-&gt;s_type-&gt;fs_flags &amp; FS_REVAL_DOT)) &#123; err = -ESTALE; /* Note: we do not d_invalidate() */ if (!nd-&gt;dentry-&gt;d_op-&gt;d_revalidate(nd-&gt;dentry, nd)) break; &#125;return_base: return 0;out_dput: dput(next.dentry); break; &#125; path_release(nd);return_err: return err;&#125; 该函数比较长，其中调用了几个重要的函数需要逐一进行介绍。follow_dotdot的作用是查找当前节点的父目录。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647//查找当前节点的父目录static inline void follow_dotdot(struct vfsmount **mnt, struct dentry **dentry)&#123; while(1) &#123; struct vfsmount *parent; struct dentry *old = *dentry; read_lock(&amp;current-&gt;fs-&gt;lock); //1.当前节点就是本进程的根节点，无法向上查找，保持不变。 if (*dentry == current-&gt;fs-&gt;root &amp;&amp; *mnt == current-&gt;fs-&gt;rootmnt) &#123; read_unlock(&amp;current-&gt;fs-&gt;lock); break; &#125; read_unlock(&amp;current-&gt;fs-&gt;lock); spin_lock(&amp;dcache_lock); //2.当前节点不是所在设备根节点，说明与父节点在同一个设备上。 if (*dentry != (*mnt)-&gt;mnt_root) &#123; //赋值为父节点 *dentry = dget((*dentry)-&gt;d_parent); spin_unlock(&amp;dcache_lock); //释放旧节点 dput(old); break; &#125; //3.当前节点就是所在设备的根节点，再上一级就是其他设备了 //mnt_parent是父设备，根设备指向其自身 parent = (*mnt)-&gt;mnt_parent; if (parent == *mnt) &#123; //如果是根设备，结束循环，保持dentry不变 spin_unlock(&amp;dcache_lock); break; &#125; //引用计数+1 mntget(parent); //设置为安装点的上一层目录 *dentry = dget((*mnt)-&gt;mnt_mountpoint); spin_unlock(&amp;dcache_lock); dput(old); //引用计数-1 mntput(*mnt); //设置当前mnt值为上层设备的vfs_mount *mnt = parent; &#125; follow_mount(mnt, dentry);&#125; do_lookup函数找到或者建立的所需的dentry结构 1234567891011121314151617181920212223242526272829303132333435363738394041/* * It's more convoluted than I'd like it to be, but... it's still fairly * small and for now I'd prefer to have fast path as straight as possible. * It _is_ time-critical. */static int do_lookup(struct nameidata *nd, struct qstr *name, struct path *path)&#123; struct vfsmount *mnt = nd-&gt;mnt; //根据dentry哈希值从内存中的dentry哈希表中查找 struct dentry *dentry = __d_lookup(nd-&gt;dentry, name); if (!dentry)//没找到 goto need_lookup; //检查文件系统是否提供了dentry验证函数，如果验证不通过就将dentry从哈希表中脱链 if (dentry-&gt;d_op &amp;&amp; dentry-&gt;d_op-&gt;d_revalidate) goto need_revalidate;done: path-&gt;mnt = mnt; path-&gt;dentry = dentry; return 0;need_lookup: //磁盘中查找 dentry = real_lookup(nd-&gt;dentry, name, nd); if (IS_ERR(dentry)) goto fail; goto done;need_revalidate: if (dentry-&gt;d_op-&gt;d_revalidate(dentry, nd)) goto done; if (d_invalidate(dentry)) goto done; dput(dentry); goto need_lookup;fail: return PTR_ERR(dentry);&#125; 该函数主要是从内存中查找inode或者从磁盘查找，从内存中的哈希表中查找使用函数__d_lookup，从磁盘中查找又会调用real_lookup。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172//在哈希表中查找一个dentrystruct dentry * __d_lookup(struct dentry * parent, struct qstr * name)&#123; unsigned int len = name-&gt;len; unsigned int hash = name-&gt;hash; const unsigned char *str = name-&gt;name; //使用d_hash对父目录也计算哈希值，为了减少不同目录下的相同名称的目录，找到队列头 struct hlist_head *head = d_hash(parent,hash); struct dentry *found = NULL; struct hlist_node *node; rcu_read_lock(); hlist_for_each (node, head) &#123; struct dentry *dentry; unsigned long move_count; struct qstr * qstr; smp_read_barrier_depends(); dentry = hlist_entry(node, struct dentry, d_hash); /* if lookup ends up in a different bucket * due to concurrent rename, fail it */ if (unlikely(dentry-&gt;d_bucket != head)) break; /* * We must take a snapshot of d_move_count followed by * read memory barrier before any search key comparison */ move_count = dentry-&gt;d_move_count; smp_rmb(); if (dentry-&gt;d_name.hash != hash) continue; if (dentry-&gt;d_parent != parent) continue; qstr = dentry-&gt;d_qstr; smp_read_barrier_depends(); //使用文件系统自己定义的文件名对比函数 if (parent-&gt;d_op &amp;&amp; parent-&gt;d_op-&gt;d_compare) &#123; if (parent-&gt;d_op-&gt;d_compare(parent, qstr, name)) continue; &#125; else &#123; //使用简单的memcmp对比 if (qstr-&gt;len != len) continue; if (memcmp(qstr-&gt;name, str, len)) continue; &#125; spin_lock(&amp;dentry-&gt;d_lock); /* * If dentry is moved, fail the lookup */ if (likely(move_count == dentry-&gt;d_move_count)) &#123; if (!d_unhashed(dentry)) &#123; atomic_inc(&amp;dentry-&gt;d_count); found = dentry; &#125; &#125; spin_unlock(&amp;dentry-&gt;d_lock); break; &#125; rcu_read_unlock(); return found;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/* * This is called when everything else fails, and we actually have * to go to the low-level filesystem to find out what we should do.. * * We get the directory semaphore, and after getting that we also * make sure that nobody added the entry to the dcache in the meantime.. * SMP-safe */static struct dentry * real_lookup(struct dentry * parent, struct qstr * name, struct nameidata *nd)&#123; struct dentry * result; struct inode *dir = parent-&gt;d_inode; //从磁盘建立dentry，使用信号量，在临界区中进行。 down(&amp;dir-&gt;i_sem); /* * First re-do the cached lookup just in case it was created * while we waited for the directory semaphore.. * * FIXME! This could use version numbering or similar to * avoid unnecessary cache lookups. * * The "dcache_lock" is purely to protect the RCU list walker * from concurrent renames at this point (we mustn't get false * negatives from the RCU list walk here, unlike the optimistic * fast walk). * * so doing d_lookup() (with seqlock), instead of lockfree __d_lookup */ //down进入临界区会睡眠等待，再次在内存查找，确认没有其他进程创建了该dentry result = d_lookup(parent, name); if (!result) &#123; //分配内存空间 struct dentry * dentry = d_alloc(parent, name); result = ERR_PTR(-ENOMEM); if (dentry) &#123; //从磁盘中寻找当前节点目录项，该函数因文件系统而异，inode节点中的i_op定义。ext2是ext2_dir_inode_operations result = dir-&gt;i_op-&gt;lookup(dir, dentry, nd); if (result) //撤销分配的dentry dput(dentry); else result = dentry; &#125; up(&amp;dir-&gt;i_sem); //成功返回dentry return result; &#125; 从磁盘中寻找当前节点目录项，该函数因文件系统而异，inode节点中的i_op定义。ext2是ext2_dir_inode_operations。继续来看ext2_lookup。 12345678910111213141516171819202122232425262728/* * Methods themselves. 从磁盘中加载dentry */static struct dentry *ext2_lookup(struct inode * dir, struct dentry *dentry, struct nameidata *nd)&#123; struct inode * inode; ino_t ino; if (dentry-&gt;d_name.len &gt; EXT2_NAME_LEN) return ERR_PTR(-ENAMETOOLONG); //找到当前节点的目录项 ino = ext2_inode_by_name(dir, dentry); inode = NULL; if (ino) &#123; //读入该目录项的索引节点，并建立inode结构 inode = iget(dir-&gt;i_sb, ino); if (!inode) return ERR_PTR(-EACCES); &#125; if (inode) return d_splice_alias(inode, dentry); //完成dentry结构设置，挂入哈希表中某个队列 d_add(dentry, inode); return NULL;&#125;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>文件系统</tag>
        <tag>open系统调用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单字符驱动程序与中断]]></title>
    <url>%2FLinux%2F%E7%AE%80%E5%8D%95%E9%94%AE%E7%9B%98%E9%A9%B1%E5%8A%A8%E7%A8%8B%E5%BA%8F%E4%B8%8E%E4%B8%AD%E6%96%AD%2F</url>
    <content type="text"><![CDATA[1.前言本文将实现一个简单的字符驱动程序，同时在驱动程序的内核模块中注册一个中断处理函数，用来获取键盘的扫描码并存入缓冲区中，最后使用用户态程序访问字符驱动程序，取出键盘扫描码。中断常用于键盘、鼠标等IO设备，当用户按下键盘时，会产生一个键盘扫描码。例如下面ps/2接口的键盘扫描码，它使用一个数字或数字序列来表示分配到键盘上的每个按键。 2.内核模块初始化内核模块在初始化的时候不仅仅需要注册一个字符设备，现在还需要在模块初始化的时候就向系统中的中断请求队列挂入我们自己的中断服务程序，也就是对其进行注册，使用的函数是 123456789101112131415request_irq(unsigned int irq, irq_handler_t handler, unsigned long flags,const char *name, void *dev)。irq是要申请的硬件中断号。handler是向系统注册的中断处理函数，是一个回调函数，中断发生时，系统调用这个函数，dev参数将被传递给它。flags是中断处理的属性，若设置了IRQF_DISABLED ，则表示中断处理程序是快速处理程序，快速处理程序被调用时屏蔽所有中断，慢速处理程序不屏蔽；若设置了IRQF_SHARED （老版本中的SA_SHIRQ），则表示多个设备共享中断，若设置了IRQF_SAMPLE_RANDOM（老版本中的SA_SAMPLE_RANDOM），表示对系统熵有贡献，对系统获取随机数有好处。name设置中断名称，通常是设备驱动程序的名称 在cat /proc/interrupts中可以看到此名称。dev在中断共享时会用到，一般设置为这个设备的设备结构体或者NULL。request_irq()返回0表示成功，返回-INVAL表示中断号无效或处理函数指针为NULL，返回-EBUSY表示中断已经被占用且不能共享。 由于我们要监控键盘的输入中断，当 I/O 设备把中断信号发送给中断控制器时,与之关联的是一个中断号。因此应当使用键盘控制器的中断号。查看proc文件系统下的中断号分配情况： 1cat /proc/interrupts 可以看到键盘控制器intel 8042的中断号为1。因此注册中断的时候中断号应当设置为1。 8042负责读取键盘扫描码并将其存在缓冲器中供程序读取；另外还有一个芯片ECE1077，它负责连接键盘和EC，将键盘动作转换成扫描码。CPU通过两个IO端口与8042通信，这两个IO端口就是0x60，0x64端口。 8042有四个8位的寄存器，它们是输入寄存器（RO）、输出寄存器(WO)、状态寄存器(RO)和命令寄存器(R/W)。 读输出寄存器：inb(0x60); 写输入寄存器：outb(0x60,data); 读状态寄存器：inb(0x64); b：代表一个字节。outb() I/O 上写入 8 位数据 ( 1 字节 )。outw() I/O 上写入 16 位数据 ( 2 字节 )；outl () I/O 上写入 32 位数据 ( 4 字节)。 状态寄存器： Bit7: PARITY-EVEN(P_E): 从键盘获得的数据奇偶校验错误 Bit6: RCV-TMOUT(R_T): 接收超时，置1 Bit5: TRANS_TMOUT(T_T): 发送超时，置1 Bit4: KYBD_INH(K_I): 为1，键盘没有被禁止。为0，键盘被禁止。 Bit3: CMD_DATA(C_D): 为1，输入缓冲器中的内容为命令，为0，输入缓冲器中的内容为数据。 Bit2: SYS_FLAG(S_F): 系统标志，加电启动置0，自检通过后置1 Bit1: INPUT_BUF_FULL(I_B_F): 输入缓冲器满置1，i8042 取走后置0 Bit0: OUT_BUF_FULL(O_B_F): 输出缓冲器满置1，CPU读取后置0 命令寄存器： Bit7: 保留，应该为0 Bit6: 将第二套扫描码翻译为第一套 Bit5: 置1，禁止鼠标 Bit4: 置1，禁止键盘 Bit3: 置1，忽略状态寄存器中的 Bit4 Bit2: 设置状态寄存器中的 Bit2 Bit1: 置1，enable 鼠标中断 Bit0: 置1，enable 键盘中断 有了以上知识，再对照键盘的扫描码表，我们就可以编写程序读取并判断键盘输入的数据，并根据不同的按键执行不同的动作。 内核模块处理函数中，还申请了一个kfifo环形缓冲区，用于存储我们获取到的键盘扫描码。 12ret = kfifo_alloc(&amp;key_buf, 32, GFP_ATOMIC); 剩余的工作就全都是注册一个字符设备驱动。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455static int __init my_init(void) &#123; // 模块入口函数 int ret; struct device *dev; printk("========== [+] Init module. ==========\n"); ret = kfifo_alloc(&amp;key_buf, 32, GFP_ATOMIC); // 申请kfifo队列 if (ret) &#123; printk("[!] Allocate memory failed.\n"); return ret; &#125; printk("[+] Allocate kfifo successfully.\n"); // Trigger interrupt ret = request_irq(1, (irq_handler_t)key_int_handler, IRQF_SHARED, "Key Hook", (void *)key_int_handler); // 申请IRQ if (ret) &#123; printk("[!] Request irq failed.\n"); return ret; &#125; printk("[+] Request irq successfully.\n"); // Register devices printk("[*] Invoke alloc_chrdev_region.\n"); ret = alloc_chrdev_region(&amp;dev_id, 0, 1, "mychar"); // 动态分配设备编号 if (ret &gt;= 0) &#123; printk("[*] Invoke cdev_init.\n"); cdev_init(&amp;key_dev, &amp;key_fops); // 初始化字符设备 key_dev.owner = THIS_MODULE; // 设置实现驱动的模块为当前模块 printk("[*] Invoke cdev_add.\n"); ret = cdev_add(&amp;key_dev, dev_id, 1); // 添加字符设备到系统中 if (ret &gt;= 0) &#123; printk("[*] Invoke class_create.\n"); key_class = class_create(THIS_MODULE, "mychar"); // 创建一个类并注册到内核中 if (key_class) &#123; printk("[*] Invoke device_create.\n"); dev = device_create(key_class, NULL, dev_id, NULL, "mychar"); // 创建一个设备并注册到sysfs中 if (dev) &#123; goto success; &#125;else &#123; printk("[!] device_create failed.\n"); &#125; &#125; else &#123; class_destroy(key_class); // 删除类 printk("[!] Invoke class_create failed.\n"); &#125; cdev_del(&amp;key_dev); // 删除字符设备 &#125; else &#123; printk("[!] Invoke cdev_add failed.\n"); &#125; unregister_chrdev_region(dev_id, 1); // 释放设备编号 return ret; &#125;success: printk("[+] Create charactor device successfully.\n"); return 0;&#125; 3.中断处理函数上面在注册中断的时候，加入了一个回调函数key_int_handler，但中断发生的时候，就会执行该函数，该函数由我们来编写。使用中断下半部的工作队列机制将处理推迟，或者进行调度。首先使用DECLARE_WORK声明一个工作队列，被推迟的任务都叫做工作，放到该工作队列中。描述工作的数据结构是work_struct。 内核里一直运行类似worker thread，它会对工作队列中的work进行处理，大致的工作流程原理可以参考下图所示： 这里的work则是work_struct变量，并且绑定一个执行函数——typedef void (*work_func_t)(struct work_struct *work);。在worker thread中会对非空的工作队列进行工作队列的出队操作，并运行work绑定的函数。 123456789struct work_struct &#123; atomic_long_t data; struct list_head entry;//工作的链表 work_func_t func;//要执行的函数#ifdef CONFIG_LOCKDEP struct lockdep_map lockdep_map;#endif&#125;; 12345678910111213// 创建需要推后完成到工作，名为key_work，待执行函数为key_work_funcDECLARE_WORK(key_work, key_work_func); // 声明工作队列irq_handler_t key_int_handler(int irq, void *dev) &#123; spin_lock(&amp;key_lock); scancode = inb(0x60); // 获取扫描码 spin_unlock(&amp;key_lock); spin_lock(&amp;key_lock); status = inb(0x64); // 获取按键状态 spin_unlock(&amp;key_lock); // printk("Key interrupt: scancode = 0x%x, status = 0x%x\n", scancode, status); schedule_work(&amp;key_work); // 调度工作队列 return (irq_handler_t)IRQ_HANDLED;&#125; 工作创建好以后，我们可以调度它，使用schedule_work提交给工作线程。其中inb(0x60)，用于从0x60 IO端口读取扫描码。 下面看工作队列调度处理函数，主要完成的工作就是将前面获取到的扫描码，存入到fifo队列，同时唤醒等待读取的字符驱动数据的进程。 1234567891011121314151617181920void key_work_func(struct work_struct *q) &#123; // 工作队列调度函数 unsigned char code; spin_lock(&amp;key_lock); // 加上自旋锁 code = scancode; // 获取当前扫描码 spin_unlock(&amp;key_lock); if (code == 0xe0) &#123; // 某些按键的特征符号 ; &#125; else if (code &amp; 0x80) &#123; // 释放按键 // printk("In work: released \"%s\"\n", mappings[code - 0x80]); &#125; else &#123; // 按下按键 // printk("In work: pressed \"%s\"\n", mappings[code]); mutex_lock(&amp;buf_lock); kfifo_in(&amp;key_buf, (void *)&amp;code, sizeof(unsigned char)); // 将扫描码入队列 key_count++; printk("[DEBUG] code = 0x%x, key_count = %d.\n", code, key_count); wake_up_interruptible(&amp;waitq); // 唤醒睡眠进程 mutex_unlock(&amp;buf_lock); &#125;&#125; 4. 字符处理函数实现字符驱动程序的读函数，读取fifo队列中的键盘扫描符，拷贝到用户空间。 1234567891011121314151617181920static ssize_t myread(struct file *filp, char __user *buf, size_t count, loff_t *pos) &#123; // 读设备函数 unsigned char *c; int ret; if (count == 0) &#123; // 传入的count值不能为0 printk("[!] Count can not be 0.\n"); return -1; &#125; printk("[DEBUG] kfifo_len = %d.\n", kfifo_len(&amp;key_buf)); if (wait_event_interruptible(waitq, (kfifo_len(&amp;key_buf) &gt;= count))) // 睡眠并等待唤醒 return -ERESTARTSYS; c = (unsigned char *)kmalloc(count, GFP_KERNEL); // 申请一块内存 mutex_lock(&amp;buf_lock); kfifo_out(&amp;key_buf, c, count); // 将指定长度扫描码数据出队列 mutex_unlock(&amp;buf_lock); printk("[+] Copy buffer to user: %s.\n", c); ret = copy_to_user(buf, c, count); // 将出队列的扫描码传给用户空间 kfree(c); // 释放申请的内存空间 c = 0; // 防止UAF return count;&#125; 用户空间函数如下： 123456789101112131415161718192021222324#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;fcntl.h&gt;#include &lt;string.h&gt;#define BUF_SIZE 32 // 设置缓冲区int main() &#123; int fd, c, i=0; unsigned char buf[BUF_SIZE]; // 初始化缓冲区 // sudo mknod /dev/ex4 c 444 0 fd = open("/dev/mychar", O_RDONLY); // 打开字符设备 if (fd &lt; 0) &#123; printf("[!] File open error.\n"); return -1; &#125; char chr=-2; while (1) &#123; c = read(fd, buf, BUF_SIZE); // 读取字符设备中的扫描码 printf("[+] Read scancode: %s (%d).\n", buf, c); // 把扫描码以字符串的形式输出 memset(buf,'\0',sizeof(buf)); &#125; close(fd); // 关闭字符设备 return 0;&#125; 5. 程序运行Makefile 文件如下： 12345678910111213#Makefile文件注意：假如前面的.c文件起名为first.c，那么这里的Makefile文件中的.o文件就要起名为first.o 只有root用户才能加载和卸载模块obj-m:=upper.o #产生interrupt模块的目标文件#目标文件 文件 要与模块名字相同CURRENT_PATH:=$(shell pwd) #模块所在的当前路径LINUX_KERNEL:=$(shell uname -r) #linux内核代码的当前版本LINUX_KERNEL_PATH:=/usr/src/linux-headers-$(LINUX_KERNEL)all: make -C $(LINUX_KERNEL_PATH) M=$(CURRENT_PATH) modules #编译模块#[Tab] 内核的路径 当前目录编译完放哪 表明编译的是内核模块clean: make -C $(LINUX_KERNEL_PATH) M=$(CURRENT_PATH) clean #清理模块 可以看到注册成功并输出了键盘的扫描符与统计的次数。 用户态程序，按照字符串格式输出了键盘的扫描符。 完整程序如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166upper.c#include &lt;linux/module.h&gt;#include &lt;linux/init.h&gt;#include &lt;linux/kernel.h&gt;#include &lt;linux/kfifo.h&gt;#include &lt;linux/workqueue.h&gt;#include &lt;linux/interrupt.h&gt;#include &lt;linux/slab.h&gt;#include &lt;linux/uaccess.h&gt;#include &lt;linux/cdev.h&gt;MODULE_LICENSE("GPL");MODULE_AUTHOR("linux");// Variables for device creationdev_t dev_id; // 设备号struct cdev key_dev; // 字符设备结构体struct class *key_class; // 字符设备对应的类// Variables for keyboard interruptstatic unsigned char scancode, status; // 键盘扫描码、状态static int key_count = 0; // 按键次数记录static struct kfifo key_buf; // 内核队列DEFINE_MUTEX(buf_lock); // 互斥锁DEFINE_SPINLOCK(key_lock); // 自旋锁static DECLARE_WAIT_QUEUE_HEAD(waitq); // 等待void key_work_func(struct work_struct *q) &#123; // 工作队列调度函数 unsigned char code; spin_lock(&amp;key_lock); // 加上自旋锁 code = scancode; // 获取当前扫描码 spin_unlock(&amp;key_lock); if (code == 0xe0) &#123; // 某些按键的特征符号 ; &#125; else if (code &amp; 0x80) &#123; // 释放按键 // printk("In work: released \"%s\"\n", mappings[code - 0x80]); &#125; else &#123; // 按下按键 // printk("In work: pressed \"%s\"\n", mappings[code]); mutex_lock(&amp;buf_lock); kfifo_in(&amp;key_buf, (void *)&amp;code, sizeof(unsigned char)); // 将扫描码入队列 key_count++; printk("[DEBUG] code = 0x%x, key_count = %d.\n", code, key_count); wake_up_interruptible(&amp;waitq); // 唤醒睡眠进程 mutex_unlock(&amp;buf_lock); &#125;&#125;// 创建需要推后完成到工作，名为key_work，待执行函数为key_work_funcDECLARE_WORK(key_work, key_work_func); // 声明工作队列irq_handler_t key_int_handler(int irq, void *dev) &#123; spin_lock(&amp;key_lock); scancode = inb(0x60); // 获取扫描码 spin_unlock(&amp;key_lock); spin_lock(&amp;key_lock); status = inb(0x64); // 获取按键状态 spin_unlock(&amp;key_lock); // printk("Key interrupt: scancode = 0x%x, status = 0x%x\n", scancode, status); schedule_work(&amp;key_work); // 调度工作队列 return (irq_handler_t)IRQ_HANDLED;&#125;static int myopen(struct inode *inode, struct file *filp) &#123; // 打开设备函数 printk("[+] Device opened.\n"); return 0;&#125;static ssize_t myread(struct file *filp, char __user *buf, size_t count, loff_t *pos) &#123; // 读设备函数 unsigned char *c; int ret; if (count == 0) &#123; // 传入的count值不能为0 printk("[!] Count can not be 0.\n"); return -1; &#125; printk("[DEBUG] kfifo_len = %d.\n", kfifo_len(&amp;key_buf)); if (wait_event_interruptible(waitq, (kfifo_len(&amp;key_buf) &gt;= count))) // 睡眠并等待唤醒 return -ERESTARTSYS; c = (unsigned char *)kmalloc(count, GFP_KERNEL); // 申请一块内存 mutex_lock(&amp;buf_lock); kfifo_out(&amp;key_buf, c, count); // 将指定长度扫描码数据出队列 mutex_unlock(&amp;buf_lock); printk("[+] Copy buffer to user: %s.\n", c); ret = copy_to_user(buf, c, count); // 将出队列的扫描码传给用户空间 kfree(c); // 释放申请的内存空间 c = 0; // 防止UAF return count;&#125;static int myrelease(struct inode *inode, struct file *filp) &#123; // 释放设备函数 printk("[+] Device released.\n"); return 0;&#125;struct file_operations key_fops = &#123; // 初始化文件访问操作函数 .open = myopen, .read = myread, .release = myrelease,&#125;;static int __init my_init(void) &#123; // 模块入口函数 int ret; struct device *dev; printk("========== [+] Init module. ==========\n"); ret = kfifo_alloc(&amp;key_buf, 32, GFP_ATOMIC); // 申请kfifo队列 if (ret) &#123; printk("[!] Allocate memory failed.\n"); return ret; &#125; printk("[+] Allocate kfifo successfully.\n"); // Trigger interrupt ret = request_irq(1, (irq_handler_t)key_int_handler, IRQF_SHARED, "Key Hook", (void *)key_int_handler); // 申请IRQ if (ret) &#123; printk("[!] Request irq failed.\n"); return ret; &#125; printk("[+] Request irq successfully.\n"); // Register devices printk("[*] Invoke alloc_chrdev_region.\n"); ret = alloc_chrdev_region(&amp;dev_id, 0, 1, "mychar"); // 动态分配设备编号 if (ret &gt;= 0) &#123; printk("[*] Invoke cdev_init.\n"); cdev_init(&amp;key_dev, &amp;key_fops); // 初始化字符设备 key_dev.owner = THIS_MODULE; // 设置实现驱动的模块为当前模块 printk("[*] Invoke cdev_add.\n"); ret = cdev_add(&amp;key_dev, dev_id, 1); // 添加字符设备到系统中 if (ret &gt;= 0) &#123; printk("[*] Invoke class_create.\n"); key_class = class_create(THIS_MODULE, "mychar"); // 创建一个类并注册到内核中 if (key_class) &#123; printk("[*] Invoke device_create.\n"); dev = device_create(key_class, NULL, dev_id, NULL, "mychar"); // 创建一个设备并注册到sysfs中 if (dev) &#123; goto success; &#125;else &#123; printk("[!] device_create failed.\n"); &#125; &#125; else &#123; class_destroy(key_class); // 删除类 printk("[!] Invoke class_create failed.\n"); &#125; cdev_del(&amp;key_dev); // 删除字符设备 &#125; else &#123; printk("[!] Invoke cdev_add failed.\n"); &#125; unregister_chrdev_region(dev_id, 1); // 释放设备编号 return ret; &#125;success: printk("[+] Create charactor device successfully.\n"); return 0;&#125;static void __exit my_exit(void) &#123; printk("========== [+] Remove module. ==========\n"); printk("[*] Free irq.\n"); free_irq(1, (void *)key_int_handler); // 释放IRQ printk("[*] Free kfifo.\n"); kfifo_free(&amp;key_buf); // 释放队列 printk("[*] Invoke device_destroy.\n"); device_destroy(key_class, dev_id); // 删除设备 printk("[*] Invoke class_destroy.\n"); class_destroy(key_class); // 删除类 printk("[*] Invoke cdev_del.\n"); cdev_del(&amp;key_dev); // 删除字符设备 printk("[*] Invoke unregister_chrdev_region.\n"); unregister_chrdev_region(dev_id, 1); // 释放设备编号&#125;module_init(my_init);module_exit(my_exit); 1234567891011121314151617181920212223242526user.c#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;fcntl.h&gt;#include &lt;string.h&gt;#define BUF_SIZE 32 // 设置缓冲区int main() &#123; int fd, c, i=0; unsigned char buf[BUF_SIZE]; // 初始化缓冲区 // sudo mknod /dev/ex4 c 444 0 fd = open("/dev/mychar", O_RDONLY); // 打开字符设备 if (fd &lt; 0) &#123; printf("[!] File open error.\n"); return -1; &#125; char chr=-2; while (1) &#123; c = read(fd, buf, BUF_SIZE); // 读取字符设备中的扫描码 printf("[+] Read scancode: %s (%d).\n", buf, c); // 把扫描码以字符串的形式输出 memset(buf,'\0',sizeof(buf)); &#125; close(fd); // 关闭字符设备 return 0;&#125;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>中断</tag>
        <tag>字符驱动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[open系统调用（一）]]></title>
    <url>%2FLinux%2Fopen%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1.前言用户进程在能够读/写一个文件之前必须要先“打开”这个文件。对文件的读/写从概念上说是一种进程与文件系统之间的一种“有连接”通信，所谓“打开文件”实质上就是在进程与文件之间建立起链接。在文件系统的处理中，每当一个进程重复打开同一个文件时就建立起一个由file数据结构代表的独立的上下文。通常，一个file数据结构，即一个读/写文件的上下文，都由一个打开文件号（fd）加以标识。 2. do_sys_open打开文件的系统调用是open()，在内核中通过do_sys_open()实现，其代码在 fs/open.c中: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273long do_sys_open(int dfd, const char __user *filename, int flags, int mode)&#123; struct open_flags op; //检查给定的 flags 参数是否有效，并处理不同的 flags 和 mode 条件。 int lookup = build_open_flags(flags, mode, &amp;op); //将文件路径从用户空间拷贝到内核空间。 char *tmp = getname(filename); int fd = PTR_ERR(tmp); if (!IS_ERR(tmp)) &#123; //从当前进程的打开文件表（files_struct）中找到一个空闲的表项，该表项的下标即为“打开文件号”。 fd = get_unused_fd_flags(flags); if (fd &gt;= 0) &#123; //创建进程与文件的链接，或者说创建file结构体代表该读写文件的上下文。 struct file *f = do_filp_open(dfd, tmp, &amp;op, lookup); if (IS_ERR(f)) &#123; //如果发生了错误，释放已分配的 fd 文件描述符 put_unused_fd(fd); //释放已分配的 struct file fd = PTR_ERR(f); &#125; else &#123; fsnotify_open(f); //将这个file结构体的指针填入当前进程的打开文件表中。 fd_install(fd, f); &#125; &#125; putname(tmp); &#125; return fd;&#125;pathname:代表需要打开的文件的文件名；flags：表示打开的标识；O_ACCMODE&lt;0003&gt;: 读写文件操作时，用于取出flag的低2位。O_RDONLY&lt;00&gt;: 只读打开O_WRONLY&lt;01&gt;: 只写打开O_RDWR&lt;02&gt;: 读写打开O_CREAT&lt;0100&gt;: 文件不存在则创建，需要mode_tO_EXCL&lt;0200&gt;: 如果同时指定了O_CREAT，而文件已经存在，则出错O_NOCTTY&lt;0400&gt;: 如果pathname代表终端设备，则不将此设备分配作为此进程的控制终端O_TRUNC&lt;01000&gt;: 如果此文件存在，而且为只读或只写成功打开，则将其长度截短为0 O_APPEND&lt;02000&gt;: 每次写时都加到文件的尾端O_NONBLOCK&lt;04000&gt;: 如果p a t h n a m e指的是一个F I F O、一个块特殊文件或一个字符特殊文件，则此选择项为此文件的本次打开操作和后续的I / O操作设置非阻塞方式。O_NDELAY&lt;O_NONBLOCK&gt;O_SYNC&lt;010000&gt;: 使每次write都等到物理I/O操作完成。FASYNC&lt;020000&gt;: 兼容BSD的fcntl同步操作O_DIRECT&lt;040000&gt;: 直接磁盘操作标识，每次读写都不使用内核提供的缓存，直接读写磁盘设备O_LARGEFILE&lt;0100000&gt;: 大文件标识O_DIRECTORY&lt;0200000&gt;: 必须是目录O_NOFOLLOW&lt;0400000&gt;: 不获取连接文件O_NOATIME&lt;01000000&gt;: 暂无mode:当新创建一个文件时，需要指定mode参数，以下说明的格式如宏定义名称&lt;实际常数值&gt;: 描述如下：S_IRWXU&lt;00700&gt;：文件拥有者有读写执行权限S_IRUSR (S_IREAD)&lt;00400&gt;：文件拥有者仅有读权限S_IWUSR (S_IWRITE)&lt;00200&gt;：文件拥有者仅有写权限S_IXUSR (S_IEXEC)&lt;00100&gt;：文件拥有者仅有执行权限S_IRWXG&lt;00070&gt;：组用户有读写执行权限S_IRGRP&lt;00040&gt;：组用户仅有读权限S_IWGRP&lt;00020&gt;：组用户仅有写权限S_IXGRP&lt;00010&gt;：组用户仅有执行权限S_IRWXO&lt;00007&gt;：其他用户有读写执行权限S_IROTH&lt;00004&gt;：其他用户仅有读权限S_IWOTH&lt;00002&gt;：其他用户仅有写权限S_IXOTH&lt;00001&gt;：其他用户仅有执行权限 dfd是传入的AT_FDCWD，其定义在 include/uapi/linux/fcntl.h。该值表明当 filename 为相对路径的情况下将当前进程的工作目录设置为起始路径。相对而言， 你可以在另一个系统调用 openat 中为这个起始路径指定一个目录， 此时 AT_FDCWD 就会被该目录的描述符所替代。 进程的task_struct结构中有个指针files，指向本进程的files_struct数据结构。与打开文件有关的信息都保存在这个数据结构中，其定义在include/linux/sched.h 中: 1234567891011121314151617181920212223struct files_struct &#123;atomic_t count;//引用计数struct fdtable __rcu *fdt;//指向自身成员fdtab或动态分配的struct fdtable实例struct fdtable fdtab;//当可打开文件的最大数目为NR_OPEN_DEFAULT时使用spinlock_t file_lock ____cacheline_aligned_in_smp;int next_fd;//已分配的描述符加1struct embedded_fd_set close_on_exec_init;//位图，比特位数目刚好与NR_OPEN_DEFAULT一致struct embedded_fd_set open_fds_init;struct file __rcu * fd_array[NR_OPEN_DEFAULT];&#125;;struct fdtable &#123;unsigned int max_fds;//当前可打开文件的最大数目，决定struct file指针数组fd的大小，close_on_exec和open_fds位图的比特位总数struct file __rcu **fd;//指向内置它的struct files_struct结构体的成员fd_array或动态分配的struct file指针数组fd_set *close_on_exec;//指向内置它的struct files_struct结构体的成员close_on_exec_init或动态分配的位图fd_set *open_fds;//指向内置它的struct files_struct结构体的成员open_fds_init或动态分配的位图 //后两个成员在销毁时使用 struct rcu_head rcu; //rcu机制 struct fdtable *next; //用于加入fdtable_defer_list链表&#125;; 其结构如下图所示。 3.do_filp_opensys_do_open主要调用的函数为do_filp_open。 123456789101112131415161718struct file * do_filp_open(int dfd, struct filename * pathname, const struct open_flags * op) &#123; struct nameidata nd; int flags = op-&gt;lookup_flags; struct file * filp; set_nameidata(&amp;nd, dfd, pathname); filp = path_openat(&amp;nd, op, flags | LOOKUP_RCU); if (unlikely(filp == ERR_PTR(-ECHILD))) filp = path_openat(&amp;nd, op, flags); if (unlikely(filp == ERR_PTR(-ESTALE))) filp = path_openat(&amp;nd, op, flags | LOOKUP_REVAL); restore_nameidata(); return filp;&#125; 参数 dfd 是相对路径的基准目录对应的文件描述符，参数 name 指向文件路径，参数 op 是查找标志。=在路径查找中有个很重要的数据结构 nameidata 用来向解析函数传递参数，保存解析结果。 123456789101112131415161718192021222324252627struct nameidata &#123; struct path path; struct qstr last; struct path root; struct inode * inode; //path.dentry.d_inode unsigned int flags; unsigned seq, m_seq; int last_type; unsigned depth; int total_link_count; struct saved &#123; struct path link; void * cookie; const char * name; struct inode * inode; unsigned seq; &#125; * stack, internal[EMBEDDED_LEVELS]; struct filename * name; struct nameidata * saved; unsigned root_seq; int dfd;&#125;; 成员 last 存放需要解析的文件路径的分量（以前提到的组件）,是一个快速字符串(quick string),不仅包字符串,还包含长度和散列值。成员 path 存放解析得到的挂载描述符和目录项,成员 iode 存放目录项对应的索引节点。path 保存已经成功解析到的信息，last 用来存放当前需要解析的信息，如果 last 解析成功那么就会更新 path。 函数 do_flp_open 三次调用函数 path_openat以解析文件路径。 第一次解析传入标志 LOOKUP_RCU,使用 RCU 查找(rcu-walk)方式。在散列表中根据{父目录, 名称}查找目录的过程中,使用 RCU 保护散列桶的链表,使用序列号保护目录，其他处理器可以并行地修改目录, RCU 查找方式速度最快。 如果在第一次解析的过程中发现其他处理器修改了正在查找的目录,返回错误号-ECHILD，那么第二次使用引用查找（ref-walk）REF 方式，在散列表中根据{父目录, 名称}查找目录的过程中,使用 RCU 保护散列桶的链表,使用自旋锁保护目录，并且把目录的引用计数加1。引用查找方式速度比较慢。 网络文件系统的文件在网络的服务器上，本地上次查询得到的信息可能过期,和服务器的当前状态不一致。如果第二次解析发现信息过期，返回错误号 -ESTALE，那么第三次解析传入标志 LOOKUP_REVAL，表示需要重新确认信息是否有效。 调用 set_nameidata() 保护当前进程现场信息。 接着调用 filp = path_openat(&amp;nd, op, flags | LOOKUP_RCU); 4.path_openat123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566static struct file * path_openat(struct nameidata * nd, const struct open_flags * op, unsigned flags) &#123; const char * s; struct file * file; int opened = 0; int error; // 获取一个空的 file 描述符。 file = get_empty_filp(); //获取失败，返回。 if (IS_ERR(file)) return file; //设置 file 描述符的查找标志。 file-&gt;f_flags = op-&gt;open_flag; // 如果是本次目标是创建一个临时文件，这里就不深入了，只研究正常的文件打开操作。 if (unlikely(file-&gt;f_flags &amp; __ O_TMPFILE)) &#123; error = do_tmpfile(nd, flags, op, file, &amp;opened); goto out2; &#125; // 路径初始化，确定查找的起始目录，初始化结构体 nameidata 的成员 path。 s = path_init(nd, flags); //如果获取的路径（待查找的路径）无效，则释放 file 描述符，返回错误。 if (IS_ERR(s)) &#123; put_filp(file); return ERR_CAST(s); &#125; // 调用函数 link_path_walk 解析文件路径的每个分量，最后一个分量除外。 // 调用函数 do_last，解析文件路径的最后一个分量，并且打开文件。 while (! (error = link_path_walk(s, nd)) &amp;&amp; (error = do_last(nd, file, op, &amp;opened)) &gt; 0) &#123; nd-&gt;flags &amp;= ~(LOOKUP_OPEN | LOOKUP_CREATE | LOOKUP_EXCL); // 如果最后一个分量是符号链接，调用 trailing_symlink 函数进行处理 // 读取符号链接文件的数据，新的文件路径是符号链接链接文件的数据，然后继续 while // 循环，解析新的文件路径。 s = trailing_symlink(nd); if (IS_ERR(s)) &#123; error = PTR_ERR(s); break; &#125; &#125; // 结束查找，释放解析文件路径的过程中保存的目录项和挂载描述符。 terminate_walk(nd);out2: if (! (opened &amp; FILE_OPENED)) &#123; BUG_ON(!error); put_filp(file); &#125; if (unlikely(error)) &#123; if (error == -EOPENSTALE) &#123; if (flags &amp; LOOKUP_RCU) error = -ECHILD; else error = -ESTALE; &#125; file = ERR_PTR(error); &#125; return file;&#125;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>文件系统</tag>
        <tag>open系统调用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编写字符设备驱动程序]]></title>
    <url>%2FLinux%2F%E7%BC%96%E5%86%99%E5%AD%97%E7%AC%A6%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[1. 前言本文主要讨论操作系统究竟如何与设备进行通信，以编写一个字符驱动程序为主线，从用户进程–&gt;系统调用–&gt;文件系统–&gt;驱动程序–&gt;设备控制器–&gt;设备这几个方面，结合程序何内核源码，探索操作系统与驱动程序的奥秘。 2. 如何与设备通信？主要有两种方式来实现与设备的交互。 第一种方法是使用明确的IO指令，这些指令规定了将数据发送到特定设备寄存器的方法。例如在x86上，in和out指令可以用来与设备交互，调用者指定一个存入数据的特定寄存器及一个代表设备的特定端口，执行该指令，就可实现需求。 12IN AL,21H；表示从21H端口读取一字节数据到ALOUT 21H,AL；将AL的值写入21H端口 第二种方法是内存映射IO。这种方式是将设备寄存器当作内存地址使用，当需要访问设备寄存器时，操作系统读取或者存入到该内存地址，然后硬件会将地址转移到设备上，而不是物理内存。 3. 如何实现一个设备无关的操作系统？设备五花八门，每个设备都有自己非常具体的接口，如何将他们接入操作系统，又能让操作系统尽可能的通用，是摆在操作系统开发者面前的大问题。这种问题，开发者都是通过抽象技术来解决。例如，文件系统实现了对数据存储、组织形式的抽象，用户态程序不必再操心，数据如何在磁盘中组织、存储。除了组织方式的多样化，还有物理设备的多样化，文件系统并不那么清楚对不同设备发出读写请求的全部细节。在底层设计一部分软件，清楚的知道设备如何进行工作，我们将这部分软件称为设备驱动程序，所有设备交互的细节都封装在其中。文件系统栈如图所示。 4. 简单的字符设备驱动程序字符设备是指只能一个字节一个字节读写的设备，不能随机读取设备内存中的某一数据，读取数据需要按照先后数据。字符设备是面向流的设备，常见的字符设备有鼠标、键盘、串口、控制台和LED设备等。 4.1 创建设备文件每一个字符设备或块设备都在/dev目录下对应一个设备文件。Linux用户程序通过设备文件（或称设备节点）来使用驱动程序操作字符设备和块设备。 创建设备文件的基本方式是使用mknod，语法如下： mknod [选项] 设备名 设备类型 主设备号 次设备号 设备类型：b，块设备；c，字符设备；u，没有缓冲的字符设备；p，fifo设备 例如创建一个名称为szp的字符设备文件，主设备号为2，次设备号为1。 1mknod /dev/szp c 2 1 但是更多情况下，设备文件在驱动程序加载的时候就自动创建好了，在驱动程序中使用class_create，device_create两个函数创建设备文件。 123 // creating device nodecl = class_create(THIS_MODULE,DEMO_NAME);device_create(cl, NULL, demo_cdev-&gt;dev,NULL,DEMO_NAME); 在/dev目录下查看所有设备文件，其中标红的就是本次将要编写的字符驱动程序的设备文件。 上面创建设备文件的两行代码中涉及了字符驱动程序的两个重要结构体，demo_cdev和dev。他们的类型分别为struct cdev和dev_t。 12static struct cdev *demo_cdev;static dev_t dev; 内核中使用struct cdev表示一个字符设备： 12345678struct cdev &#123; struct kobject kobj; // 每个cdev 都是一个 kobject struct module *owner; // 指向实现驱动的模块 const struct file_operations *ops; // 操纵这个字符设备文件的方法 struct list_head list; // 与cdev 对应的字符设备文件的 inode-&gt;i_devices 的链表头 dev_t dev; // 起始设备编号 unsigned int count; // 设备范围号大小&#125;; 内核用dev_t类型（&lt;linux/types.h&gt;）来保存设备编号。 创建设备文件是编写设备驱动程序的最后一步。字符驱动程序的编写总共可以归结为四步： 申请设备编号。 申请并初始化（编写file_operations操作集）cdev结构体。 将设备编号与cdev结构体关联，注册到操作系统。 创建设备文件。 4.2 申请设备编号设备号在驱动程序中起什么作用？为什么要有主设备号和次设备号？ 申请设备号用如下两个函数：//自动分配设备号alloc_chrdev_region() //分配已设定的设备号register_chrdev_region() 通常而言，主设备号标识设备对应的驱动程序。例如，/dev/null和/dev/zero由驱动程序1管理、而虚拟控制台和串口终端由驱动程序4管理。现代的Linux内核允许多个驱动程序共享主设备号，但我们看到的大多数设备仍然按照“一个主设备号对应一个驱动程序”的原则组织。 次设备号由内核使用，用于正确确定设备文件所指的设备。依赖于驱动程序的编写方式，我们可以通过次设备号获得一个指向内核设备的直接指针，也可将次设备号当作设备本地数组的索引。不管用哪种方式，除了知道次设备号用来指向驱动程序所实现的设备之外，内核本身基本上不关心关于次设备号的任何其他信息。 4.3 申请并初始化cdevcdev定义有两种方式，struct cdev cdev；另外一种struct cdev *cdev; cdev=cdev_alloc();一种静态声明定义，另一种动态分配。cdev通过函数cdev_init()初始化，主要工作就是将file_operations和cdev关联起来。file_operations是字符驱动需要实现的主要内容。 4.4 注册设备驱动程序那么驱动程序的注册和注销函数都做了哪些工作？为什么要进注册和注销？ cdev通过cdev_add()实现cdev的注册，所谓注册就是将cdev根据设备号（dev_t）添加到cdev数组（cdev_map）中供系统管理。 如果它返回一个负的错误码，则设备不会被添加到系统中。但这个调用几乎总会成功返回，此时，我们又面临另一个问题:只要cdev_add返回了，我们的设备就“活”了，它的操作就会被内核调用。因此，在驱动程序还没有完全准备好处理设备上的操作时，就不能调用cdev_add。 cdev通过cdev_del()将cdev从cdev_map中移除。要清楚的是，在将cdev结构传递到cdev_del函数之后，就不应再访问cdev结构了。 5. 用户态程序用户态通过/dev目录下的设备文件，与字符设备驱动程序进行交互。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# include &lt;stdio.h&gt;# include &lt;fcntl.h&gt;# include &lt;unistd.h&gt;# include &lt;string.h&gt;# define DEMO_DEV_NAME "/dev/my_demo_dev"int trace_fd = -1;int marker_fd = -1;char *debugfs = "/sys/kernel/debug";void trace_on()&#123; char path[256]; strcpy(path, debugfs); strcat(path,"/tracing/tracing_on"); trace_fd = open(path, O_WRONLY); if (trace_fd &gt;= 0) write(trace_fd, "1", 1); strcpy(path, debugfs); strcat(path,"/tracing/trace_marker"); marker_fd = open(path, O_WRONLY); if (marker_fd &gt;= 0) write(marker_fd, "In critical area\n", 17);&#125;void trace_off()&#123; if (marker_fd &gt;= 0) write(marker_fd, "Out critical area\n", 17); write(trace_fd, "0", 1); close(trace_fd); close(marker_fd); trace_fd = -1; marker_fd = -1;&#125;int main() &#123; char buffer[64]; int fd; fd = open(DEMO_DEV_NAME,O_RDONLY); if(fd&lt;0) &#123; printf("open device %s failed\n",DEMO_DEV_NAME); return -1; &#125; trace_on(); read(fd,buffer,64); trace_off(); close(fd); return 0;&#125; 5.1 open、read系统调用函数用户态程序使用open系统调用，打开刚刚编写的设备文件，使用read系统调用读取字符设备。 这些系统调用函数如何陷入内核？又是如何和file_operations函数集进行关联？ 使用strace命令跟踪用户态程序，发现确实是进行了open和read系统调用，然后无法看到内核的函数调用关系。 Linux系统，用户空间通过向内核空间发出Syscall，产生软中断， 从而让程序陷入内核态，执行相应的操作。对于每个系统调用都会有一个对应的系统调用号。 用户空间的方法xxx，对应系统调用层方法则是 sys_xxx； unistd.h 文件记录着系统调用中断号的信息。 宏定义 SYSCALL_DEFINEx(xxx,…)，展开后对应的方法则是 sys_xxx； 方法参数的个数x，对应于 SYSCALL_DEFINEx。 open系统调用处理函数如下 12345678910111213SYSCALL_DEFINE3(open, const char __user *, filename, int, flags, int, mode)&#123; long ret; if (force_o_largefile()) flags |= O_LARGEFILE; ret = do_sys_open(AT_FDCWD, filename, flags, mode); /* avoid REGPARM breakage on x86: */ asmlinkage_protect(3, ret, filename, flags, mode); return ret;&#125; 其调用了do_sys_open函数进行处理。在使用ftrace工具根据内核函数调用关系时，由于内容较多，不容易找到我们的用户态程序进行的系统调用，因此编写两个函数trace_on和trace_off，使用trace_marker在用户态系统调用前和系统调用后，打印一个日志标签，In critical area和Out critical area，方便查找。用于跟踪内核函数调用关系的脚本程序如下： 1234567891011121314#!/bin/sh#ftrace.sh dir="/sys/kernel/debug/tracing/"save="/home/szp/" echo 0 &gt; $&#123;dir&#125;tracing_onecho function_graph &gt; $&#123;dir&#125;current_tracerecho sys_open &gt; $&#123;dir&#125;set_graph_functionecho 1 &gt; $&#123;dir&#125;tracing_onsleep 5echo 0 &gt; $&#123;dir&#125;tracing_oncat $&#123;dir&#125;trace &gt; $&#123;save&#125;trace_records 可以从以下调用结构中，可以看到最终调用了我们编写的demodrv_read()。 read系统调用的处理函数如下所示。 1234567891011121314151617SYSCALL_DEFINE3(read, unsigned int, fd, char __user *, buf, size_t, count)&#123; struct file *file; ssize_t ret = -EBADF; int fput_needed; file = fget_light(fd, &amp;fput_needed); if (file) &#123; loff_t pos = file_pos_read(file); ret = vfs_read(file, buf, count, &amp;pos); file_pos_write(file, pos); fput_light(file, fput_needed); &#125; return ret;&#125; vfs_read函数如下所示。 12345678910111213141516171819202122232425262728ssize_t vfs_read(struct file *file, char __user *buf, size_t count, loff_t *pos)&#123; ssize_t ret; if (!(file-&gt;f_mode &amp; FMODE_READ)) return -EBADF; if (!file-&gt;f_op || (!file-&gt;f_op-&gt;read &amp;&amp; !file-&gt;f_op-&gt;aio_read)) return -EINVAL; if (unlikely(!access_ok(VERIFY_WRITE, buf, count))) return -EFAULT; ret = rw_verify_area(READ, file, pos, count); if (ret &gt;= 0) &#123; count = ret; if (file-&gt;f_op-&gt;read) //调用file结构体中的file_operations函数集中自定义的read函数，本次为demodrv_read() ret = file-&gt;f_op-&gt;read(file, buf, count, pos); else ret = do_sync_read(file, buf, count, pos); if (ret &gt; 0) &#123; fsnotify_access(file); add_rchar(current, ret); &#125; inc_syscr(current); &#125; return ret;&#125; 使用ftrace对内核函数进行跟踪，其调用关系如下所示。可以看到vfs_read中调用了我们在字符驱动程序中定义的demodrv_read()。 自定义file_operations函数集如下所示： 12345678910111213141516171819202122static ssize_t demodrv_read(struct file *file, char __user *buf,size_t lbuf,loff_t *ppos)&#123; printk("%s enter\n",__func__); return 0;&#125;static ssize_t demodrv_write(struct file *file, const char __user *buf,size_t count,loff_t *f_pos)&#123; printk("%s enter\n",__func__); return 0;&#125;static const struct file_operations demodrv_fops = &#123; .owner = THIS_MODULE, .open = demodrv_open, .read = demodrv_read, .write = demodrv_write&#125;; 5.2 装载字符驱动程序内核模块的Makefile文件如下： 123456789101112131415#Makefile文件注意：假如前面的.c文件起名为first.c，那么这里的Makefile文件中的.o文#件就要起名为first.o 只有root用户才能加载和卸载模块obj-m:=device_driver.o #产生device_drive模块的目标文件#目标文件 文件 要与模块名字相同CURRENT_PATH:=$(shell pwd) #模块所在的当前路径LINUX_KERNEL:=$(shell uname -r) #linux内核代码的当前版本LINUX_KERNEL_PATH:=/usr/src/linux-headers-$(LINUX_KERNEL)all: make -C $(LINUX_KERNEL_PATH) M=$(CURRENT_PATH) modules #编译模块#[Tab] 内核的路径 当前目录编译完放哪 表明编译的是内核模块clean: make -C $(LINUX_KERNEL_PATH) M=$(CURRENT_PATH) clean #清理模块 使用make编译内核模块得到device_driver.ko文件。 使用insmod插入内核，并使用dmesg查看系统日志。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>字符驱动程序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 18降低内核版本]]></title>
    <url>%2FLinux%2Freduce-kernel-version%2F</url>
    <content type="text"><![CDATA[Ubuntu18默认的内核版本已经到了5.x，由于一些需要，需要将内核版本降低，下面记录一下，将内核版本降低到4.15.0-30的过程。 查找内核可用版本 grep menuentry /boot/grub/grub.cfg 如下所示有5.4和4.15两个版本，通常情况下我们需要的版本是没有的，需要去下载。 szp@szp-pc:~$ grep menuentry /boot/grub/grub.cfg if [ x&quot;${feature_menuentry_id}&quot; = xy ]; then menuentry_id_option=&quot;--id&quot; menuentry_id_option=&quot;&quot; export menuentry_id_option menuentry &apos;Ubuntu&apos; --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option &apos;gnulinux-simple-735805aa-d418-4396-ac05-1617c239261c&apos; { submenu &apos;Ubuntu 高级选项&apos; $menuentry_id_option &apos;gnulinux-advanced-735805aa-d418-4396-ac05-1617c239261c&apos; { menuentry &apos;Ubuntu，Linux 5.4.0-53-generic&apos; --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option &apos;gnulinux-5.4.0-53-generic-advanced-735805aa-d418-4396-ac05-1617c239261c&apos; { menuentry &apos;Ubuntu, with Linux 5.4.0-53-generic (recovery mode)&apos; --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option &apos;gnulinux-5.4.0-53-generic-recovery-735805aa-d418-4396-ac05-1617c239261c&apos; { menuentry &apos;Ubuntu，Linux 4.15.0-30-generic&apos; --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option &apos;gnulinux-4.15.0-30-generic-advanced-735805aa-d418-4396-ac05-1617c239261c&apos; { menuentry &apos;Ubuntu, with Linux 4.15.0-30-generic (recovery mode)&apos; --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option &apos;gnulinux-4.15.0-30-generic-recovery-735805aa-d418-4396-ac05-1617c239261c&apos; { menuentry &apos;Memory test (memtest86+)&apos; { menuentry &apos;Memory test (memtest86+, serial console 115200)&apos; { 下载所需内核版本 sudo apt-get install linux-headers-4.15.0-30-generic linux-image-4.15.0-30-generic 这里下载headers和image。 修改GRUB编辑/etc/default/grub文件中的GRUB_DEFAULT默认为GRUB_DEFAULT=0 vim /etc/default/grub 英文版Ubuntu修改启动的版本信息如下 GRUB_DEFAULT=”Advanced options for Ubuntu &gt; Ubuntu, with Linux 4.15.0-30-generic” 中文版如下 GRUB_DEFAULT=”Ubuntu 高级选项&gt;Ubuntu，Linux 4.15.0-30-generic” 修改完成后如下： 1 # If you change this file, run &apos;update-grub&apos; afterwards to update 2 # /boot/grub/grub.cfg. 3 # For full documentation of the options in this file, see: 4 # info -f grub -n &apos;Simple configuration&apos; 5 GRUB_DEFAULT=&quot;Ubuntu 高级选项&gt;Ubuntu，Linux 4.15.0-30-generic&quot; 6 GRUB_TIMEOUT_STYLE=hidden 7 GRUB_TIMEOUT=0 8 GRUB_DISTRIBUTOR=`lsb_release -i -s 2&gt; /dev/null || echo Debian` 9 GRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet splash&quot; 10 GRUB_CMDLINE_LINUX=&quot;&quot; 保存以后执行更新操作： update-grub 最后reboot重启系统，可以使用uname -r查看内核版本已经变了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程管理笔记]]></title>
    <url>%2FLinux%2F%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1.前言进程的定义非常简单：进程就是运行中的程序。程序本身没有生命周期，它只是存在磁盘上面的一些指令或者静态数据。这些字节需要操作系统的帮助运行起来，发挥其应该有的作用。 我们在使用计算机的时候，会喜欢一边听着音乐，一边玩着游戏，这样是没问题的，我们的操作系统可能有上百个进程在同时运行。但是我们的硬件资源是有限的，假设一个计算机只有一个CPU（实际上也不会太多），成百个进程都在运行着，那CPU应该给谁用呢？操作系统就创造了一个美丽的假象-虚拟化CPU，将一个物理CPU虚拟成多个虚拟CPU分给每一个进程使用，因此每一个进程都以为自己在独占CPU。 2.如何提供有许多CPU的假象？通过让一个进程只运行一个时间片，然后切换到其他进程，操作系统提供了存在多个虚拟CPU的假象。这叫做时分共享CPU技术。那么操作系统如何完成这一切功能。 2.1 操作系统是管理多个进程执行的软件简单来看，操作系统就是一组系统调用API。进程执行系统调用，会使用指令（syscall）回到操作系统的代码执行，当然这就属于进程主动的触发中断；除此之外，还有硬件中断（时钟，I/O设备），对于一个进程来说，是被动发生的。假设以两个程序qq和微信为例，在最开始的时候肯定执行的操作系统程序。将物理内存分别分一部分给微信、qq和内核，我们上面说了进程是运行的程序，其是动态变化的，也就是包含其占有的内存和寄存器，里面存储了当前进程的状态。操作系统代码执行一段时间以后，加载微信程序，也就是恢复它的内存和寄存器状态，让其在CPU上运行，这一过程通过iret中断指令完成。接下来执行权就到了微信进程，也叫做用户态。当然当前进程不能无限执行下去，通过一次中断或者系统调用，会再次执行os代码，os会负责保存微信进行的执行现场也就是内存和寄存器信息。最后再次加载qq程序，一直这样切换执行下去。操作系统始终在内核态运行，而应用程序始终在用户态运行。正是通过这些切换，操作系统实现了对CPU的虚拟化，当某个应用程序运行的时候它感觉到的就是现在它独占了CPU资源。 3.操作系统提供哪些系统调用API?上面提到了一个进程可以主动放弃自己的执行权，也是就通过系统调用，将执行权交还给操作系统。那么操作系统内核都提供了哪些系统调用来管理进程？主要有创建（fork）、替换执行(execve)、删除(exit)。 3.1创建（fork）为什么需要这么一个系统调用呢，因为我们的系统在最开始启动后并不会有太多的进程，需要有一个系统调用能够在接下来的使用中创建更多的进程。例如我们的shell，可以在shell中使用命令创建一个进程。fork就像一把叉子，主要完成 做一份进程完整的复制（内存、寄存器运行现场） 父进程返回子进程的pid，子进程返回0 那么创建一个进程为什么要叫做叉子呢？还是以我们刚才的qq进程为例，如果它调用了fork系统调用就会在当前系统中多出一个进程，而这个进程是之前qq进程的拷贝，它包括几乎所有的进程当前的状态（除了返回值pid），包括内存、寄存器。原来的进程也叫父进程和新创建的进程也叫子进程都可以继续向下执行，所以说它像一个叉子，走着走着就分了个叉。父进程返回值是子进程的pid，子进程的返回值是0。 3.2 直面fork下面通过两个小实验，来看一下fork究竟是如何工作的。 3.2.1 实验一123456789101112 1 #include&lt;unistd.h&gt; 2 #include&lt;stdio.h&gt; 3 4 int main()&#123; 5 pid_t pid1 = fork(); 6 pid_t pid2 = fork(); 7 pid_t pid3 = fork(); 8 printf("Hello world : (%d,%d,%d)\n",pid1,pid2,pid3); 9 return 0;1011 12 &#125; 根据父进程返回子进程的pid，子进程返回0，我们可以对上面的程序进行画图分析，括号中的内容用于记录三个变量的值和接下来要执行的PC指令，～代表目前未知的变量值，（pid1,pid2,pid3,PC）。程序执行完，一共会有八个进程对printf语句进行输出。红色标出的便是最终的结果。 下面运行程序，查看实际的运行结果，结果和上面画图分析的结果是一致的，共有八种不同的结果输出。做完该实验就能理解子进程是父进程的一个拷贝，其中黑色标注出的0和FORK2_PID都会一直传递下去，并且fork会返回不同的函数值给父子进程。 3.2.2 实验21234567891011 1 #include&lt;unistd.h&gt; 2 #include&lt;stdio.h&gt; 3 #define n 2 4 int main()&#123; 5 for(int i = 0; i &lt; n; i++) 6 &#123; 7 fork(); 8 printf("Hello world\n"); 9 &#125;10 return 0;11 &#125; 程序的执行流程如下： 可以看到程序会打印出六个Hello world，我们执行程序结果也是一样的。 似乎毫无疑问，现在将输出结果重定向到一个文件中。 重定向输出结果到一个txt文件后，发现比刚才控制台多了两个输出。这是为什么呢？原因就在于printf函数，printf函数在标准输出为控制台的时候会直接输出，但是当标准输出是一个文件的时候，printf会将结果暂存在缓冲区中，在第二次fork的时候，毫无疑问，缓冲区中的hello world 也被拷贝了一份，如下图所示，因此最终文件中就包含了8个Hello world。 3.3 execve系统调用除了能够对进程进行创建，还需要能够执行别的程序，execve系统调用就是这样一个功能，将当前运行的进程“替换”成另一个程序，从头开始执行。fork以后得到的进程是与父进程相同的，但是跟多的时候我们创建子进程是为了让他执行不同的程序，去做别的工作。execve就是启动一个新程序的系统调用。exec函数一共有六个，其中execve为内核级系统调用，其他（execl，execle，execlp，execv，execvp）都是调用execve的库函数。 123execve(filename,argv,enpv)//执行名称为filename的程序//分别传入参数v，环境变量e 一个进程调用execve以后，它没有创建新的进程，而是从传入的可执行程序中加载代码和静态数据，并用它覆写自己的代码段、堆、栈，以及其他内存空间也会被重新初始化。然后操作系统就执行该程序。所以对execve的成功调用永远不会返回，只有在发生错误的时候才会返回-1，从原程序的调用点接着往下执行。 1234567891011121314151617181920arch/i386/kernel/process.c/* * sys_execve() executes a new program. */asmlinkage int sys_execve(struct pt_regs regs)&#123; int error; char * filename; filename = getname((char *) regs.ebx); error = PTR_ERR(filename); if (IS_ERR(filename)) goto out; error = do_execve(filename, (char **) regs.ecx, (char **) regs.edx, &amp;regs); if (error == 0) current-&gt;ptrace &amp;= ~PT_DTRACE; putname(filename);out: return error;&#125; regs.ebx存储的是需要调用的程序名称，也就是第一个参数，使用getname 将该字符串从用户空间拷贝到系统空间。接下来就调用do_execve完成其主体功能。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455 fs/exec.c/* * sys_execve() executes a new program. */int do_execve(char * filename, char ** argv, char ** envp, struct pt_regs * regs)&#123; struct linux_binprm bprm; struct file *file; int retval; int i; file = open_exec(filename); ...... bprm.file = file; bprm.filename = filename; bprm.sh_bang = 0; bprm.loader = 0; bprm.exec = 0; if ((bprm.argc = count(argv, bprm.p / sizeof(void *))) &lt; 0) &#123; allow_write_access(file); fput(file); return bprm.argc; &#125; if ((bprm.envc = count(envp, bprm.p / sizeof(void *))) &lt; 0) &#123; allow_write_access(file); fput(file); return bprm.envc; &#125; retval = prepare_binprm(&amp;bprm); if (retval &lt; 0) goto out; retval = copy_strings_kernel(1, &amp;bprm.filename, &amp;bprm); if (retval &lt; 0) goto out; bprm.exec = bprm.p; retval = copy_strings(bprm.envc, envp, &amp;bprm); if (retval &lt; 0) goto out; retval = copy_strings(bprm.argc, argv, &amp;bprm); if (retval &lt; 0) goto out; retval = search_binary_handler(&amp;bprm,regs); if (retval &gt;= 0) /* execve success */ return retval; ......&#125; 使用 open_exec找到并打开可执行程序的文件，接下来就需要装载可执行程序了，内核中为可执行程序的装入提供了一个数据结构linux_binprm，以便于存储和组织可执行程序中的信息。 12345678910111213141516 source/include/linux/binfmts.h#L22 /* * This structure is used to hold the arguments that are used when loading binaries. */struct linux_binprm&#123; char buf[BINPRM_BUF_SIZE]; struct page *page[MAX_ARG_PAGES]; unsigned long p; /* current top of mem */ int sh_bang; struct file * file; int e_uid, e_gid; kernel_cap_t cap_inheritable, cap_permitted, cap_effective; int argc, envc; char * filename; /* Name of binary */ unsigned long loader, exec;&#125;; 函数open_exec()返回一个file结构指针，代表着读入可执行文件的上下文，所以将其保存在数据结构 bprm中。最后的准备工作就是把执行的参数，也就是 argv[]，以及运行的环境，也就是envp[]，从用户空间拷贝到数据结构 bprm 中。其中的第1个参数 argv[0]就是可执行文件的路径名，已经在 bprm.filename中了，所以用copy_strings_kernel()从系统空间中拷贝，其它的就要用copy_strings()从用户空间拷贝。最后就到了关键的函数 search_binary_handler()，该函数就负责具体的装入和执行。 3.4 exit系统调用上面介绍了可以使用fork创建一个进程，然后使用execve将该进程替换为任意一个程序，显而易见还缺少一个销毁进程的系统调用，才能完整的运转整个流程。常用的C库函数void exit(status) 可以传入一个状态码status,0表示正常退出，其他表示非正常退出，一般都用-1或者1，标准C里有EXIT_SUCCESS和EXIT_FAILURE两个宏。 父进程可以使用wait系统调用接受子进程的返回值，从而针对不同的情况进行处理。 除了上面的exit函数，exit还有其他几种形态，_exit(0 )，SYS_exit。通过一个程序看一下他们的区别： 1234567891011121314151617181920#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;time.h&gt;#include &lt;sys/syscall.h&gt;void func() &#123; printf( "Goodbye,world!\n");&#125;int main(int argc, char *argv[])&#123; //当程序正常终止时，调用指定的函数 func。您可以在任何地方注册你的终止函数，但它会在程序终止的时候被调用。 atexit(func); //通过控制台传入不同的参数，调用不同的exit if (argc &lt; 2 ) return EXIT_FAILURE; if (strcmp( argv[1],"exit")==0) exit(0); if (strcmp( argv[1], "_exit") ==0) _exit(0) ; if (strcmp( argv[1], "__exit") ==0) syscall(SYS_exit,0);&#125; 结果如下 不传入参数调用exit(0)，会执行func输出字符串，其余两个都不会执行atexit注册的func，没有任何输出。因为前者是libc的库函数，后两者是系统调用，看不到libc的atexit函数。 使用strace分别跟踪后两个程序，发现_exit最终调用了exit_group(0)，会终止进程中的所有线程。 SYS_exit最终调用了exit(0)，只会终止当前的线程。 exit的几种形态 exit(0) - stdlib.h中声明的 libc函数，会调用atexit，上面介绍的函数。 _exit(0) - glibc的syscall wrapper 执行“exit_group”系统调用终止整个进程(所有线程) 不会调用atexit syscall(SYS_exit，0) 执行“exit”系统调用终止当前线程，不会调用atexit 4.处理器调度 通过前面已经了解到了，操作系统通过分时，实现了CPU的虚拟化。在中断发生以后，返回之前，操作系统一定要选择一个进程进行执行，那么这里就有了一个问题。 4.1中断以后选择哪个进程在处理器上执行？ 选择哪个进程在处理器上运行，就是调度的概念，调度的实质就是资源的分配。操作系统考虑了不同的算法来实现资源的分配，而且一个好的调度算法应该考虑以下几个方面： 公平：保证每个进程得到合理的CPU时间 高效：使CPU保持忙碌状态 响应时间：使交互响应时间尽可能短 周转时间：等待输出的时间尽可能短 吞吐量：单位时间内处理的进程数量尽可能多显然不可能有算法同时达到这五个目标，不同的操作系统会有不同的取舍。 常见的调度算法有： 时间片轮转调度算法 优先权调度算法 多级反馈队列调度算法 实时调度 完全公平调度算法 4.2Linux系统中进程的组织方式一个系统中通常有成百上千的进程，相应的就有这么多的PCB，为了有效的进行管理，就必须用适当的方式将他们组织起来。组织方式主要包括： 进程链表 哈希表 就绪队列 等待队列 4.3实际Linux内核的调度算法是如何取舍的？Linux 2.4中使用goodness()函数，给每个处于可运行状态的进程赋予一个权值（Weight）,使用这个权值衡量一个处于可运行状态的进程值得运行的程度。调度程序以这个权值作为选择进程的唯一依据。 虽然Linux2.4进程调度程序简单有效，但是也有其缺点。 单个就绪队列问题，时间片耗尽的进程依然会放进同一个就绪队列中，在不可被调度的情况下，还会参与调度。 多处理器问题，多个处理器上的进程放在同一个就绪队列中，因而调度器对它的所有操作都会因全局自旋锁而导致系统各个处理机之间的等待，使得就绪队列成为一个明显的瓶颈。 内核不可抢占问题，如果某个进程，一旦进了内核态那么再高优先级的进程都无法剥夺，只有等进程返回内核态的时候才可以进行调度。缺乏对实时进程的支持。 针对以上问题，Linux 2.6做了较大的改进。针对多处理器问题，为每个CPU设置一个就绪队列。针对单就绪队列问题，设置两个队列组，即active队列组和expired队列组。借助以上队列实现了时间复杂度为O(1)的调度算法。直到Linxu 2.6.23内核版本中，O(1)调度算法才真正替代为CFS（完全公平）调度算法。 4.3.1 就绪队列O(1)调度器是以进程的动态优先级prio为调度依据的,它总是选择目前就绪队列中优先级最高的进程作为候选进程 next。由于实时进程的优先级总是比普通进程的优先级高,故能保证实时进程总是比普通进程先被调度。 Linux2.6 中,优先级 prio 的计算不再集中在调度器选择 next 进程时,而是分散在进程状态改变的任何时候,这些时机有: 进程被创建时; 休眠进程被唤醒时; 从TASK_INTERRUPTIBLE 状态中被唤醒的进程被调度时; 因时间片耗尽或时间片过长而分段被剥夺 CPU 时; 在这些情况下,内核都会调用 effective_prio()重新计算进程的动态优先级 prio并根据计算结果调整它在就绪队列中的位置。 Linux 2.6为每个cpu定义了一个struck runqueue数据结构，每个就绪队列都有一个自旋锁，从而解决了 2.4 中因只有一个就绪队列而造成的瓶颈。 12345struct runqueue &#123; ... prio_array_t *active, *expired, array[2]; ... &#125; active 是指向活动进程队列的指针；expired 是指向过期进程队列的指针；array[2]是实际的优先级进程队列，其中一个是活跃的一个是过期的，过期数组存放时间片耗完的进程。 每个处理器的就绪队列都有两个优先级数组,它们是 prio_array 类型的结构体。Linux2.6内核正是因为使用了优先级数组,才实现了 O(1)调度算法。该结构定义在 kernel/sched.c 中: 123456789101112struct prio_array&#123; ... unsigned int nr_active; //相应 runqueue 中的进程数 unsigned long bitmap[BITMAP_SIZE]; /*索引位图，BITMAP_SIZE 默认值为 5,5个long(32位)类型，每位代表一个优先级，可以代表160个优先级，但实际中只有140。与下面的queue[]对应。分布0-99对应为实时进程，100-140对应为普通的进程*/ struct list_head queue[MAX_PRIO]; /*每个优先级的进程队列,MAX_PRIO 是系统允许的最大优先级数,默认值为 140,数值越小优先级越高 bitmap每一位都与 queue[i]相对应,当 queue[i]的进程队列不为空时,bitmap 相应位为 1,否则就为 0。*/&#125; 4.3.2 调度算法介绍选择并运行候选进程，确定next,下一个应该占有 CPU 并运行的进程，schedule()函数是完成进程调度的主要函数，并完成进程切换的工作。schedule()用于确定最高优先级进程的代码非常快捷高效，其性能的好坏对系统性能有着直接影响，它在/kernel/sched.c 中的定义如下: 123456789101112...int idx;...preempt_disable();...idx = sched_find_first_bit( array -&gt; bitmap);queue = array -&gt; queue + idx;next = list_entry( queue -&gt; next, task_t, run_list);...prev = context_switch( rq, prev, next);...&#125; 其中,sched_find_first_bit()能快速定位优先级最高的非空就绪进程链表,运行时间和就绪队列中的进程数无关,是实现O(1)调度算法的一个关键所在。schedule()的执行流程:首先,调用 pre_empt_disable(),关闭内核抢占,因为此时要对内核的一些重要数据结构进行操作,所以必须将内核抢占关闭;其次,调用sched_find_first_bit()找到位图中的第1个置1的位，该位正好对应于就绪队列中的最高优先级进程链表;再者,调用 context_switch()执行进程切换,选择在最高优先级链表中的第 1个进程投入运行; 图中的网格为 140 位优先级数组,queue[7]为优先级为 7 的就绪进程链表。此种算法保证了调度器运行的时间上限,加速了候选进程的定位过程。 Linux2.4 调度系统在所有就绪进程的时间片都耗完以后在调度器中一次性重新计算,其中重算是用for循环相当耗时。 Linux2.6 为每个 CPU 保留 active 和 expired 两个优先级数组, active 数组中包含了有剩余时间片的任务,expired 数组中包含了所有用完时间片的任务。 当一个任务的时间片用完了就会重新计算其时间片,并插入到 expired 队列中,当 active 队列中所有进程用完时间片时,只需交换指向 active 和 expired 队列的指针即可。此交换是实现 O(1)算法的核心,由 schedule()中以下程序来实现: 1234567array = rq -&gt;active;if (unlikely(!array-&gt;nr_active)) &#123; rq -&gt; active = rq -&gt; expired; rq -&gt; expired = array; array = rq -&gt;active;...&#125;]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>进程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gdb常用调试命令]]></title>
    <url>%2FC%E8%AF%AD%E8%A8%80%2Fgdb%E5%B8%B8%E7%94%A8%E8%B0%83%E8%AF%95%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[1. 前言GNU调试器（英语：GNU Debugger，缩写：GDB），是GNU软件系统中的标准调试器，此外GDB也是个具有移携性的调试器，经过移携需求的调修与重新编译，如今许多的类UNIX操作系统上都可以使用GDB，而现有GDB所能支持调试的编程语言有C、C++、Pascal以及FORTRAN。在Linux下开发C语言，经常用到gdb进行调试，下面总结一下gdb调试过程中常用的命令。 2. 应用场景一般来说，GDB主要帮忙你完成下面四个方面的功能： 1、启动你的程序，可以按照你的自定义的要求随心所欲的运行程序。 2、可让被调试的程序在你所指定的调置的断点处停住。（断点可以是条件表达式） 3、当程序被停住时，可以检查此时你的程序中所发生的事。 4、动态的改变你程序的执行环境。 3. 调试程序要使用gdb进行调试，必须在编译的时候加上-g参数，不然是无法使用gdb进行调试的。 $ cc -g lkm.c -o lkm 编译完成后，使用gdb 可执行文件，启动gdb调试。 szp@szp-pc:~/code/mynode$ gdb lkm GNU gdb (Ubuntu 9.2-0ubuntu1~20.04) 9.2 Copyright (C) 2020 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt; This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type &quot;show copying&quot; and &quot;show warranty&quot; for details. This GDB was configured as &quot;x86_64-linux-gnu&quot;. Type &quot;show configuration&quot; for configuration details. For bug reporting instructions, please see: &lt;http://www.gnu.org/software/gdb/bugs/&gt;. Find the GDB manual and other documentation resources online at: &lt;http://www.gnu.org/software/gdb/documentation/&gt;. For help, type &quot;help&quot;. Type &quot;apropos word&quot; to search for commands related to &quot;word&quot;... Reading symbols from lkm... (gdb) 使用l 行数可以列出源代码，然后直接按回车会重复执行l命令，会继续列出后面的代码。 (gdb) l 1 1 // 2 // Created by szp on 2020/10/10. 3 // 4 #include &lt;stdio.h&gt; 5 #include &lt;fcntl.h&gt; 6 #include &lt;unistd.h&gt; 7 #include &lt;string.h&gt; 8 void show_opt () { 9 printf(&quot;1. process name \n&quot; 10 &quot;2. group id \n&quot; (gdb) l 11 &quot;3. parent process \n&quot; 12 &quot;4. process group leader \n&quot; 13 &quot;5. child processes created \n&quot; 14 &quot;6. process memory segments \n&quot; 15 &quot;7 . virtual memory mapping \n&quot; 16 &quot;8. process priority \n&quot; 17 &quot;9. process state \n&quot; 18 &quot;10. cpu used by process \n&quot; 19 &quot;11. total fault count of process \n&quot; 20 &quot;12. process start time \n&quot; (gdb) 21 &quot;13. process link count \n&quot; 22 &quot;0. quit \n&quot;); 23 printf (&quot;\nEnter Option : &quot;); 24 } 25 26 27 int main () { 28 char buf [700],opt[50]; 29 int rt; 30 // open device node of LKM (gdb) 按照行数设置断点，通过break 行数 (gdb) break 9 Breakpoint 1 at 0x1271: file lkm.c, line 9. 按照函数名设置断点，break 函数名 (gdb) break show_opt Breakpoint 2 at 0x1269: file lkm.c, line 8. 查看断点信息info break (gdb) info break Num Type Disp Enb Address What 1 breakpoint keep y 0x0000000000001271 in show_opt at lkm.c:9 2 breakpoint keep y 0x0000000000001269 in show_opt at lkm.c:8 (gdb) 运行程序 r，run命令简写。在设置的断点处停下。 (gdb) r Starting program: /home/szp/code/mynode/lkm Enter Process PID:1 Breakpoint 2, show_opt () at lkm.c:8 8 void show_opt () { (gdb) 单条语句执行，next命令简写n。 (gdb) n Breakpoint 1, show_opt () at lkm.c:9 9 printf(&quot;1. process name \n&quot; 打印变量的值p 变量，print的缩写。 (gdb) p buf $1 = 0x0 GDB会根据变量的类型输出变量的值。但你也可以自定义GDB的输出的格式。x 按十六进制格式显示变量。d 按十进制格式显示变量。u 按十六进制格式显示无符号整型。o 按八进制格式显示变量。t 按二进制格式显示变量。a 按十六进制格式显示变量。c 按字符格式显示变量。f 按浮点数格式显示变量。 (gdb) p/c buf $3 = 0 &apos;\000&apos; (gdb) 产看函数堆栈 bt (gdb) bt #0 show_opt () at lkm.c:23 #1 0x0000555555555301 in main () at lkm.c:37 退出当前函数 finish (gdb) finish Run till exit from #0 show_opt () at lkm.c:23 main () at lkm.c:38 38 scanf(&quot;%s&quot;,opt); (gdb) 继续执行c，continue缩写。 (gdb) c Continuing. 要退出gdb时，只用发quit或命令简称q就行了。 gdb的命令很多，gdb把之分成许多个种类。help命令只是例出gdb的命令种类，如果要看种类中的命令，可以使用help 命令，如：help breakpoints，查看设置断点的所有命令。也可以直接help 来查看命令的帮助。(gdb) helpList of classes of commands: aliases – Aliases of other commands.breakpoints – Making program stop at certain points.data – Examining data.files – Specifying and examining files.internals – Maintenance commands.obscure – Obscure features.running – Running the program.stack – Examining the stack.status – Status inquiries.support – Support facilities.tracepoints – Tracing of program execution without stopping the program.user-defined – User-defined commands. Type “help” followed by a class name for a list of commands in that class.Type “help all” for the list of all commands.Type “help” followed by command name for full documentation.Type “apropos word” to search for commands related to “word”.Type “apropos -v word” for full documentation of commands related to “word”.Command name abbreviations are allowed if unambiguous.(gdb) gdb中，输入命令时，可以不用将命令敲完整，只用打命令的前几个字符就可以了，当然，命令的前几个字符应该要标志着一个唯一的命令，在Linux下，你可以敲击两次TAB键来补齐命令的全称，如果有重复的，那么gdb会把其例出来。 在gdb中，我们可以有以下几种暂停方式：断点（BreakPoint）、观察点（WatchPoint）、捕捉点（CatchPoint）、信号（Signals）、线程停止（Thread Stops）。如果要恢复程序运行，可以使用c或是continue命令。详细内容不介绍了。 info line命令查看源代码在内存中的地址。info line后面可以跟“行号”，“函数名”，“文件名:行号”，“文件名:函数名”，这个命令会打印出所指定的源码在运行时的内存地址，如： (gdb) info line show_opt Line 8 of &quot;lkm.c&quot; starts at address 0x555555555269 &lt;show_opt&gt; and ends at 0x555555555271 &lt;show_opt+8&gt;. disassemble可以查看源程序的当前执行时的汇编代码，这个命令会把目前内存中的指令dump出来。 (gdb) disassemble show_opt Dump of assembler code for function show_opt: =&gt; 0x0000555555555269 &lt;+0&gt;: endbr64 0x000055555555526d &lt;+4&gt;: push %rbp 0x000055555555526e &lt;+5&gt;: mov %rsp,%rbp 0x0000555555555271 &lt;+8&gt;: lea 0xd90(%rip),%rdi # 0x555555556008 0x0000555555555278 &lt;+15&gt;: callq 0x5555555550e0 &lt;puts@plt&gt; 0x000055555555527d &lt;+20&gt;: lea 0xebe(%rip),%rdi # 0x555555556142 0x0000555555555284 &lt;+27&gt;: mov $0x0,%eax 0x0000555555555289 &lt;+32&gt;: callq 0x555555555110 &lt;printf@plt&gt; 0x000055555555528e &lt;+37&gt;: nop 0x000055555555528f &lt;+38&gt;: pop %rbp 0x0000555555555290 &lt;+39&gt;: retq End of assembler dump. 查看寄存器的值info registers (gdb) info registers rax 0x0 0 rbx 0x555555555430 93824992236592 rcx 0x0 0 rdx 0x0 0 rsi 0xa 10 rdi 0x7fffffffd750 140737488344912 rbp 0x7fffffffdc80 0x7fffffffdc80 rsp 0x7fffffffdc80 0x7fffffffdc80 r8 0xa 10 r9 0x12 18 r10 0x555555556172 93824992239986 r11 0x246 582 r12 0x555555555180 93824992235904 r13 0x7fffffffe0a0 140737488347296 r14 0x0 0 r15 0x0 0 rip 0x555555555271 0x555555555271 &lt;show_opt+8&gt; eflags 0x206 [ PF IF ] cs 0x33 51 ss 0x2b 43 ds 0x0 0 es 0x0 0 fs 0x0 0 --Type &lt;RET&gt; for more, q to quit, c to continue without paging-- 修改变量的值修改被调试程序运行时的变量值，在GDB中很容易实现，使用GDB的print命令即可完成。如：(gdb) print x=4 4. 总结以上总结的都是gdb最基础的调试命令，满足简单使用，gdb还有很多高级命令，后续进行补充，此外，善用help命令可以发现更多gdb好用的命令。]]></content>
      <categories>
        <category>C语言</category>
      </categories>
      <tags>
        <tag>编程工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSAPP课后习题统计]]></title>
    <url>%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2Fcsapp-chapter1%2F</url>
    <content type="text"><![CDATA[习题1.1-1.21.1 假设你是个卡车司机，要将土豆从爱达荷州的Boise运送到明尼苏达州的Minneapolis，全程2500公里。在限速范围内，你估计平均速度为100公里/小时，整个行程需要25个小时。 A. 你听到新闻说蒙大拿州刚刚取消了限速，这使得行程中有1500公里卡车的速度可以为150公里/小时。那么这对整个行程的加速比是多少？ B. 你可以在www.fasttrucks.com网站上为自己的卡车买个新的涡轮增压器。网站现货供应各种型号，不过速度越快，价格越高。如果想要让整个行程的加速比为1.67x，那么你必须以多快的速度通过蒙大拿州？ 解：假设系统某个部分所需要执行时间与该时间的比例为 α ，而该部分性能提升比例为 k ，加速比为 由题意可知 α = 1500 / 2500 = 0.6 k = 150 / 100 = 1.5 所以 s = 1.25x 由题意可知 s = 1.67x， α = 0.6 所以 k = 3 因此 通过蒙大拿州的速度为300公里/小时。 1.2 公司的市场部向你的客户承诺，下一个版本的软件性能将改进2X。这项任务被分配给你。你已经确认只有80%的系统能够被改进，那么，这部分需要被改进多少（即k取何值）才能达到整体性能目标？ 解：有 s = 2x， α = 0.8 由 可得k = 2.67。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内存管理笔记]]></title>
    <url>%2FLinux%2F%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1. 概述本文对内存管理一章学习内容进行补充和记录，包括进程地址空间的分配、与撤销（mmap，munmap），动态链接与静态链接的区别，静态链接简单实验。 2. 虚拟内存、内核空间和用户空间32位平台上，线性空间的大小为4GB，Linux将4G的空间分为两部分。最高位的1GB（从虚地址0xC0000000到0xFFFFFFFF）供内核使用，称为“内核空间”。而较低的3GB（从虚地址0x00000000到0xBFFFFFFF），供进程使用，称为“用户空间”。因为内核空间由系统内的所有进程共享，所以每个进程可以拥有4GB的虚拟地址空间，其中0GB-3GB是进程私有空间，这个空间对其他进程不可见，最高的1GB内核空间为所有进程以及内核共享。 3. 进程的地址空间进程执行指令需要代码、数据、堆栈。 代码（main,%rip会从此处取出待执行的指令） 数据（static int x） 堆栈（int x） 可以用指针访问 动态链接库 运行时分配的内存 进程地址空间是一段一段连续的内存，每一段都有自己的职责，拥有相应的访问权限。 Linux提供mmap系统调用，可以为进程虚拟地址空间创建一个新的段，这个段可以是硬盘中某个文件的映射，也可以是匿名的数据，用来分配内存。munmap用于移除地址空间中的某一个段，mprotect用于修改某个段的权限。 1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;sys/mman.h&gt;void *mmap(void *start, size_t length, int prot, int flags, int fd, off_t offset);/*start：映射区的开始地址，设置为0时表示由系统决定映射区的起始地址。 length：映射区的长度。 prot：期望的内存保护标志，不能与文件的打开模式冲突。是以下的某个值，可以通过or运算（“|”）合理地组合在一起 PROT_EXEC //页内容可以被执行 PROT_READ //页内容可以被读取 PROT_WRITE //页可以被写入 PROT_NONE //页不可访问 flags：指定映射对象的类型，映射选项和映射页是否可以共享。它的值可以是一个或者多个以下位的组合体 MAP_FIXED //使用指定的映射起始地址，如果由start和len参数指定的内存区重叠于现存的映射空间，重叠部分将会被丢弃。如果指定的起始地址不可用，操作将会失败。 //并且起始地址必须落在页的边界上。 MAP_SHARED //与其它所有映射这个对象的进程共享映射空间。对共享区的写入，相当于输出到文件。直到msync()或者munmap()被调用，文件实际上不会被更新。 MAP_PRIVATE //建立一个写入时拷贝的私有映射。内存区域的写入不会影响到原文件。这个标志和以上标志是互斥的，只能使用其中一个。 MAP_DENYWRITE //这个标志被忽略。 MAP_EXECUTABLE //同上 MAP_NORESERVE //不要为这个映射保留交换空间。当交换空间被保留，对映射区修改的可能会得到保证。当交换空间不被保留，同时内存不足，对映射区的修改会引起段违例信号。 MAP_LOCKED //锁定映射区的页面，从而防止页面被交换出内存。 MAP_GROWSDOWN //用于堆栈，告诉内核VM系统，映射区可以向下扩展。 MAP_ANONYMOUS //匿名映射，映射区不与任何文件关联。 MAP_ANON //MAP_ANONYMOUS的别称，不再被使用。 MAP_FILE //兼容标志，被忽略。 MAP_32BIT //将映射区放在进程地址空间的低2GB，MAP_FIXED指定时会被忽略。当前这个标志只在x86-64平台上得到支持。 MAP_POPULATE //为文件映射通过预读的方式准备好页表。随后对映射区的访问不会被页违例阻塞。 MAP_NONBLOCK //仅和MAP_POPULATE一起使用时才有意义。不执行预读，只为已存在于内存中的页面建立页表入口。 fd：有效的文件描述词。一般是由open()函数返回，其值也可以设置为-1，此时需要指定flags参数中的MAP_ANON,表明进行的是匿名映射。 offset：被映射对象内容的起点 */int munmap(void *start, size_t length);int mprotect(const void *start, size_t len, int prot);/*把自start开始的、长度为len的内存区的保护属性修改为prot指定的值。prot可以取以下几个值，并且可以用“|”将几个属性合起来使用：1）PROT_READ：表示内存段内的内容可写；2）PROT_WRITE：表示内存段内的内容可读；3）PROT_EXEC：表示内存段中的内容可执行；4）PROT_NONE：表示内存段中的内容根本没法访问。*/ 4. 动态链接与静态链接下面编写一个简单的C程序，来看静态链接与动态链接的区别。 123456#include&lt;stdio.h&gt;int main()&#123; while(1); return 0;&#125; 首先使用静态链接编译a.c程序生成a.out文件，然后使用动态链接生成b.out。 szp@szp-pc:~$ gcc -static a.c szp@szp-pc:~$ gcc a.c -o b.out 可以看到静态链接的a.out的文件大小要远远大于动态链接的b.out。 -rwxr-xr-x 1 szp szp 845056 10月 24 16:56 a.out -rwxr-xr-x 1 szp szp 8160 10月 24 16:58 b.out 同时编译所用的时间，静态链接也会大于动态链接。szp@szp-pc:~$ time gcc a.c -o b.out real 0m0.063suser 0m0.011ssys 0m0.053sszp@szp-pc:~$ time gcc -static a.c real 0m0.091suser 0m0.071ssys 0m0.020s 让两个程序都run起来，我们查看他们的虚存空间有什么不同。先看静态链接程序的虚存空间。 szp@szp-pc:~$ cat /proc/4103/maps 00400000-004b6000 r-xp 00000000 08:01 395781 /home/szp/a.out（代码段） 006b6000-006bc000 rw-p 000b6000 08:01 395781 /home/szp/a.out（数据段） 006bc000-006bd000 rw-p 00000000 00:00 0 （.bss） 0153c000-0155f000 rw-p 00000000 00:00 0 [heap] 7ffe2595b000-7ffe2597c000 rw-p 00000000 00:00 0 [stack] 7ffe259f7000-7ffe259fa000 r--p 00000000 00:00 0 [vvar] 7ffe259fa000-7ffe259fb000 r-xp 00000000 00:00 0 [vdso] ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0 [vsyscall] 第一行是代码段，第二行是数据段，第三行应该是bss，第四行是堆，第五行是栈。 动态链接的虚存空间。 szp@szp-pc:~$ cat /proc/4114/maps 5556f7913000-5556f7914000 r-xp 00000000 08:01 395789 /home/szp/b.out（代码段） 5556f7b13000-5556f7b14000 r--p 00000000 08:01 395789 /home/szp/b.out 5556f7b14000-5556f7b15000 rw-p 00001000 08:01 395789 /home/szp/b.out（数据段） 7fd257004000-7fd2571eb000 r-xp 00000000 08:01 1080562 /lib/x86_64-linux-gnu/libc-2.27.so 7fd2571eb000-7fd2573eb000 ---p 001e7000 08:01 1080562 /lib/x86_64-linux-gnu/libc-2.27.so 7fd2573eb000-7fd2573ef000 r--p 001e7000 08:01 1080562 /lib/x86_64-linux-gnu/libc-2.27.so 7fd2573ef000-7fd2573f1000 rw-p 001eb000 08:01 1080562 /lib/x86_64-linux-gnu/libc-2.27.so 7fd2573f1000-7fd2573f5000 rw-p 00000000 00:00 0 7fd2573f5000-7fd25741e000 r-xp 00000000 08:01 1080558 /lib/x86_64-linux-gnu/ld-2.27.so 7fd257607000-7fd257609000 rw-p 00000000 00:00 0 7fd25761e000-7fd25761f000 r--p 00029000 08:01 1080558 /lib/x86_64-linux-gnu/ld-2.27.so 7fd25761f000-7fd257620000 rw-p 0002a000 08:01 1080558 /lib/x86_64-linux-gnu/ld-2.27.so 7fd257620000-7fd257621000 rw-p 00000000 00:00 0 7ffe5779d000-7ffe577be000 rw-p 00000000 00:00 0 [stack] 7ffe577f0000-7ffe577f3000 r--p 00000000 00:00 0 [vvar] 7ffe577f3000-7ffe577f4000 r-xp 00000000 00:00 0 [vdso] ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0 [vsyscall] 可以看出动态链接程序多出了许多libc.so和ld.so。 接下来使用gdb命令对动态链接程序进行调试。 szp@szp-pc:~$ cc -g a.c -o b.out szp@szp-pc:~$ gdb b.out 使用starti命令，在程序执行第一条指令的时候让程序停下来，并在此时查看其虚存空间。 (gdb) starti Starting program: /home/szp/b.out Program stopped. 0x00007ffff7dd4090 in _start () from /lib64/ld-linux-x86-64.so.2 (gdb) !cat /proc/4233/maps 555555554000-555555555000 r-xp 00000000 08:01 395789 /home/szp/b.out 555555754000-555555756000 rw-p 00000000 08:01 395789 /home/szp/b.out 7ffff7dd3000-7ffff7dfc000 r-xp 00000000 08:01 1080558 /lib/x86_64-linux-gnu/ld-2.27.so 7ffff7ff8000-7ffff7ffb000 r--p 00000000 00:00 0 [vvar] 7ffff7ffb000-7ffff7ffc000 r-xp 00000000 00:00 0 [vdso] 7ffff7ffc000-7ffff7ffe000 rw-p 00029000 08:01 1080558 /lib/x86_64-linux-gnu/ld-2.27.so 7ffff7ffe000-7ffff7fff000 rw-p 00000000 00:00 0 7ffffffde000-7ffffffff000 rw-p 00000000 00:00 0 [stack] ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0 [vsyscall] 可以看到和刚才相比虚存空间中少了libc.so。其实libc是在程序执行的时候，使用ld.so（加载器）动态链接进来的。我们在mian处打一个断点，继续查看虚存空间。 (gdb) break main Breakpoint 1 at 0x5555555545fe: file a.c, line 4. (gdb) n Single stepping until exit from function _start, which has no line number information. Breakpoint 1, main () at a.c:4 4 while(1); (gdb) !cat /proc/4233/maps 555555554000-555555555000 r-xp 00000000 08:01 395789 /home/szp/b.out 555555754000-555555755000 r--p 00000000 08:01 395789 /home/szp/b.out 555555755000-555555756000 rw-p 00001000 08:01 395789 /home/szp/b.out 7ffff79e2000-7ffff7bc9000 r-xp 00000000 08:01 1080562 /lib/x86_64-linux-gnu/libc-2.27.so 7ffff7bc9000-7ffff7dc9000 ---p 001e7000 08:01 1080562 /lib/x86_64-linux-gnu/libc-2.27.so 7ffff7dc9000-7ffff7dcd000 r--p 001e7000 08:01 1080562 /lib/x86_64-linux-gnu/libc-2.27.so 7ffff7dcd000-7ffff7dcf000 rw-p 001eb000 08:01 1080562 /lib/x86_64-linux-gnu/libc-2.27.so 7ffff7dcf000-7ffff7dd3000 rw-p 00000000 00:00 0 7ffff7dd3000-7ffff7dfc000 r-xp 00000000 08:01 1080558 /lib/x86_64-linux-gnu/ld-2.27.so 7ffff7fe1000-7ffff7fe3000 rw-p 00000000 00:00 0 7ffff7ff8000-7ffff7ffb000 r--p 00000000 00:00 0 [vvar] 7ffff7ffb000-7ffff7ffc000 r-xp 00000000 00:00 0 [vdso] 7ffff7ffc000-7ffff7ffd000 r--p 00029000 08:01 1080558 /lib/x86_64-linux-gnu/ld-2.27.so 7ffff7ffd000-7ffff7ffe000 rw-p 0002a000 08:01 1080558 /lib/x86_64-linux-gnu/ld-2.27.so 7ffff7ffe000-7ffff7fff000 rw-p 00000000 00:00 0 7ffffffde000-7ffffffff000 rw-p 00000000 00:00 0 [stack] ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0 [vsyscall] 从程序的第一条指令到执行main函数这段过程中，发现libc.so已经被成功链接进来了。也就是说，动态链接程序是在程序运行的时候，将所需的库文件加载进虚存空间，所以编译后的程序比静态链接要小的多，而静态链接是在编译的时候就将所需的库文件打包到了一块，所以文件体积较大。 5. vdso(virtual dynamic shared object)刚才查看了许多情况的虚存空间，其中有三个段vdso，vvar，vsyscall，存在于每个进程的虚存空间中，并且地址非常高。由于系统调用陷入内核的代价非常大，操作系统提供了一种针对可读系统调用，无需陷入内核的功能。这段代码就在vdso段中，它是可读可执行的。vvar：内核和进程共享的数据。vdso：系统调用代码的实现。 可以看到操作系统实现了四个函数，可以不陷入内核执行系统调用。time函数会打印出从1970.1.1到今天所经过的秒数。下面调试一下time函数。 x86-64 functions The table below lists the symbols exported by the vDSO. All of these symbols are also available without the &quot;__vdso_&quot; prefix, but you should ignore those and stick to the names below. symbol version ───────────────────────────────── __vdso_clock_gettime LINUX_2.6 __vdso_getcpu LINUX_2.6 __vdso_gettimeofday LINUX_2.6 __vdso_time LINUX_2.6 程序如下： 12345678#include&lt;stdio.h&gt;int main()&#123; printf("%d\n",time(0)); return 0;&#125; 在main处打断点，运行程序，然后进入汇编模式。 (gdb) b main Breakpoint 1 at 0x68e: file a.c, line 4. (gdb) r Starting program: /home/szp/a.out Breakpoint 1, main () at a.c:4 4 printf(&quot;%d\n&quot;,time(0)); (gdb) layout asm ┌────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ B+&gt;│0x55555555468e &lt;main+4&gt; mov $0x0,%edi │ │0x555555554693 &lt;main+9&gt; mov $0x0,%eax │ │0x555555554698 &lt;main+14&gt; callq 0x555555554560 &lt;time@plt&gt; │ │0x55555555469d &lt;main+19&gt; mov %eax,%esi │ │0x55555555469f &lt;main+21&gt; lea 0x9e(%rip),%rdi # 0x555555554744 │ │0x5555555546a6 &lt;main+28&gt; mov $0x0,%eax │ │0x5555555546ab &lt;main+33&gt; callq 0x555555554550 &lt;printf@plt&gt; │ │0x5555555546b0 &lt;main+38&gt; mov $0x0,%eax │ │0x5555555546b5 &lt;main+43&gt; pop %rbp │ │0x5555555546b6 &lt;main+44&gt; retq │ │0x5555555546b7 nopw 0x0(%rax,%rax,1) │ │0x5555555546c0 &lt;__libc_csu_init&gt; push %r15 │ │0x5555555546c2 &lt;__libc_csu_init+2&gt; push %r14 │ │0x5555555546c4 &lt;__libc_csu_init+4&gt; mov %rdx,%r15 │ │0x5555555546c7 &lt;__libc_csu_init+7&gt; push %r13 │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ native process 4399 In: main L4 PC: 0x55555555468e (gdb) si 输入si单步执行。time调用了time@plt函数。 ┌────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ &gt;│0x7ffff7ffb931 &lt;time+1&gt; test %rdi,%rdi │ │0x7ffff7ffb934 &lt;time+4&gt; mov -0x389b(%rip),%rax # 0x7ffff7ff80a0 │ │0x7ffff7ffb93b &lt;time+11&gt; mov %rsp,%rbp │ │0x7ffff7ffb93e &lt;time+14&gt; je 0x7ffff7ffb943 &lt;time+19&gt; │ │0x7ffff7ffb940 &lt;time+16&gt; mov %rax,(%rdi) │ │0x7ffff7ffb943 &lt;time+19&gt; pop %rbp │ │0x7ffff7ffb944 &lt;time+20&gt; retq │ │0x7ffff7ffb945 nop │ │0x7ffff7ffb946 nopw %cs:0x0(%rax,%rax,1) │ │0x7ffff7ffb950 &lt;clock_gettime&gt; push %rbp │ │0x7ffff7ffb951 &lt;clock_gettime+1&gt; cmp $0xf,%edi │ │0x7ffff7ffb954 &lt;clock_gettime+4&gt; mov %rsp,%rbp │ │0x7ffff7ffb957 &lt;clock_gettime+7&gt; push %r12 │ │0x7ffff7ffb959 &lt;clock_gettime+9&gt; mov %rsi,%r12 │ │0x7ffff7ffb95c &lt;clock_gettime+12&gt; push %rbx │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ native process 4399 In: time L?? PC: 0x7ffff7ffb931 0x0000555555554560 in time@plt () (gdb) info inferiors Num Description Executable * 1 process 4399 /home/szp/a.out (gdb) si 0x00007ffff7ffb930 in time () 程序就跳转到了vdso段内地址，0x7ffff7ffb931是位于7ffff7ffb000-7ffff7ffc000内的。 szp@szp-pc:~$ cat /proc/4399/maps 555555554000-555555555000 r-xp 00000000 08:01 395781 /home/szp/a.out 555555754000-555555755000 r--p 00000000 08:01 395781 /home/szp/a.out 555555755000-555555756000 rw-p 00001000 08:01 395781 /home/szp/a.out 7ffff79e2000-7ffff7bc9000 r-xp 00000000 08:01 1080562 /lib/x86_64-linux-gnu/libc-2.27.so 7ffff7bc9000-7ffff7dc9000 ---p 001e7000 08:01 1080562 /lib/x86_64-linux-gnu/libc-2.27.so 7ffff7dc9000-7ffff7dcd000 r--p 001e7000 08:01 1080562 /lib/x86_64-linux-gnu/libc-2.27.so 7ffff7dcd000-7ffff7dcf000 rw-p 001eb000 08:01 1080562 /lib/x86_64-linux-gnu/libc-2.27.so 7ffff7dcf000-7ffff7dd3000 rw-p 00000000 00:00 0 7ffff7dd3000-7ffff7dfc000 r-xp 00000000 08:01 1080558 /lib/x86_64-linux-gnu/ld-2.27.so 7ffff7fe1000-7ffff7fe3000 rw-p 00000000 00:00 0 7ffff7ff8000-7ffff7ffb000 r--p 00000000 00:00 0 [vvar] 7ffff7ffb000-7ffff7ffc000 r-xp 00000000 00:00 0 [vdso] 7ffff7ffc000-7ffff7ffd000 r--p 00029000 08:01 1080558 /lib/x86_64-linux-gnu/ld-2.27.so 7ffff7ffd000-7ffff7ffe000 rw-p 0002a000 08:01 1080558 /lib/x86_64-linux-gnu/ld-2.27.so 7ffff7ffe000-7ffff7fff000 rw-p 00000000 00:00 0 7ffffffde000-7ffffffff000 rw-p 00000000 00:00 0 [stack] ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0 [vsyscall] 看接下来这条汇编指令，将%rip（当前指令）寄存器减去一个值，得到的内存地址赋值给%rax（函数返回值）寄存器，后面给出了注释，%rax的地址# 0x7ffff7ff80a0，而这个地址正是位于vvar段中。所以系统将时间从内存中某个位置拷贝到了vvar段中。操作系统通过这种共享内存的方式，为所有的进程提供了获取当前系统时间的系统调用。当然这个段只允许进程读，而不允许进程写，会触发段错误。 10x7ffff7ffb934 &lt;time+4&gt; mov -0x389b(%rip),%rax # 0x7ffff7ff80a0 还有最后一个vsyscall段，vsyscall中的指令只是简单调用了syscall系统调用，因为它是废弃的不陷入内核的系统调用方法，已经不再使用，为了向下兼容，保留了下来，并且让它直接调用syscall。 5. 静态链接实验上面简单介绍了静态链接与动态链接，接下来通过一些实验来直观的看一下静态链接是如何实现的。 有如下两个程序a.c，b.c，a程序中调用了b程序中函数，通过这两个程序观察是a如何链接b的。 1234567891011121314//a.c#include&lt;stdio.h&gt;int fun(int x);int main()&#123; printf("%d\n",fun(0)); return 0;&#125;//b.c#include&lt;stdio.h&gt;int fun(int x)&#123; return x+1;&#125; 使用如下命令对程序a.c进行编译。 szp@szp-pc:~$ gcc -o a.o -g -c -static a.c 查看其对应的汇编代码。 szp@szp-pc:~$ objdump -S -d a.o a.o： 文件格式 elf64-x86-64 Disassembly of section .text: 0000000000000000 &lt;main&gt;: #include&lt;stdio.h&gt; int fun(int x); int main(){ 0: 55 push %rbp 1: 48 89 e5 mov %rsp,%rbp printf(&quot;%d\n&quot;,fun(0)); 4: bf 00 00 00 00 mov $0x0,%edi 9: e8 00 00 00 00 callq e &lt;main+0xe&gt; e: 89 c6 mov %eax,%esi 10: 48 8d 3d 00 00 00 00 lea 0x0(%rip),%rdi # 17 &lt;main+0x17&gt; 17: b8 00 00 00 00 mov $0x0,%eax 1c: e8 00 00 00 00 callq 21 &lt;main+0x21&gt; return 0; 21: b8 00 00 00 00 mov $0x0,%eax } 26: 5d pop %rbp 27: c3 retq 124: bf 00 00 00 00 mov $0x0,%edi9: e8 00 00 00 00 callq e &lt;main+0xe&gt; x86使用edi寄存器保存第一个参数的值，所以0x4处的指令后面的00 00 00 00应该是存放的变量x的值，它默认是初始化为0。而0x9处的指令应该是调用fun函数，后面的00 00 00 00 为fun函数的地址。因为该程序引用了一个外部的函数fun，当前并不知道fun函数会在哪里，所以编译器会预留位置，然后链接的时候对这些位置进行重填。那么链接器如何知道重填的位置呢？答案是存储在了elf文件中，链接器就是解析elf文件对这些位置进行重填。使用readelf，可以看到在elf文件中存储的应该重填的位置。 szp@szp-pc:~$ readelf -r a.o 重定位节 &apos;.rela.text&apos; at offset 0xa48 contains 3 entries: 偏移量 信息 类型 符号值 符号名称 + 加数 00000000000a 001000000004 R_X86_64_PLT32 0000000000000000 fun - 4 000000000013 000500000002 R_X86_64_PC32 0000000000000000 .rodata - 4 00000000001d 001100000004 R_X86_64_PLT32 0000000000000000 printf - 4 可以看到fun函数重填的位置在0x00000000000a，也就是上面的0x9指令行的第二个位置。 总结ELF文件中会有一个ELF header和若干个Program Header，每个Program Header都描述了需要将内存中的某一段映射成程序中的某一段。链接器就会负责解析ELF文件完成映射和地址的重定向。静态链接实验展示了这个过程。vdso机制提供了非陷入内核的系统调用，对于只读系统调用，减小了切换的开销。实现系统调用的关键，在于让内核知道某个进程想要进行系统调用，并且让程序能够知道哪里可以获取到结果。利用这种共享内存的方式，或许还可以实现更多的内核功能。 参考链接：https://www.bilibili.com/video/BV1N741177F5?p=15]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>内存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu18搭建ebpf运行环境]]></title>
    <url>%2FLinux%2FUbuntu18%E6%90%AD%E5%BB%BAebpf%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[前言eBPF之于Linux的作用与JavaScript对HTML的作用相同。 因此，JavaScript改变了静态HTML网站，使我们可以定义一些网页上的触发事件，例如用户的点击、鼠标滑动，这些js程序在浏览器的安全引擎中运行。 使用eBPF，而不是固定的内核，现在可以编写在例如磁盘I/O等事件上运行的触发程序，这些程序在内核中的安全引擎中运行。 实际上，eBPF更像是运行JavaScript的v8引擎，而不是JavaScript程序本身。 eBPF是Linux内核的一部分。直接在eBPF中进行编程非常困难，与在v8字节码中进行编码相同。它们使用JavaScript编写代码，或者通常是JavaScript之上的框架（jQuery，Angular，React等）。 eBPF也是如此，人们将使用它并通过框架对其进行封装，主要的是框架是bcc和bpftrace。 它们不存在于内核代码库中，而是存在于github上名为iovisor的Linux Foundation项目中。 本文不是介绍如何搭建bcc或者bpftrace的开发环境，而是介绍不借助框架的情况下，ebpf的开发环境如何进行搭建。使用的操作系统是Ubuntu 18，Linux内核版本为5.4.0.48。 下载Linux内核源代码在下载代码之前，建议先更新Ubuntu中的软件下载源，更新为国内源，下载速度会更加感人。编辑下载源文件 sudo vim /etc/apt/sources.list 将阿里源放到文件中保存。 12345678910111213141516171819deb http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse 保存好以后，使用下面的命令，进行软件源更新。 123sudo apt-get updatesudo apt-get upgrade 接下来就查看缓存仓库中可以下载的源码 sudo apt search linux-source 使用命令 sudo apt install linux-source 下载源列表中最新的源码压缩包。也可以指定版本进行下载。 sudo apt install linux-source[-version] 建议下载和自身内核版本一致的源代码。 进入/usr/src 查看刚才下载的源码包。 将该压缩包解压。 sudo tar -xjvf linux-source-5.4.0.tar.bz2 编译ebpf样例程序安装依赖 1234567sudo apt install libncurses5-dev flex bison libelf-dev binutils-dev libssl-dev sudo cd linux-source-5.4.0 进入内核源码目录sudo apt install clang llvm 安装clang和llvm编译器sudo make headers_instal 与本系统内核头文件进行关联 sudo make menuconfig 配置内核选项生成.config文件 可视化配置界面，直接选择save。 生成.config文件，选择ok后，然后选择exit。 编译sampls/bpf目录下的样例文件。 sudo make M=samples/bpf 成功后就会生成.o文件。 根据样例程序自定义ebpf程序ebpf程序包含两个文件，一个运行在内核态，通常用*_kern.c命名，一个运行在用户态，通常用 *_user.c命名。 为运行在内核空间的示例源代码（kern.c），编译生成.o后缀的目标文件，以便加载到对应BPF提供的钩子中去 为运行在用户空间的示例源代码（user.c），编译生成可以在本机直接运行的可执行文件，以便用户可以直接运行测试 编写ebpf程序放到samples/bpf目录下。内核态程序： 12345678910111213//mybpf_01_kern.c#include &lt;linux/bpf.h&gt;#include "bpf_helpers.h"#define SEC(NAME) __attribute__((section(NAME), used))SEC("tracepoint/syscalls/sys_enter_execve")int bpf_prog(void *ctx)&#123; char msg[] = "Hello world ebpf!\n"; bpf_trace_printk(msg, sizeof(msg)); return 0;&#125;char _license[] SEC("license") = "GPL"; 用户态程序： 12345678910111213//mybpf_01_user.c #include &lt;stdio.h&gt;#include "bpf_load.h"int main(int argc, char **argv)&#123; if(load_bpf_file("mybpf_01_kern.o")!=0)&#123; printf("The kernel didn't load BPF program\n"); return -1; &#125; read_trace_pipe(); return 0;&#125; 修改bpf目录下的Makefile文件。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308# SPDX-License-Identifier: GPL-2.0BPF_SAMPLES_PATH ?= $(abspath $(srctree)/$(src))TOOLS_PATH := $(BPF_SAMPLES_PATH)/../../tools# List of programs to buildhostprogs-y := test_lru_disthostprogs-y += sock_examplehostprogs-y += fds_examplehostprogs-y += sockex1hostprogs-y += sockex2hostprogs-y += sockex3hostprogs-y += tracex1hostprogs-y += tracex2hostprogs-y += tracex3hostprogs-y += tracex4hostprogs-y += tracex5hostprogs-y += tracex6hostprogs-y += tracex7hostprogs-y += test_probe_write_userhostprogs-y += trace_outputhostprogs-y += lathisthostprogs-y += offwaketimehostprogs-y += spintesthostprogs-y += map_perf_testhostprogs-y += test_overheadhostprogs-y += test_cgrp2_array_pinhostprogs-y += test_cgrp2_attachhostprogs-y += test_cgrp2_sockhostprogs-y += test_cgrp2_sock2hostprogs-y += xdp1hostprogs-y += xdp2hostprogs-y += xdp_router_ipv4hostprogs-y += test_current_task_under_cgrouphostprogs-y += trace_eventhostprogs-y += sampleiphostprogs-y += tc_l2_redirecthostprogs-y += lwt_len_histhostprogs-y += xdp_tx_iptunnelhostprogs-y += test_map_in_maphostprogs-y += per_socket_stats_examplehostprogs-y += xdp_redirecthostprogs-y += xdp_redirect_maphostprogs-y += xdp_redirect_cpuhostprogs-y += xdp_monitorhostprogs-y += xdp_rxq_infohostprogs-y += syscall_tphostprogs-y += cpustathostprogs-y += xdp_adjust_tailhostprogs-y += xdpsockhostprogs-y += xdp_fwdhostprogs-y += task_fd_queryhostprogs-y += xdp_sample_pktshostprogs-y += ibumadhostprogs-y += hbm#自定义hostprogs-y += mybpf_01_kern# Libbpf dependenciesLIBBPF = $(TOOLS_PATH)/lib/bpf/libbpf.aCGROUP_HELPERS := ../../tools/testing/selftests/bpf/cgroup_helpers.oTRACE_HELPERS := ../../tools/testing/selftests/bpf/trace_helpers.ofds_example-objs := fds_example.osockex1-objs := sockex1_user.osockex2-objs := sockex2_user.osockex3-objs := bpf_load.o sockex3_user.otracex1-objs := bpf_load.o tracex1_user.otracex2-objs := bpf_load.o tracex2_user.otracex3-objs := bpf_load.o tracex3_user.otracex4-objs := bpf_load.o tracex4_user.otracex5-objs := bpf_load.o tracex5_user.otracex6-objs := bpf_load.o tracex6_user.otracex7-objs := bpf_load.o tracex7_user.o test_probe_write_user-objs := bpf_load.o test_probe_write_user_user.otrace_output-objs := bpf_load.o trace_output_user.o $(TRACE_HELPERS)lathist-objs := bpf_load.o lathist_user.ooffwaketime-objs := bpf_load.o offwaketime_user.o $(TRACE_HELPERS)spintest-objs := bpf_load.o spintest_user.o $(TRACE_HELPERS)map_perf_test-objs := bpf_load.o map_perf_test_user.otest_overhead-objs := bpf_load.o test_overhead_user.otest_cgrp2_array_pin-objs := test_cgrp2_array_pin.otest_cgrp2_attach-objs := test_cgrp2_attach.otest_cgrp2_sock-objs := test_cgrp2_sock.otest_cgrp2_sock2-objs := bpf_load.o test_cgrp2_sock2.oxdp1-objs := xdp1_user.o# reuse xdp1 source intentionallyxdp2-objs := xdp1_user.oxdp_router_ipv4-objs := xdp_router_ipv4_user.otest_current_task_under_cgroup-objs := bpf_load.o $(CGROUP_HELPERS) \ test_current_task_under_cgroup_user.otrace_event-objs := bpf_load.o trace_event_user.o $(TRACE_HELPERS)sampleip-objs := bpf_load.o sampleip_user.o $(TRACE_HELPERS)tc_l2_redirect-objs := bpf_load.o tc_l2_redirect_user.olwt_len_hist-objs := bpf_load.o lwt_len_hist_user.oxdp_tx_iptunnel-objs := xdp_tx_iptunnel_user.otest_map_in_map-objs := bpf_load.o test_map_in_map_user.oper_socket_stats_example-objs := cookie_uid_helper_example.oxdp_redirect-objs := xdp_redirect_user.oxdp_redirect_map-objs := xdp_redirect_map_user.oxdp_redirect_cpu-objs := bpf_load.o xdp_redirect_cpu_user.oxdp_monitor-objs := bpf_load.o xdp_monitor_user.oxdp_rxq_info-objs := xdp_rxq_info_user.osyscall_tp-objs := bpf_load.o syscall_tp_user.ocpustat-objs := bpf_load.o cpustat_user.oxdp_adjust_tail-objs := xdp_adjust_tail_user.oxdpsock-objs := xdpsock_user.oxdp_fwd-objs := xdp_fwd_user.otask_fd_query-objs := bpf_load.o task_fd_query_user.o $(TRACE_HELPERS)xdp_sample_pkts-objs := xdp_sample_pkts_user.o $(TRACE_HELPERS)ibumad-objs := bpf_load.o ibumad_user.o $(TRACE_HELPERS)hbm-objs := bpf_load.o hbm.o $(CGROUP_HELPERS)#自定义mybpf_01-objs := bpf_load.o mybpf_01_user.o # Tell kbuild to always build the programsalways := $(hostprogs-y)always += sockex1_kern.oalways += sockex2_kern.oalways += sockex3_kern.oalways += tracex1_kern.oalways += tracex2_kern.oalways += tracex3_kern.oalways += tracex4_kern.oalways += tracex5_kern.oalways += tracex6_kern.oalways += tracex7_kern.oalways += sock_flags_kern.oalways += test_probe_write_user_kern.oalways += trace_output_kern.oalways += tcbpf1_kern.oalways += tc_l2_redirect_kern.oalways += lathist_kern.oalways += offwaketime_kern.oalways += spintest_kern.oalways += map_perf_test_kern.oalways += test_overhead_tp_kern.oalways += test_overhead_raw_tp_kern.oalways += test_overhead_kprobe_kern.oalways += parse_varlen.o parse_simple.o parse_ldabs.oalways += test_cgrp2_tc_kern.oalways += xdp1_kern.oalways += xdp2_kern.oalways += xdp_router_ipv4_kern.oalways += test_current_task_under_cgroup_kern.oalways += trace_event_kern.oalways += sampleip_kern.oalways += lwt_len_hist_kern.oalways += xdp_tx_iptunnel_kern.oalways += test_map_in_map_kern.oalways += cookie_uid_helper_example.oalways += tcp_synrto_kern.oalways += tcp_rwnd_kern.oalways += tcp_bufs_kern.oalways += tcp_cong_kern.oalways += tcp_iw_kern.oalways += tcp_clamp_kern.oalways += tcp_basertt_kern.oalways += tcp_tos_reflect_kern.oalways += tcp_dumpstats_kern.oalways += xdp_redirect_kern.oalways += xdp_redirect_map_kern.oalways += xdp_redirect_cpu_kern.oalways += xdp_monitor_kern.oalways += xdp_rxq_info_kern.oalways += xdp2skb_meta_kern.oalways += syscall_tp_kern.oalways += cpustat_kern.oalways += xdp_adjust_tail_kern.oalways += xdp_fwd_kern.oalways += task_fd_query_kern.oalways += xdp_sample_pkts_kern.oalways += ibumad_kern.oalways += hbm_out_kern.oalways += hbm_edt_kern.o#自定义always += mybpf_01_kern.oKBUILD_HOSTCFLAGS += -I$(objtree)/usr/includeKBUILD_HOSTCFLAGS += -I$(srctree)/tools/lib/bpf/KBUILD_HOSTCFLAGS += -I$(srctree)/tools/testing/selftests/bpf/KBUILD_HOSTCFLAGS += -I$(srctree)/tools/lib/ -I$(srctree)/tools/includeKBUILD_HOSTCFLAGS += -I$(srctree)/tools/perfKBUILD_HOSTCFLAGS += -DHAVE_ATTR_TEST=0HOSTCFLAGS_bpf_load.o += -I$(objtree)/usr/include -Wno-unused-variableKBUILD_HOSTLDLIBS += $(LIBBPF) -lelfHOSTLDLIBS_tracex4 += -lrtHOSTLDLIBS_trace_output += -lrtHOSTLDLIBS_map_perf_test += -lrtHOSTLDLIBS_test_overhead += -lrtHOSTLDLIBS_xdpsock += -pthread# Allows pointing LLC/CLANG to a LLVM backend with bpf support, redefine on cmdline:# make samples/bpf/ LLC=~/git/llvm/build/bin/llc CLANG=~/git/llvm/build/bin/clangLLC ?= llcCLANG ?= clangLLVM_OBJCOPY ?= llvm-objcopyBTF_PAHOLE ?= pahole# Detect that we're cross compiling and use the cross compilerifdef CROSS_COMPILEHOSTCC = $(CROSS_COMPILE)gccCLANG_ARCH_ARGS = -target $(ARCH)endif# Don't evaluate probes and warnings if we need to run make recursivelyifneq ($(src),)HDR_PROBE := $(shell echo "\#include &lt;linux/types.h&gt;\n struct list_head &#123; int a; &#125;; int main() &#123; return 0; &#125;" | \ $(HOSTCC) $(KBUILD_HOSTCFLAGS) -x c - -o /dev/null 2&gt;/dev/null &amp;&amp; \ echo okay)ifeq ($(HDR_PROBE),)$(warning WARNING: Detected possible issues with include path.)$(warning WARNING: Please install kernel headers locally (make headers_install).)endifBTF_LLC_PROBE := $(shell $(LLC) -march=bpf -mattr=help 2&gt;&amp;1 | grep dwarfris)BTF_PAHOLE_PROBE := $(shell $(BTF_PAHOLE) --help 2&gt;&amp;1 | grep BTF)BTF_OBJCOPY_PROBE := $(shell $(LLVM_OBJCOPY) --help 2&gt;&amp;1 | grep -i 'usage.*llvm')BTF_LLVM_PROBE := $(shell echo "int main() &#123; return 0; &#125;" | \ $(CLANG) -target bpf -O2 -g -c -x c - -o ./llvm_btf_verify.o; \ readelf -S ./llvm_btf_verify.o | grep BTF; \ /bin/rm -f ./llvm_btf_verify.o)BPF_EXTRA_CFLAGS += -fno-stack-protectorifneq ($(BTF_LLVM_PROBE),) EXTRA_CFLAGS += -gelseifneq ($(and $(BTF_LLC_PROBE),$(BTF_PAHOLE_PROBE),$(BTF_OBJCOPY_PROBE)),) EXTRA_CFLAGS += -g LLC_FLAGS += -mattr=dwarfris DWARF2BTF = yendifendifendif# Trick to allow make to be run from this directoryall: $(MAKE) -C ../../ $(CURDIR)/ BPF_SAMPLES_PATH=$(CURDIR)clean: $(MAKE) -C ../../ M=$(CURDIR) clean @find $(CURDIR) -type f -name '*~' -delete$(LIBBPF): FORCE# Fix up variables inherited from Kbuild that tools/ build system won't like $(MAKE) -C $(dir $@) RM='rm -rf' LDFLAGS= srctree=$(BPF_SAMPLES_PATH)/../../ O=$(obj)/syscall_nrs.h: $(obj)/syscall_nrs.s FORCE $(call filechk,offsets,__SYSCALL_NRS_H__)targets += syscall_nrs.sclean-files += syscall_nrs.hFORCE:# Verify LLVM compiler tools are available and bpf target is supported by llc.PHONY: verify_cmds verify_target_bpf $(CLANG) $(LLC)verify_cmds: $(CLANG) $(LLC) @for TOOL in $^ ; do \ if ! (which -- "$$&#123;TOOL&#125;" &gt; /dev/null 2&gt;&amp;1); then \ echo "*** ERROR: Cannot find LLVM tool $$&#123;TOOL&#125;" ;\ exit 1; \ else true; fi; \ doneverify_target_bpf: verify_cmds @if ! ($&#123;LLC&#125; -march=bpf -mattr=help &gt; /dev/null 2&gt;&amp;1); then \ echo "*** ERROR: LLVM ($&#123;LLC&#125;) does not support 'bpf' target" ;\ echo " NOTICE: LLVM version &gt;= 3.7.1 required" ;\ exit 2; \ else true; fi$(BPF_SAMPLES_PATH)/*.c: verify_target_bpf $(LIBBPF)$(src)/*.c: verify_target_bpf $(LIBBPF)$(obj)/tracex5_kern.o: $(obj)/syscall_nrs.h$(obj)/hbm_out_kern.o: $(src)/hbm.h $(src)/hbm_kern.h$(obj)/hbm.o: $(src)/hbm.h$(obj)/hbm_edt_kern.o: $(src)/hbm.h $(src)/hbm_kern.h# asm/sysreg.h - inline assembly used by it is incompatible with llvm.# But, there is no easy way to fix it, so just exclude it since it is# useless for BPF samples.$(obj)/%.o: $(src)/%.c @echo " CLANG-bpf " $@ $(Q)$(CLANG) $(NOSTDINC_FLAGS) $(LINUXINCLUDE) $(EXTRA_CFLAGS) -I$(obj) \ -I$(srctree)/tools/testing/selftests/bpf/ \ -D__KERNEL__ -D__BPF_TRACING__ -Wno-unused-value -Wno-pointer-sign \ -D__TARGET_ARCH_$(SRCARCH) -Wno-compare-distinct-pointer-types \ -Wno-gnu-variable-sized-type-not-at-end \ -Wno-address-of-packed-member -Wno-tautological-compare \ -Wno-unknown-warning-option $(CLANG_ARCH_ARGS) \ -I$(srctree)/samples/bpf/ -include asm_goto_workaround.h \ -O2 -emit-llvm -c $&lt; -o -| $(LLC) -march=bpf $(LLC_FLAGS) -filetype=obj -o $@ifeq ($(DWARF2BTF),y) $(BTF_PAHOLE) -J $@endif Makefile文件只需修改三处，在上面文件中用“自定义”标识。需要注意的是 hostprogs-y += mybpf_01_kern，等号后面的文件名称，除去.c后缀后，务必写完整。否则会有一个gcc错误“no input file”。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux 2.6进程O(1)调度算法]]></title>
    <url>%2Funcategorized%2FLinux%E8%BF%9B%E7%A8%8B%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1. 前言Linux 2.4中使用goodness()函数，给每个处于可运行状态的进程赋予一个权值（Weight）,使用这个权值衡量一个处于可运行状态的进程值得运行的程度。调度程序以这个权值作为选择进程的唯一依据。 虽然Linux2.4进程调度程序简单有效，但是也有其缺点。 单个就绪队列问题，时间片耗尽的进程依然会放进同一个就绪队列中，在不可被调度的情况下，还会参与调度。 多处理器问题，多个处理器上的进程放在同一个就绪队列中，因而调度器对它的所有操作都会因全局自旋锁而导致系统各个处理机之间的等待，使得就绪队列成为一个明显的瓶颈。 内核不可抢占问题，如果某个进程，一旦进了内核态那么再高优先级的进程都无法剥夺，只有等进程返回内核态的时候才可以进行调度。缺乏对实时进程的支持。 针对以上问题，Linux 2.6做了较大的改进。针对多处理器问题，为每个CPU设置一个就绪队列。针对单就绪队列问题，设置两个队列组，即active队列组和expired队列组。借助以上队列实现了时间复杂度为O(1)的调度算法。直到Linxu 2.6.23内核版本中，O(1)调度算法才真正替代为CFS（完全公平）调度算法。 2. 就绪队列O(1)调度器是以进程的动态优先级prio为调度依据的,它总是选择目前就绪队列中优先级最高的进程作为候选进程 next。由于实时进程的优先级总是比普通进程的优先级高,故能保证实时进程总是比普通进程先被调度。 Linux2.6 中,优先级 prio 的计算不再集中在调度器选择 next 进程时,而是分散在进程状态改变的任何时候,这些时机有: 进程被创建时; 休眠进程被唤醒时; 从TASK_INTERRUPTIBLE 状态中被唤醒的进程被调度时; 因时间片耗尽或时间片过长而分段被剥夺 CPU 时; 在这些情况下,内核都会调用 effective_prio()重新计算进程的动态优先级 prio并根据计算结果调整它在就绪队列中的位置。 Linux 2.6为每个cpu定义了一个struck runqueue数据结构，每个就绪队列都有一个自旋锁，从而解决了 2.4 中因只有一个就绪队列而造成的瓶颈。 12345struct runqueue &#123; ... prio_array_t *active, *expired, array[2]; ... &#125; active 是指向活动进程队列的指针；expired 是指向过期进程队列的指针；array[2]是实际的优先级进程队列，其中一个是活跃的一个是过期的，过期数组存放时间片耗完的进程。 每个处理器的就绪队列都有两个优先级数组,它们是 prio_array 类型的结构体。Linux2.6内核正是因为使用了优先级数组,才实现了 O(1)调度算法。该结构定义在 kernel/sched.c 中: 123456789101112struct prio_array&#123; ... unsigned int nr_active; //相应 runqueue 中的进程数 unsigned long bitmap[BITMAP_SIZE]; /*索引位图，BITMAP_SIZE 默认值为 5,5个long(32位)类型，每位代表一个优先级，可以代表160个优先级，但实际中只有140。与下面的queue[]对应。分布0-99对应为实时进程，100-140对应为普通的进程*/ struct list_head queue[MAX_PRIO]; /*每个优先级的进程队列,MAX_PRIO 是系统允许的最大优先级数,默认值为 140,数值越小优先级越高 bitmap每一位都与 queue[i]相对应,当 queue[i]的进程队列不为空时,bitmap 相应位为 1,否则就为 0。*/&#125; 3. 调度算法介绍选择并运行候选进程，确定next,下一个应该占有 CPU 并运行的进程，schedule()函数是完成进程调度的主要函数，并完成进程切换的工作。schedule()用于确定最高优先级进程的代码非常快捷高效，其性能的好坏对系统性能有着直接影响，它在/kernel/sched.c 中的定义如下: 123456789101112...int idx;...preempt_disable();...idx = sched_find_first_bit( array -&gt; bitmap);queue = array -&gt; queue + idx;next = list_entry( queue -&gt; next, task_t, run_list);...prev = context_switch( rq, prev, next);...&#125; 其中,sched_find_first_bit()能快速定位优先级最高的非空就绪进程链表,运行时间和就绪队列中的进程数无关,是实现O(1)调度算法的一个关键所在。schedule()的执行流程:首先,调用 pre_empt_disable(),关闭内核抢占,因为此时要对内核的一些重要数据结构进行操作,所以必须将内核抢占关闭;其次,调用sched_find_first_bit()找到位图中的第1个置1的位，该位正好对应于就绪队列中的最高优先级进程链表;再者,调用 context_switch()执行进程切换,选择在最高优先级链表中的第 1个进程投入运行; 图中的网格为 140 位优先级数组,queue[7]为优先级为 7 的就绪进程链表。此种算法保证了调度器运行的时间上限,加速了候选进程的定位过程。 Linux2.4 调度系统在所有就绪进程的时间片都耗完以后在调度器中一次性重新计算,其中重算是用for循环相当耗时。 Linux2.6 为每个 CPU 保留 active 和 expired 两个优先级数组, active 数组中包含了有剩余时间片的任务,expired 数组中包含了所有用完时间片的任务。 当一个任务的时间片用完了就会重新计算其时间片,并插入到 expired 队列中,当 active 队列中所有进程用完时间片时,只需交换指向 active 和 expired 队列的指针即可。此交换是实现 O(1)算法的核心,由 schedule()中以下程序来实现: 1234567array = rq -&gt;active;if (unlikely(!array-&gt;nr_active)) &#123; rq -&gt; active = rq -&gt; expired; rq -&gt; expired = array; array = rq -&gt;active;...&#125; 4.Linux 2.6调度算法源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276/** * 调度函数。 */asmlinkage void __sched schedule(void)&#123; long *switch_count; /** * next指向被选中的进程，这个进程将取代当前进程在CPU上执行。 * 如果系统中没有优先级高于当前进程，那么next会和current相等。不发生任何切换。 */ task_t *prev, *next; runqueue_t *rq; prio_array_t *array; struct list_head *queue; unsigned long long now; unsigned long run_time; int cpu, idx; /* * Test if we are atomic. Since do_exit() needs to call into * schedule() atomically, we ignore that path for now. * Otherwise, whine if we are scheduling when we should not be. */ if (likely(!current-&gt;exit_state)) &#123; if (unlikely(in_atomic())) &#123; printk(KERN_ERR "scheduling while atomic: " "%s/0x%08x/%d\n", current-&gt;comm, preempt_count(), current-&gt;pid); dump_stack(); &#125; &#125; profile_hit(SCHED_PROFILING, __builtin_return_address(0));need_resched: /** * 先禁止抢占，再初始化一些变量。 * 此处需要禁止抢占，因为后面需要访问任务的运行队列。禁止抢占后可以防止进程飘移。 */ preempt_disable(); prev = current; /** * 释放大内核锁。当内核抢占打开时，并且当前中断正在抢占当前进程，那么会将lock_depth置为-1. * 这样，不会释放内核锁。只有当进程获得了大内核锁并且是主动调度出来时，才会释放锁。 * 注意，释放锁并不会修改lock_depth。当进程恢复执行后，如果lock_depth&gt;=0，就会再次获得大内核锁。 */ release_kernel_lock(prev);need_resched_nonpreemptible: rq = this_rq(); /* * The idle thread is not allowed to schedule! * Remove this check after it has been exercised a bit. */ if (unlikely(prev == rq-&gt;idle) &amp;&amp; prev-&gt;state != TASK_RUNNING) &#123; printk(KERN_ERR "bad: scheduling from the idle thread!\n"); dump_stack(); &#125; schedstat_inc(rq, sched_cnt); /** * 计算当前进程的运行时间。不超过1秒。 */ now = sched_clock(); if (likely(now - prev-&gt;timestamp &lt; NS_MAX_SLEEP_AVG)) run_time = now - prev-&gt;timestamp; else run_time = NS_MAX_SLEEP_AVG; /* * Tasks charged proportionately less run_time at high sleep_avg to * delay them losing their interactive status */ /** * 对有较长睡眠时间的进程，进行一定奖励。 */ run_time /= (CURRENT_BONUS(prev) ? : 1); /** * 在开始寻找可运行进程之前，需要关中断并获得保护运行队列的自旋锁。 */ spin_lock_irq(&amp;rq-&gt;lock); /** * 当前进程可能是一个正在准备被终止的进程。可能现在是通过do_exit进入schedule函数。 */ if (unlikely(prev-&gt;flags &amp; PF_DEAD)) prev-&gt;state = EXIT_DEAD; switch_count = &amp;prev-&gt;nivcsw; /** * 如果进程不是TASK_RUNNING状态，并且没有被内核抢占。就把该进程从运行队列中删除。 */ if (prev-&gt;state &amp;&amp; !(preempt_count() &amp; PREEMPT_ACTIVE)) &#123; switch_count = &amp;prev-&gt;nvcsw; /** * 如果进程是被信号打断的，就将它设置成TASK_RUNNING */ if (unlikely((prev-&gt;state &amp; TASK_INTERRUPTIBLE) &amp;&amp; unlikely(signal_pending(prev)))) prev-&gt;state = TASK_RUNNING; else &#123;/* 将它从运行队列中删除 */ if (prev-&gt;state == TASK_UNINTERRUPTIBLE) rq-&gt;nr_uninterruptible++; deactivate_task(prev, rq); &#125; &#125; cpu = smp_processor_id(); /** * 检查是否有可运行的进程。 */ if (unlikely(!rq-&gt;nr_running)) &#123;/* 没有了 */go_idle: /** * 运行队列中没有可运行的进程存在，调用idle_balance，从另外一个运行队列迁移一些可运行进程到本地运行队列中。 */ idle_balance(cpu, rq); if (!rq-&gt;nr_running) &#123;/* 没有迁移新进程到本运行队列。 */ next = rq-&gt;idle; rq-&gt;expired_timestamp = 0; /** * wake_sleeping_dependent重新调度空闲CPU中的可运行进程。主要是处于超线程的情况。 */ wake_sleeping_dependent(cpu, rq); /* * wake_sleeping_dependent() might have released * the runqueue, so break out if we got new * tasks meanwhile: */ if (!rq-&gt;nr_running)/* 如果支持超线程，并且其他逻辑CPU也没有可运行进程，那么只好运行IDLE进程了。 */ goto switch_tasks; &#125; &#125; else &#123;/* 有可能运行的进程 */ /** * dependent_sleeper一般返回为0,但是如果内核支持超线程技术，函数检查要被选中执行的进程。 * 其优先级是否比当前已经在相同物理CPU的逻辑CPU上运行的兄弟进程的优先级，如果新进程优先级低，就拒绝选择低优先级进程，而去执行swapper进程。 */ if (dependent_sleeper(cpu, rq)) &#123; next = rq-&gt;idle; goto switch_tasks; &#125; /* * dependent_sleeper() releases and reacquires the runqueue * lock, hence go into the idle loop if the rq went * empty meanwhile: */ if (unlikely(!rq-&gt;nr_running)) goto go_idle; &#125; /** * 运行到此，说明运行队列中有线程可被运行。 */ array = rq-&gt;active; if (unlikely(!array-&gt;nr_active)) &#123; /** * 活动队列中没有可运行进程了。交换活动集合和过期集合。 */ /* * Switch the active and expired arrays. */ schedstat_inc(rq, sched_switch); rq-&gt;active = rq-&gt;expired; rq-&gt;expired = array; array = rq-&gt;active; rq-&gt;expired_timestamp = 0; rq-&gt;best_expired_prio = MAX_PRIO; &#125; else schedstat_inc(rq, sched_noswitch); /** * 现在开始在活动集合中搜索一个可运行的进程。 * 首先搜索第一个非0位，并找到对应的链表。 */ idx = sched_find_first_bit(array-&gt;bitmap); queue = array-&gt;queue + idx; /** * 将下一个可运行进程描述符放到next中 */ next = list_entry(queue-&gt;next, task_t, run_list); /** * 如果进程是一个普通进程，并且是从TASK_INTERRUPTIBLE或者TASK_STOPPED状态被唤醒。 * 就把自从进程插入运行队列开始所经过的纳秒数加到平均睡眠时间中。 */ if (!rt_task(next) &amp;&amp; next-&gt;activated &gt; 0) &#123; unsigned long long delta = now - next-&gt;timestamp; /** * 如果是被系统调用服务例程或者内核线程所唤醒，就只增加部分睡眠时间(30%) * 否则增加100%的睡眠时间。这样，交互式进程由于经常被中断打断，它的睡眠时间会增加得更快。 */ if (next-&gt;activated == 1) delta = delta * (ON_RUNQUEUE_WEIGHT * 128 / 100) / 128; array = next-&gt;array; dequeue_task(next, array); recalc_task_prio(next, next-&gt;timestamp + delta); enqueue_task(next, array); &#125; next-&gt;activated = 0;switch_tasks: /** * 运行到这里，开始进行进程切换了。 */ if (next == rq-&gt;idle) schedstat_inc(rq, sched_goidle); /** * prefetch提示CPU控制单元把next的进程描述符的第一部分字段的内容装入硬件高速缓存。 * 这改善了schedule的性能。 */ prefetch(next); /** * 清除TIF_NEED_RESCHED标志。 */ clear_tsk_need_resched(prev); /** * 记录CPU正在经历静止状态。主要与RCU相关。 */ rcu_qsctr_inc(task_cpu(prev)); /** * 减少prev的平均睡眠时间 */ prev-&gt;sleep_avg -= run_time; if ((long)prev-&gt;sleep_avg &lt;= 0) prev-&gt;sleep_avg = 0; /** * 更新进程的时间戳 */ prev-&gt;timestamp = prev-&gt;last_ran = now; sched_info_switch(prev, next); if (likely(prev != next)) &#123;/* prev和next不同，需要切换 */ next-&gt;timestamp = now; rq-&gt;nr_switches++; rq-&gt;curr = next; ++*switch_count; prepare_arch_switch(rq, next); /** * context_switch执行真正的进程切换 */ prev = context_switch(rq, prev, next); /** * 当进程再次被切换进来后，以下代码被接着运行。 * 但是此时prev并不指向当前进程，而是指代码从哪一个进程切换到本进程。 * 由于此时已经进行了进程空间的切换，寄存器中缓存的变量等都不再有效，所以用barrier产生一个优化屏障。 */ barrier(); /** * 对前一个进程进行一些收尾工作，比如减少它的mm_struct,task_struct的引用计数等。 */ finish_task_switch(prev); &#125; else/* 如果prev和next是同一个进程，就不做进程切换。当prev仍然是当前活动集合中的最高优先级进程时，这是有可能发生的。 */ spin_unlock_irq(&amp;rq-&gt;lock); /** * 在前几句中(context_switch之后)，prev代表的是从哪个进程切换到本进程。 * 在继续进行调度之前(因此在context_switch中开了中断，可能刚切回本进程就来了中断，并需要重新调度)，将prev设置成当前进程。 */ prev = current; /** * 重新获得大内核锁。 */ if (unlikely(reacquire_kernel_lock(prev) &lt; 0)) goto need_resched_nonpreemptible; /** * 打开抢占，并检查是否需要重新调度。 */ preempt_enable_no_resched(); if (unlikely(test_thread_flag(TIF_NEED_RESCHED))) goto need_resched;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[linux哈希表]]></title>
    <url>%2FLinux%2Flinux%E5%93%88%E5%B8%8C%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[1. 前言Linux内核中选取双向链表作为其基本的数据结构，并将其嵌入到其他的数据结构中，使得其他的数据结构不必再一一实现其各自的双链表结构。实现了双链表结构的统一，同时可以演化出其他复杂数据结构。本文对linux中基于双链表实现的哈希表进行分析，并编写内核模块，调用其中的函数和宏，实现哈希表的建立和查找。 2. 哈希表的定义散列表（Hash table，也叫哈希表），是根据键（Key）而直接访问在内存储存位置的数据结构。也就是说，它通过计算一个关于键值的函数，将所需查询的数据映射到表中一个位置来访问记录，这加快了查找速度。这个映射函数称做散列函数，存放记录的数组称做散列表。 通过哈希函数使用关键字计算存储地址的时候，不可避免的会产生冲突，通常处理冲突的方法有：开放定地址法（线性探测、平方探测）、单独链表法、双散列、再散列。linux中使用了其中的单独链表法，即利用了我们上面介绍的双向链表实现，将散列到同一个存储位置的所有元素保存在一个链表中。linux中关于哈希表结构体的定义可以从/usr/src/linux-headers-5.4.0-48-generic/include/linux/types.h文件中找到。 1234567struct hlist_head &#123; struct hlist_node *first;&#125;;struct hlist_node &#123; struct hlist_node *next, **pprev;&#125;; 可以看到哈希表包含两个数据结构，一个是哈希链表节点hlist_node，另一个是哈希表头hlist_head。可以看到哈希节点hlist_node和内核普通双向链表的节点唯一的区别就在于，前向节点pprev是个两级指针。同时并没有使用hlist_node作为哈希表头，而是重新定义了hlist_head结构体，这是因为哈希链表并不需要双向循环，为了节省空间使用一个指针first指向该哈希表的第一个节点就可以了。整个哈希表结构如下图所示，其中ppre是个二级指针，它指向前一个节点的第一个指针变量，例如node1的ppre指向mylist的first指针，node2的ppre指向node1的next指针。 之所以使用ppre二级指针是为了避免在首节点之后插入删除节点和在其他位置插入删除节点实现逻辑的不同，读者可以将ppre改成一级指针指向前一个节点，就可以发现实现逻辑的不同。 3. 哈希表的声明和初始化宏Linux的链表和散列表的操作函数的定义在/usr/src/linux-headers-5.4.0-48-generic/include/linux/list.h文件中，接下来就打开这个文件看一下hlist数据结构的操作函数和宏。 1234#define HLIST_HEAD_INIT &#123; .first = NULL &#125;#define HLIST_HEAD(name) struct hlist_head name = &#123; .first = NULL &#125;#define INIT_HLIST_HEAD(ptr) ((ptr)-&gt;first = NULL) 这三个初始化宏都是建立一个hlist_head结构体，并把first成员设置为NULL。 初始化hlist_node结构体，把两个成员变量赋值为NULL。 123456static inline void INIT_HLIST_NODE(struct hlist_node *h)&#123; h-&gt;next = NULL; h-&gt;pprev = NULL;&#125; 4. 在哈希表中增加节点在内核代码list.h中增加节点的函数为： 12345678910static inline void hlist_add_head(struct hlist_node *n, struct hlist_head *h)static inline void hlist_add_before(struct hlist_node *n, struct hlist_node *next)static inline void hlist_add_behind(struct hlist_node *n, struct hlist_node *prev)static inline void hlist_add_fake(struct hlist_node *n) hlist_add_head是把一个哈希链表的节点插入到哈希链表的头节点的后边，也就是头插法。传入了哈希表头h和待插入的节点n，首先得到hlist_head的first成员，就是后边的节点的指针，这个节点可能是NULL，然后新插入的节点的next指向first后边的节点，如果first不为空，也就是后边有节点存在，head的后边的节点的pprev成员就指向新插入的节点的next成员的地址，head的first就指向新插入的节点，新插入节点的pprev成员指向head的first成员的地址。 12345678910static inline void hlist_add_head(struct hlist_node *n, struct hlist_head *h)&#123; struct hlist_node *first = h-&gt;first; n-&gt;next = first; if (first) first-&gt;pprev = &amp;n-&gt;next; h-&gt;first = n; n-&gt;pprev = &amp;h-&gt;first;&#125; 每次插入一个节点后，哈希表的存储情况如下图所示。 hlist_add_before作用是把一个节点插入到一个哈希链表的节点的前边，首先把将要插入的节点的pprev成员变量指向next的前边的节点，要插入的节点的next指向下一个节点，然后next节点的pprev就要指向已经插入的节点的next节点的地址，已经插入的节点的pprev指向的前一个节点的值就要变成已经插入节点的地址。 123456789static inline void hlist_add_before(struct hlist_node *n, struct hlist_node *next)&#123; n-&gt;pprev = next-&gt;pprev; n-&gt;next = next; next-&gt;pprev = &amp;n-&gt;next; *(n-&gt;pprev) = n;&#125; 5. 遍历哈希表list.h中定义了如下遍历链表的宏，\代表换行： 123#define hlist_for_each(pos, head) \ for (pos = (head)-&gt;first; pos &amp;&amp; (&#123; prefetch(pos-&gt;next); 1; &#125;); \ pos = pos-&gt;next) 这个宏是对哈希链表进行遍历的宏，pos代表一个hlist_node结构体指针，head代表hlist_head结构体，就是哈洗链表的头。得到pos后，在宏展开后就可以在循环体中取到结构体具体的数值。 6. 哈希表的应用6.1 搭建Clion内核模块开发环境Clion用CMake构建C或者C++工程（20版本的CLion也支持自定义Makefile），利用CMake的配置文件创建编辑器本身的代码环境，比如子项目，比如预处理宏等，这跟IntelliJ Idea利用Maven、Gradle构建项目管理依赖库是同样的道理。 在Ubuntu系统下启动Clion新建工程，选择C executable项目，C语言标准选择99。 项目新建后会有一个CMakeLists.txt文件，main.c文件，我们可以新建一个src文件夹存放linux模块代码。同时将src文件夹Mark Directory as Project sources and Headers。 接下来我们在CMake配置文件中引入我们开发所需要的内核头文件。 1234567891011121314151617181920212223# 设置Cmake版本需要和系统中的Cmake版本保持一致cmake_minimum_required(VERSION 3.10)# 工程名称project(os C)# 设置编译选项set(CMAKE_C_FLAGS "$&#123;CMAKE_C_FLAGS&#125; -std=c99 -nostdinc") # C 编译器设置add_definitions(-D__KERNEL__=1) #手动添加预处理宏# 设置内核路径set(KERNEL_ROOT /usr/src/linux-headers-5.4.0-48-generic)# 添加需要的内核文件include_directories( # kernel headers "$&#123;KERNEL_ROOT&#125;/include" #"$&#123;KERNEL_ROOT&#125;/arch/arm/include" #"$&#123;KERNEL_ROOT&#125;/arch/arm64/include" # kernel source #"$&#123;KERNEL_ROOT&#125;/mm")set(SOURCE_FILES main.c)add_executable(os $&#123;SOURCE_FILES&#125;) 如何不通过add_definitions(-D__KERNEL__=1)手动添加KERNEL宏，有很多宏，CLion将无法识别。 kernel为了保护其代码，故意设置了这么一个宏，只有在编译kernel的时候才会在命令行传递这个宏进去，像libc等C runtime编译时要用到kernel的部分代码，但是它不会设置KERNEL这个宏，于是被这个宏保护的kernel代码它是看不到的。自然，我们的Clion没有定义这个宏，那些kernel代码(大部分代码)它也是看不到的，于是parser在解析头文件时，很多符号是解析不出来的。 配置完成后就可以使用CLion进行开发了。 6.2 哈希表的使用下面编写一个linux内核模块，用以创建、增加、删除和遍历一个哈希表。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677// hashlist.c// Created by linux on 2020/9/25.//#include &lt;linux/kernel.h&gt;#include &lt;linux/module.h&gt;#include &lt;linux/slab.h&gt;#include &lt;linux/list.h&gt;MODULE_LICENSE("GPL");MODULE_AUTHOR("linux");#define N 10//数字链表struct numlist&#123; struct hlist_head hlistHead;&#125;;//数字链表节点struct numnode&#123; int num; struct hlist_node hlistNode;&#125;;struct numlist nhead;struct numnode nnode;static int __init hlist_init(void)&#123; //init head node struct hlist_node *pos; struct numnode *listnode; int i; printk("hashlist is starting...\n"); //初始化头节点 INIT_HLIST_HEAD(&amp;nhead.hlistHead); for ( i = 0; i &lt; N; ++i) &#123; listnode = (struct numnode *)kmalloc(sizeof(struct numnode),GFP_KERNEL); listnode-&gt;num = i+1; //添加节点在头节点之前 hlist_add_head(&amp;(listnode-&gt;hlistNode),&amp;nhead.hlistHead); printk("Node %d has added to the hash list...\n",i+1); &#125; //遍历链表 i = 1; struct numnode *p; hlist_for_each(pos,&amp;nhead.hlistHead)&#123; //取得数字节点的数据域 p = hlist_entry(pos,struct numnode,hlistNode); printk("Node %d data:%d\n",i,p-&gt;num); i++; &#125; return 0;&#125;static void __exit hashlist_exit(void)&#123; struct hlist_node *pos,*n; struct numnode *p; int i; i =1; //遍历数字链表 hlist_for_each_safe(pos,n,&amp;nhead.hlistHead)&#123; //删除哈希节点 hlist_del(pos); //取得删除节点的数据域值 p = hlist_entry(pos,struct numnode,hlistNode); kfree(p); printk("Node %d has removed from the hashlist ...\n",i++); &#125; printk("hash list is exiting...\n");&#125;module_init(hlist_init);module_exit(hashlist_exit); 对应的Makefile文件如下： 123456789obj-m := hashlist.oCURRENT_PATH := $(shell pwd)LINUX_KERNEL := $(shell uname -r)LINUX_KERNEL_PATH :=/usr/src/linux-headers-$(LINUX_KERNEL)all: make -C $(LINUX_KERNEL_PATH) M=$(CURRENT_PATH) modulesclean: make -C $(LINUX_KERNEL_PATH) M=$(CURRENT_PATH) clean 参考资料：Linux操作系统原理与应用（第二版）https://zh.wikipedia.org/wiki/%E5%93%88%E5%B8%8C%E8%A1%A8https://developer.aliyun.com/article/515874https://blog.csdn.net/sanwenyublog/article/details/50747219http://ybin.cc/tools/clion-for-linux-driver-developer/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>哈希表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编写块设备驱动程序]]></title>
    <url>%2Funcategorized%2F%E7%BC%96%E5%86%99%E5%9D%97%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[1. 前言块设备也和字符设备一样可以通过/dev目录下的设备文件来访问。此外块设备(例如磁盘）上能够容纳文件系统。我们来看一下/dev目录下的一些成员。 访问权限之前的字母是b或c，分别表示块设备和字符设备。设备文件没有文件长度，而增加了主设备号和从设备号。二者共同形成一个唯一的号码，内核可由此查找对应的设备驱动程序。 驱动程序则负责将用户的一组功能调用映射作用于实际硬件设备的特有操作上，是系统软件与硬件设备沟通的桥梁。 编写简单驱动程序主要包含以下步骤。 2. 注册2.1 块设备驱动程序注册块设备要想被内核知道其存在，必须使用内核提供的一系列注册函数进行注册。驱动程序的第一步就是向内核注册自己，提供该功能的函数是 参数是该设备使用的主设备号及其名字，name通常与设备文件名称相同，但也可以是任意有效的字符串。 如果传递的主设备号是0,内核将分派一个新的主设备号给设备，并将该设备号返回给调用者。 使用该函数，块设备将会显示在/proc/devices。 对应的注销函数为 2.2 磁盘注册通过注册驱动程序我们获得了主设备号，但是现在还不能对磁盘进行操作。内核对于磁盘的表示是使用的gendisk结构体， gendisk结构中的许多成员必须由驱动程序进行初始化。 gendisk结构是一个动态分配的结构，它需要一些内核的特殊处理来进行初始化;驱动程序不能自己动态分配该结构，而是必须调用 参数是该磁盘使用的次设备号数目。当不再需要一个磁盘时，调用下面的函数卸载磁盘 分配一个gendisk结构并不能使磁盘对系统可用。为达到这个目的，必须初始化结构，并调用add_disk。Gendisk中包含了一个指针struct block_ device_ operations * fops ;指向对应的块设备操作函数，接下来看一下block_ device_ operations都有哪些函数需要驱动程序来实现。 3. 块设备操作字符设备使用file__operations结构告诉系统对它们的操作接口。块设备使用类似的数据结构，在&lt;linux/fs.h&gt;中声明了结构block_ device_ operations。同时块设备在VFS层也提供了统一的标准操作结构file_ operations。 open、release和ioctl 与file_ operations中 等价函数的语义相同，分别用于打开、关闭文件以及向块设备发送特殊命令（查询设备物理信息，扇区，磁头数）。 file_ operations 与block device operations的结构类似，但不能混淆二者。file_ operations由VFS层用来与用户空间通信，其中的例程会调用block_ device_operations中的函数，以实现与块设备的通信。block_ device_ operations必须针对各种块设备分别实现，对设备的属性加以抽象，而在此之上建立的file_ operations, 使用同样的操作即可处理所有的块设备。 3.1 请求队列块设备的读写请求放置在一个队列上，称之为请求队列。gendisk结构包括了一个指针，指向这个特定于设备的队列，由以下数据类型表示。 queue_ head是该数据结构的主要成员，是一个表头，用于构建一个I/O请求的双链表。 链表每个元素的数据类型都是request,代表向块设备读取数据的一个请求。我们需要为gendisk创建并初始化对应的请求队列，函数如下： 该函数的参数是一个需要驱动实现的函数，用来处理该队列中的request和控制访问队列权限的自旋锁。 3.2 请求处理请求队列创建初始化如下所示：其中request的处理函数my_request编写如下：主要实现的功能有： 使用blk_fetch_request函数获取队列中的request，循环处理队列中的request。 获取请求的起始地址与读写扇区数。 根据读写的请求不同，分别处理。（因为是用内存模拟的设备，使用memcpy函数直接拷贝数据）4. 编译加载驱动 编写对应的Makefile文件，使用make命令，编译生成.ko文。 123456789101112ifneq ($(KERNELRELEASE),)obj-m := ramdisk_driver.o elsePWD := $(shell pwd)KVER := $(shell uname -r)KDIR := /lib/modules/$(KVER)/buildall: $(MAKE) -C $(KDIR) M=$(PWD) modulesclean: rm -rf .*.cmd *.o *.mod.c *.ko .tmp_versions modules.* Module.*endif 编译结果：使用insmod命令加载驱动程序。使用dmesg命令查看系统日志信息，可以看到在自定义request处理函数中打印的日志信息。同时在设备文件列表中可以看到设备myramdisk。 5. 格式化磁盘使用ext4文件系统格式化设备myramdisk，格式化的过程中可以看到打印出的每一次request处理的信息。 6. 挂载设备最后使用mount命令将设备挂载到/mnt目录下，就可以像其他设备一样进行数据的读写操作。为什么一个设备已经被系统识别在/dev下，为什么不能直接访问，而需要继续mount。原因在于，设备文件只能读取设备自身的一些基本信息。如果读取内部数据的话，由于块设备支持文件系统，很多设备的文件系统并不一样没法直接读取。必须得按照一定的格式去解析设备里的文件。而mount就按照你指定的格式去读取设备里的数据。 7. 总结本文基于内核版本2.6简单介绍了，一个块设备驱动程序编写所需的步骤。其中的大多数api在内核版本5.0以后已经不再使用，后面将对新的api进行总结。 参考资料：《LINUX设备驱动程序》《深入LINUX内核架构》参考源码]]></content>
  </entry>
  <entry>
    <title><![CDATA[dd命令测试磁盘]]></title>
    <url>%2Funcategorized%2Fdd%E5%91%BD%E4%BB%A4%E6%B5%8B%E8%AF%95%E7%A3%81%E7%9B%98%2F</url>
    <content type="text"><![CDATA[1. 测试工具1.1 Linux dd命令Linux dd命令用于读取、转换并输出数据。dd可从标准输入或文件中读取数据，根据指定的格式来转换数据，再输出到文件、设备或标准输出。 参数说明: if=文件名：输入文件名，默认为标准输入。即指定源文件。 of=文件名：输出文件名，默认为标准输出。即指定目的文件。 ibs=bytes：一次读入bytes个字节，即指定一个块大小为bytes个字节。obs=bytes：一次输出bytes个字节，即指定一个块大小为bytes个字节。bs=bytes：同时设置读入/输出的块大小为bytes个字节。 cbs=bytes：一次转换bytes个字节，即指定转换缓冲区大小。 skip=blocks：从输入文件开头跳过blocks个块后再开始复制。 seek=blocks：从输出文件开头跳过blocks个块后再开始复制。 count=blocks：仅拷贝blocks个块，块大小等于ibs指定的字节数。 conv=&lt;关键字&gt;，关键字可以有以下11种： conversion：用指定的参数转换文件。 ascii：转换ebcdic为ascii ebcdic：转换ascii为ebcdic ibm：转换ascii为alternate ebcdic block：把每一行转换为长度为cbs，不足部分用空格填充 unblock：使每一行的长度都为cbs，不足部分用空格填充 lcase：把大写字符转换为小写字符 ucase：把小写字符转换为大写字符 swab：交换输入的每对字节 noerror：出错时不停止 notrunc：不截短输出文件 sync：将每个输入块填充到ibs个字节，不足部分用空（NUL）字符补齐。 –help：显示帮助信息 –version：显示版本信息 1.2 /dev/zero/dev/zero，是一个输入设备，可用它来初始化文件。该设备无穷尽地提供0，可以使用任何需要的数目——设备提供的要多的多。它可以用于向设备或文件写入字符串0。 1.3 /dev/null/dev/null 是一个特殊的文件，写入到它的内容都会被丢弃；如果尝试从该文件读取内容，那么什么也读不到。但是 /dev/null 文件非常有用，将命令的输出重定向到它，会起到”禁止输出”的效果。 2. 测试命令2.1 异步缓存方式 dd bs=8k count=4k if=/dev/zero of=test 该命令表示，每次从/dev/zero中读或者写8k，一共执行4k次，输出到test文件中。 没有任何参数，dd默认的方式不包括“同步(sync)”命令（没加关于操作系统“写缓存”的参数，默认“写缓存”启作用），也就是说，dd命令完成前并没有让系统真正把文件写到磁盘上。dd先把数据写到操作系统“写缓存”，就完成了写操作。所以以上命令只是单纯地把数据读到内存缓冲当中（写缓存[write cache]）。通常称为update的系统守护进程会周期性地（一般每隔30秒）调用sync函数，把“写缓存”中的数据刷入磁盘。因为“写缓存”起作用，会测试出一个超快的性能。因为dd给的只是读取速度，直到dd完成后系统才开始真正往磁盘上写数据，但这个速度是看不到了。 dd命令执行结果如下，可以看到向test文件中成功写入了约34MB数据。 2.2 一次同步2.2.1同步包含文件元数据 dd bs=8k count=4k if=/dev/zero of=test conv=fsync 加入fsync 参数后，dd命令执行到最后会真正执行一次“同步(sync)”操作，，这样算出来的时间才是比较符合实际使用结果的。conv=fsync表示把文件的“数据”和“metadata”都写入磁盘（metadata包括size、访问时间st_atime &amp; st_mtime等等），因为文件的数据和metadata通常存在硬盘的不同地方，因此fsync至少需要两次IO写操作，fsync 与fdatasync相差不大。 2.2.2 同步不含文件元数据 dd bs=8k count=4k if=/dev/zero of=test conv=fdatasync 加入fdatasync参数后，dd命令执行到最后会真正执行一次“同步(sync)”操作，conv=fdatasync表示只把文件的“数据”写入磁盘，而不写入文件的元数据。 2.3 每次同步 dd bs=8k count=4k if=/dev/zero of=test oflag=dsync 加入dsync参数后，dd在执行时每次都会进行同步写入操作。每次读取8k后就要先把这8k写入磁盘，然后再读取下面一个8k，一共重复4K次，生成一个34M文件。这是最慢的一种方式，基本上没有用到写缓存(write cache)。 conv=fdatasync与oflag=dsync的区别在于：fdatasync只是将所有修改过的块缓冲区排入写队列，然后就返回，它并不等待实际写磁盘操作结束。dsync函数只对由文件描述符filedes指定的单一文件起作用，并且等待写磁盘操作结束，然后返回。所以看到的fdatasync速度比dsync好。]]></content>
  </entry>
  <entry>
    <title><![CDATA[编写文件系统之装载与卸载]]></title>
    <url>%2Funcategorized%2F%E7%BC%96%E5%86%99%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%A3%85%E8%BD%BD%E4%B8%8E%E5%8D%B8%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[前言一个通常意义上的文件系统驱动可以单独被编译成模块动态加载，也可以被直接编译到内核中，为了调试的方便，本文中的文件系统采用动态加载的方式实现。实现一个文件系统必须遵照内核的一些“规则”，以下我将以递进的顺序阐述文件系统的实现过程。 1.文件系统的加载与卸载首先为了能够成功加载文件系统，文件系统需要提供文件系统的名字，超级块的加载和删除方法。这些东西反应在file_system,_type中。 123456struct file_system_type MISER_fs_type = &#123; .owner = THIS_MODULE, .name = &quot;MISER_fs&quot;, .mount = MISER_fs_mount, .kill_sb = MISER_fs_kill_superblock, /* unmount */ &#125;; 文件系统作为一种块设备驱动，自然也需要实现module_init以及mocule_exit。代码如下： 123456789101112131415161718192021222324252627/* Called when the module is loaded. */ int MISER_fs_init(void) &#123; int ret; ret = register_filesystem(&amp;MISER_fs_type); if (ret == 0) printk(KERN_INFO &quot;Sucessfully registered MISER_fs\n&quot;); else printk(KERN_ERR &quot;Failed to register MISER_fs. Error: [%d]\n&quot;, ret); return ret; &#125; /* Called when the module is unloaded. */ void MISER_fs_exit(void) int ret; ret = unregister_filesystem(&amp;MISER_fs_type); if (ret == 0) printk(KERN_INFO &quot;Sucessfully unregistered MISER_fs\n&quot;); else printk(KERN_ERR &quot;Failed to unregister MISER_fs. Error: [%d]\n&quot;, ret); &#125; module_init(MISER_fs_init); module_exit(MISER_fs_exit); MODULE_LICENSE(&quot;MIT&quot;); MODULE_AUTHOR(&quot;cv&quot;); 我们可以看到，设备驱动加载的时候，驱动向内核注册了文件系统，而驱动卸载的时候，文件系统的信息也被删除。文件系统加载时调用的函数为MISER_fs_mount，实际上，这个函数向内核注册了一个回调： int MISER_fs_fill_super(struct super_block *sb, void *data, int silent)这个函数是用来与VFS交互从而生成VFS超级块的。在MISER fs中，超级块在磁盘的第二个4096字节上，即块号为1。这个函数执行时会从磁盘中读取信息，填充到VFS提供的超级块结构体中，下列为部分关键代码。 1234567891011121314151617int MISER_fs_fill_super(struct super_block *sb, void *data, int silent) &#123; struct buffer_head *bh; bh = sb_bread(sb, 1); struct MISER_fs_super_block *sb_disk; sb_disk = (struct MISER_fs_super_block *)bh-&gt;b_data; struct inode *root_inode; if (sb_disk-&gt;block_size != 4096) &#123; printk(KERN_ERR &quot;MISER_fs expects a blocksize of %d\n&quot;, 4096); ret = -EFAULT; goto release; &#125; //fill vfs super block sb-&gt;s_magic = sb_disk-&gt;magic; sb-&gt;s_fs_info = sb_disk; sb-&gt;s_maxbytes = MISER_BLOCKSIZE * MISER_N_BLOCKS; /* Max file size */ sb-&gt;s_op = &amp;MISER_fs_super_ops; &#125; 从上述代码可以看出，我们用sb_read来读取磁盘上的内容，然后填充super_block结构体。值得注意的是，有关超级块的操作函数即superblock_operations也是在此处赋值的。由于super_block* sb在文件系统卸载之前是一直存在于内存中的，所以我们可以使用s_fs_info来存储原始的超级块信息，避免后期交互时 再次读取磁盘。文件系统卸载的时候超级块信息需要被删除，所以MISER_fs_kill_superblock的作用时释放该超级块，通知VFS该挂载点已经卸载。实现基本函数后，可以对文件系统进行挂载操作，挂载操作的脚本内容如下： 1234567sudo umount ./test sudo rmmod MISER_fs dd bs=4096 count=100 if=/dev/zero of=image ./mkfs image insmod MISER_fs.ko mount -o loop -t MISER_fs image ./test dmesg 上述脚本，将项目下的test文件夹作为文件系统的挂载点，并在挂载之后答应出了内核调试目录。成功执行该脚本的截图如下：成功运行我们可以看到test目录已经挂载成功而且内核调试信息显示文件系统挂载成功。 2.ls命令的实现加载文件系统之后第一个要实现的功能是读取文件系统中的数据，所以选择实现文件夹读取操作，这一操作在2.x内核中是.readdir函数指针，在最新版本中是,.iterate函数指针。这个指针在保存在file_operation中，如下所示。 1234const struct file_operations MISER_fs_dir_ops = &#123; .owner = THIS_MODULE, .iterate = MISER_fs_iterate, &#125;; MISER_fs_iterate函数主要功能逻辑是读取inode的块数据，并且将块数据中的inode和文件名通过dir_emit函数传输到VFS层。以根目录为例，根目录的包含三个数据项，分别是父目录，当前目录和欢迎文件，所以该函数会执行以下三个语句 1234//参数分别表示上下文，文件/目录名，文件/目录名长度，inode号，文件类型 dir_emit(ctx, &quot;.&quot;, 1,0, DT_DIR); dir_emit(ctx, &quot;..&quot;, 2,0, DT_DIR); dir_emit(ctx, &quot;file&quot;, 4,1, DT_REG); 完成该函数后，在填充根目录inode时将MISER_fs_dir_ops指针赋值，即可在挂在文件系统后执行ls命令。成功运行ls如上图所示，我们成功看到了欢迎文件。但是此时我们不能对文件进行任何操作，因为还没有实现其他的接口。 3.磁盘管理相关逻辑的实现这个磁盘管理的内涵包括向磁盘写入和从磁盘取出读取inode，更新inode信息，维护imap，bmap，inode table等操作。为了使磁盘上的内容有序的组合起来，磁盘空间的管理十分的重要，后续的文件读写操作都与此相关。写入和删除inode的操作存放在super_operations这个结构体中。 12345678910111213141516171819202122232425const struct super_operations MISER_fs_super_ops = &#123; .evict_inode = MISER_evict_inode, .write_inode = MISER_write_inode, &#125;;MISER_fs_super_ops需要在填充超级块时赋值到super_block的s_ops字段中。MISER_write_inode函数的功能是将内存中的inode保存在磁盘上。关键代码如下。int MISER_write_inode(struct inode *inode, struct writeback_control *wbc) &#123; struct buffer_head * bh; struct MISER_inode * raw_inode = NULL; MISER_fs_get_inode(inode-&gt;i_sb, inode-&gt;i_ino, raw_inode); if (!raw_inode) return -EFAULT; raw_inode-&gt;mode = inode-&gt;i_mode; raw_inode-&gt;i_uid = fs_high2lowuid(i_uid_read(inode)); raw_inode-&gt;i_gid = fs_high2lowgid(i_gid_read(inode)); raw_inode-&gt;i_nlink = inode-&gt;i_nlink; raw_inode-&gt;file_size = inode-&gt;i_size; raw_inode-&gt;i_atime = (inode-&gt;i_atime.tv_sec); raw_inode-&gt;i_mtime = (inode-&gt;i_mtime.tv_sec); raw_inode-&gt;i_ctime = (inode-&gt;i_ctime.tv_sec); mark_buffer_dirty(bh); brelse(bh); return 0; &#125; 可以看到，该函数的将vfs inode中的相关信息存储到MISER_inode结构体中，然后写入磁盘。这个是单独的写入磁盘操作，事实上，当我们申请inode时，imap也是需要检查刷新的，需要把相应位置标记为1。同理，evict_inode函数的作用时删除inode，删除成功后，我们需要刷新imap的值，把相应位置标记为0。设置和写入map的操作都在map.c中，以下以imap为例。对于imap来讲，申请inode的时候需要检查第一个空闲的inode编号，当inode被释放的时候也要及时清零对应的imap。与此相关的函数如下。 123456789//从磁盘中读取数据并存在imap数组中 int get_imap(struct super_block* sb, uint8_t* imap, ssize_t imap_size); //在vaddr数组中找到第一个为0的bit，这个函数用于定位空inode或者block int MISER_find_first_zero_bit(const void *vaddr, unsigned size); //将imap的某一位置0或者1，并保存在磁盘上 int set_and_save_imap(struct super_block* sb, uint64_t inode_num, uint8_t value); //定义的位操作宏如下 #define setbit(number,x) number |= 1UL &lt;&lt; x #define clearbit(number, x) number &amp;= ~(1UL &lt;&lt; x) 由于本文件系统并不是为了实际使用，所以上述的操作都没有考虑性能以及准确性问题。事实上，能够加上校验或者冗余备份是最好的。 4.读写文件内容为了能够快速看到文件系统在正常工作，所以接下来需要实现文件的读写操作。文件读写操作按照一般处理，应该是实现在struct file_operations这个结构体中的。事实上，最开始我是实现在这个结构体中的read_iter函数指针中的。但是比较有趣的一点是，如果我们实现了struct address_space_operations结构体中的函数，那么struct file_operations结构体中的函数则可以交由VFS实现。代码如下： 1234567891011121314const struct file_operations MISER_fs_file_ops = &#123; .owner = THIS_MODULE, .llseek = generic_file_llseek, .mmap = generic_file_mmap, .fsync = generic_file_fsync, .read_iter = generic_file_read_iter, .write_iter = generic_file_write_iter, &#125;; const struct address_space_operations MISER_fs_aops = &#123; .readpage = MISER_fs_readpage, .writepage = MISER_fs_writepage, .write_begin = MISER_fs_write_begin, .write_end = generic_write_end, &#125;; 上述的generic开头的函数是不需要我们手动实现的。上述的address_space_operations操作其实是实现了页高速缓存的一些操作。页高速缓存是linux内核实现的一种主要磁盘缓存，它主要用来减少对磁盘的IO操作，具体地讲，是通过把磁盘中的数据缓存到物理内存中，把对磁盘的访问变为对物理内存的访问。这些接口一旦实现，那么对文件的操作就可以转移到内存中，这就是为什么可以使用generic开头的这些函数来代替手写。MISER_fs_readpage, MISER_fs_writepage以及MISER_fs_write_begin都被注册回调到同一个函数MISER_fs_get_block。MISER_fs_get_block主要返回内核请求长度的数据。至于读写操作，内核调用__bwrite函数最终调用块设备驱动执行。因为在我没有采用二级或者多级索引，故而MISER_fs_get_block函数逻辑比较简单，部分代码如下： 123456789101112131415int MISER_fs_get_block(struct inode *inode, sector_t block, struct buffer_head *bh, int create) &#123; struct super_block *sb = inode-&gt;i_sb; if (block &gt; MISER_N_BLOCKS) return -ENOSPC; struct MISER_inode H_inode; if (-1 == MISER_fs_get_inode(sb, inode-&gt;i_ino, &amp;H_inode)) return -EFAULT; if (H_inode.blocks == 0) if(alloc_block_for_inode(sb, &amp;H_inode, 1)) return -EFAULT; map_bh(bh, sb, H_inode.block[block]); return 0; &#125; 如上所示，该函数判断传入的block的大小，并将磁盘内容映射到bh中。后续的读写操作将有VFS帮我们完成。 5.inode操作Inode操作涉及文件(夹)的创建删除，将MISER_inode映射到VFS中的inode等操作。具体实现的函数如下。 123456const struct inode_operations MISER_fs_inode_ops = &#123; .lookup = MISER_fs_lookup, .mkdir = MISER_fs_mkdir, .create = MISER_fs_create, .unlink = MISER_fs_unlink, &#125;; MISER_fs_lookup是其中比较复杂的一个函数，它负责将一个目录下的inode信息交由VFS管理。首先，MISER_fs_lookup读取文件夹的内容，然后遍历文件夹下面的MISER_inode，找到我们想要的MISER_inode，根据不同的文件属性，申请vfs_inode；并对不同的vfs_inode设置不同的操作。假设vfs_inode对应的是一个文件，那么就设置vfs_inode-&gt;mapping-&gt;a_ops，如果vfs_inode对应的是文件夹，那么就设置vfs_inode-&gt;f_ops = &MISER_fs_dir_ops;最后将vfs_inode注册到VFS中。这部分的关键代码如下： 123456789101112131415161718192021222324252627282930313233struct dentry *MISER_fs_lookup(struct inode *parent_inode, struct dentry *child_dentry, unsigned int flags) &#123; struct super_block *sb = parent_inode-&gt;i_sb; struct MISER_inode H_inode; //省略代码 for (i = 0; i &lt; H_inode.dir_children_count; i++) &#123; if (strncmp (child_dentry-&gt;d_name.name, dtptr[i].filename, MISER_FILENAME_MAX_LEN) == 0)&#123; inode = iget_locked(sb, dtptr[i].inode_no); if (inode-&gt;i_state &amp; I_NEW) &#123; inode_init_owner(inode, parent_inode, 0); struct MISER_inode H_child_inode; if (-1 == MISER_fs_get_inode(sb, dtptr[i].inode_no, &amp;H_child_inode)) return ERR_PTR(-EFAULT); MISER_fs_convert_inode(&amp;H_child_inode, inode); inode-&gt;i_op = &amp;MISER_fs_inode_ops; if (S_ISDIR(H_child_inode.mode)) &#123; inode-&gt;i_fop = &amp;MISER_fs_dir_ops; &#125; else if (S_ISREG(H_child_inode.mode)) &#123; inode-&gt;i_fop = &amp;MISER_fs_file_ops;; inode-&gt;i_mapping-&gt;a_ops = &amp;MISER_fs_aops; &#125; inode-&gt;i_mode = H_child_inode.mode; inode-&gt;i_size = H_child_inode.file_size; insert_inode_hash(inode); unlock_new_inode(inode); &#125; &#125; &#125; //省略代码 &#125; 只有在这里注册了相关函数，系统调用才能正常执行。不然就会出现不支持的操作这种报错信息。.create与.mkdir都是对应了inode的创建，只是inode的属性不能而已。.create创建普通文件而.mkdir创建文件夹。所以这两个函数的功能被函数MISER_fs_create_obj所处理。这个函数接受新建文件（夹）的请求，检查磁盘的大小，检查是否有空余的indoe，并且分配inode号，然后更新imap信息，最后更新超级块信息。由于该函数逻辑简单但是代码量比较大，故而不在此展示其具体实现。 总结在完成上述工作之后，我们的文件系统基本已经完成了，这个系统采用线性（区别于minixi二级索引用树来管理）的方式管理磁盘空间，支持基本的增删改查文件操作，支持文件权限，支持多用户。]]></content>
  </entry>
  <entry>
    <title><![CDATA[编写文件系统之格式化磁盘]]></title>
    <url>%2FLinux%2F%E7%BC%96%E5%86%99%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A0%BC%E5%BC%8F%E5%8C%96%E7%A3%81%E7%9B%98%2F</url>
    <content type="text"><![CDATA[为了加深对Linux操作系统中文件系统的理解，本篇文章将详细介绍编写一个简单文件系统的第一步，如何实现磁盘的格式化。作者能力有限，仅作零基础入门学习的实验，所以相关概念也会尽量解释到详细。好了，那就从准备工作开始吧 准备工作准备工作主要对文件系统的概念，格式化的概念，磁盘的准备，文件系统逻辑结构设计，重要数据结构进行介绍。 文件系统的概念文件系统是操作系统向用户提供一套存取数据的抽象数据结构，方便用户管理一组数据。文件系统在Linux操作系统中的位置在下图红框中标出，如Ext2、Ext4等。而在windows中现在常用的文件系统为NTFS、exFAT等，想必大家在格式化U盘、硬盘的时候就经常见到了。 为什么要用文件系统来存取数据呢？是为了图个方便。试想如果没有文件系统，放置在存储介质（硬盘）中的数据将是一个庞大的数据主体，无法分辨一个数据从哪里停止，下一个数据又从哪里开始。我们在田地里面种菜，还需要划分出几畦几垄。放到磁盘上也是一样的，通过将数据分为一块一块的，并为每一块都赋予一个名字，数据将会很容易隔离和确定。当然这都是在逻辑上去划分，并不是像对土地那样，去把磁盘切开，聚拢成堆。既然是在逻辑上划分，那总得有个依据，将划分的结果落实下来，不能凭空的，今天我这样划分，明儿个他那样划分，就乱套了。这时候我们就需要创建一系列的数据结构（包含数据和对此数据的一系列操作），来表示我们划分的逻辑。 格式化的概念如果还是很模糊，我们就离了菜地，再来换个例子。如果将硬盘比作一张空白A4纸，那么将其格式化为某种文件系统（如下图所示，格式化的时候会让我们选择一个文件系统），就类似在纸上面印上田字格，页码，为其加上目录，用来练习书法；有的人想练习英文，那就印上四线三格。这样一张纸根据不同的用途，就有了不同的结构划分。言而总之，通过定义一系列数据结构，用于划分硬盘存储的结构和管理数据，就是文件系统。 。 我们顺便把格式化也给说完了，如果还不清楚，后面还会有介绍。 磁盘的准备因为我们是实验是编写简单文件系统，为了方便和简化，我们将创建一个名字为image的文件，来代替真正的物理磁盘。我们对该文件进行格式化，查看文件的内容，来检验我们的成果。下面我们将使用linux中dd命令来创建一个指定块大小的文件。我们的image包含100个块，并且每个块的大小为4096字节，全部用0填充。 1dd bs=4096 count=100 if=/dev/zero of=image dd命令：用指定大小的块拷贝一个文件，并在拷贝的同时进行指定的转换。 if=文件名：输入文件名，缺省为标准输入。即指定源文件。 of=文件名：输出文件名，缺省为标准输出。 count=blocks：只拷贝blocks个块，即拷贝块的个数。 bs=bytes：同时设置读入/输出的块大小为bytes个字节。 /dev/zero 是类 Unix 系统中一个特殊的文件，当读取该文件时，它会提供无限的空字符 null或者0。 可以使用hexdump命令，查看image里面的数据全都为0。这就是一个未格式化磁盘的样子。它的地址从0x00000000-0x00064000(409600)，也就是文件大小为409600 byte。 szp@szp-pc:~/code/myfs$ dd bs=4096 count=100 if=/dev/zero of=image 记录了100+0 的读入 记录了100+0 的写出 409600 bytes (410 kB, 400 KiB) copied, 0.00192153 s, 213 MB/s szp@szp-pc:~/code/myfs$ hexdump -C image 00000000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| * 00064000 文件系统逻辑结构设计磁盘准备好了，下面就应该考虑如何进行逻辑上的划分了。磁盘都是以块为单位，我们上面创建的磁盘一共包含100块，每一块的大小为4k。参考现有的文件系统，一个文件系统必须包含以下逻辑结构划分。 将100块，分为6大部分，现做简要介绍。 0-1块：引导块，用于引导操作系统启动，必须保留。 1-2块：Super Block(超级块)，包含了文件系统布局的重要信息，一旦破坏，将导致磁盘不可读。包括inode节点的个数、磁盘块数以及空闲块链表的起始位置。 2-3块：block map(块位图)，记录磁盘中所有块的使用情况，即这100个块哪个是被使用了，哪个没有被使用。 3-4块：inode map(inode位图)，与块位图同理，存储的是inode节点的占用情况。 4-10块：inode table（inode节点），每个inode节点表示一个确切的文件。 10-99块：data block(数据块)，存放文件具体内容的块。 inode用于描述一个具体的文件的元信息，例如该文件的创建时间，是目录还是具体文件等等。需要注意的是它并不存储文件内部具体的数据，文件中具体数据是存放在数据块（data block）中的，那我们怎么知道当前inode对应那几个数据块呢，毕竟我们这里数据块就有89个。inode中设置一个block数组，block数组存储着每个块的索引，用于定位文件。 位图或位向量（块位图和inode位图）是一系列位或位的集合，其中每个位对应一个磁盘块，该位可以采用两个值: 0和1, 1表示已分配该块，而0表示一个空闲块。如1111 0000表示前四块已分配，后四块空闲。 uint64_t block[MISER_N_BLOCKS]; MISER_N_BLOCKS设置为10，也就是一个文件最大也就能用10个块来存储，因为一个块的大小为4k，那么一个文件最大也就40k。这也太小了，真正的文件系统中inode会有多级索引，可以支持更多的块索引，我们这里只用10个一级索引，所以小了一些。 重要数据结构磁盘逻辑结构既然划分好了，就要定义一系列数据结构来进行表示、存储和操作。 super block超级块代表了整个文件系统，超级块是文件系统的控制块，有整个文件系统信息。 12345678910111213141516171819202122232425struct MISER_fs_super_block &#123; //版本 uint64_t version; //区别于其他文件系统的标识 uint64_t magic; //块大小 uint64_t block_size; //inode节点总数 uint64_t inodes_count; //空闲块数 uint64_t free_blocks; //总块数 uint64_t blocks_count; //bmap开始的块索引 uint64_t bmap_block; //imap开始的块索引 uint64_t imap_block; //inode开始的块索引 uint64_t inode_table_block; //data block开始的块索引 uint64_t data_block_number; //填充，让super block结构体大小正好占满一个块（4096 byte），方便计算 char padding[4016];&#125;; 超级块中的char padding[4016]，是为了使超级块的大小为4096bytes，以简化计算； magic为1314522，用以区别其他文件系统； inodes_count记录文件系统所支持的inode个数，在格式化的时候写入。 bmap_block记录着bmap开始的块索引，我们这里根据上面的逻辑划分，取值为2。 imap_block记录imap开始的块索引，取值为3。 inode_table_block记录inode开始的块索引，取值为4。 data_block_number记录数据部分开始的块索引，这里为10，记录索引是为了简化文件块的定位操作。 MISER_inodeMISER_inode对应磁盘中的inode。 123456789101112131415161718struct MISER_inode &#123; mode_t mode;//4byte uint64_t inode_no; uint64_t blocks; uint64_t block[MISER_N_BLOCKS]; union &#123; uint64_t file_size; uint64_t dir_children_count; &#125;; int32_t i_uid; int32_t i_gid; int32_t i_nlink; int64_t i_atime; int64_t i_mtime; int64_t i_ctime; char padding[112];&#125;; mode表示当前文件是目录（目录也是文件）还是一个常规文件 inode_no表示节点号码 blocks表示该inode自身所占块的数目 block[MISER_N_BLOCKS]表示文件中数据存储位置的数据块索引 file_size如果该inode是常规文件，表示文件大小 dir_children_count如果该inode是目录，表示子目录数 i_uid、i_gid表示用户id，组id 目录项文件只有打开后才能够被读取。在文件打开后，操作系统会使用用户提供的路径名来定位磁盘中的目录。目录项提供了查找文件磁盘块所需要的信息。如果使用相对路径，则从当前进程的当前目录开始查找，否则就从根目录开始。在以上两种情况中，第一个目录的i节点很容易定位:在进程描述符中有指向它的指针，或者在使用根目录的情况下，它存储在磁盘上预定的块上。 123456struct MISER_dir_record&#123; char filename[MISER_FILENAME_MAX_LEN]; uint64_t inode_no;&#125;; filename字符数组，存储文件的名称，MISER_FILENAME_MAX_LEN是256，文件名称最长为256 inode_no存储对应的inode节点号码 功能实现为了实现我们设计的文件系统，我们需要创建一个mkfs格式化程序，使用该程序，将image磁盘改写成我们划分好的结构。mkfs程序依次向image写入超级块（引导块保留）、block map、inode map、inode，在data block创建一个根目录，根目录中创建一个测试文件。 我们将按照如下顺序依次写入文件： 123456789101112 //初始化superblock init_disk(fd, argv[1]); //写入引导块write_dummy(fd); //写入超级块write_sb(fd); //写入块位图write_bmap(fd);//写入inode位图write_imap(fd);//创建根目录，测试文件write_itable(fd); 初始化超级块和块位图超级块包含了文件系统的基本信息，其信息在上文中有详细描述。写入超级块信息，需要计算整个磁盘的大小，然后计算imap，bmap以及inode table的大小，这样才能确定各个区域在磁盘中的位置。这些工作都是在init_disk这个函数中完成的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061static int init_disk(int fd, const char* path)&#123; disk_size = get_file_size(path); if (disk_size == -1) &#123; perror("Error: can not get disk size!\n"); return -1; &#125; printf("Disk size id %lu\n", disk_size); super_block.version = 1; super_block.block_size = MISER_BLOCKSIZE; super_block.magic = MAGIC_NUM; super_block.blocks_count = disk_size/MISER_BLOCKSIZE; printf("blocks count is %llu\n", super_block.blocks_count); super_block.inodes_count = super_block.blocks_count; super_block.free_blocks = 0; //计算bmap，8*MISER_BLOCKSIZE表示1B=8位，4kb=4096*8=32768位二进制，因为是位图，用一位表示一个数据块，一个4k的块存储位图，共可以表示32768个数据块。 //super_block.blocks_count/(8*MISER_BLOCKSIZE)表示，该磁盘总的块数整除用一个块存储该磁盘位图可以表示的磁盘块数，就可以得到该磁盘应该用多少个块来存放块位图， //这里bmap_size等于0，表示存储磁盘块位图，一块磁盘都用不了，因为我们的磁盘一共100块，而用一个4k的块来存储位图就可以表示32768块，所以表示100块绰绰有余。 bmap_size = super_block.blocks_count/(8*MISER_BLOCKSIZE); super_block.bmap_block = RESERVE_BLOCKS;//当计算出存储位图的块数不是整数时，要对块位图块数做+1操作，以免不够用。 if (super_block.blocks_count%(8*MISER_BLOCKSIZE) != 0) &#123; bmap_size += 1; &#125; bmap = (uint8_t *)malloc(bmap_size*MISER_BLOCKSIZE); //将申请的4k大小的bmap初始化为0 memset(bmap,0,bmap_size*MISER_BLOCKSIZE); //计算imap imap_size = super_block.inodes_count/(8*MISER_BLOCKSIZE); super_block.imap_block = super_block.bmap_block + bmap_size; if(super_block.inodes_count%(8*MISER_BLOCKSIZE) != 0) &#123; imap_size += 1; &#125; imap = (uint8_t *)malloc(imap_size*MISER_BLOCKSIZE); memset(imap,0,imap_size*MISER_BLOCKSIZE); //计算inode_table，表示需要多少个数据块来存储,(MISER_BLOCKSIZE/MISER_INODE_SIZE)代表一个块能够存放几个inode,inodes_count代表inode总数 inode_table_size = super_block.inodes_count/(MISER_BLOCKSIZE/MISER_INODE_SIZE); //计算inode_table的数据块索引 super_block.inode_table_block = super_block.imap_block + imap_size; //计算数据块的块索引 super_block.data_block_number = RESERVE_BLOCKS + bmap_size + imap_size + inode_table_size; //计算空闲块数 super_block.free_blocks = super_block.blocks_count - super_block.data_block_number - 1; //设置bmap int idx; // plus one becase of the root dir for (idx = 0; idx &lt; super_block.data_block_number + 1; ++idx) &#123; if (set_bmap(idx, 1)) &#123; return -1; &#125; &#125; return 0;&#125; 主要流程为，获取磁盘大小（image文件），计算bmap，imap的大小，需要用几个块存储；计算inode总共占用几个块，分别计算出6个部分逻辑划分的起始块索引。计算出了磁盘元数据占用的块数，就可以使用set_bmap修改bmap了。根据逻辑划分可以知道，引导块，超级块，bmap，imap,inode总共使用了10个块，然后我们还需要创建一个根目录和一个文件，会占用一个data block。因此image中前11个块都会被占用。set_bmap函数执行完成后，使用gdb单步调试可以看到bmap数组前11位被置为1。该数组会在后面直接写入到文件。 设置bmap函数如下： 123456789101112131415161718192021222324static int set_bmap(uint64_t idx, int value)&#123; if(!bmap) &#123; return -1; &#125; //sizeof(char)的值为1，bmap数组中的元素类型为8位无符号整型，因为磁盘的前11块已经被使用，所以这里将数组的前11位置为1 //因为数组把每八位二进制作为一个元素，这里对8整除得到数组元素的索引 uint64_t array_idx = idx/(sizeof(char)*8); //对8取余，得到在对应数组元素中的偏移量 uint64_t off = idx%(sizeof(char)*8); //数组下标超过了位图的数据大小 if(array_idx &gt; bmap_size*MISER_BLOCKSIZE) &#123; printf("Set bmap error and idx is %llu\n", idx); return -1; &#125; if(value)//对应块位图置1 bmap[array_idx] |= (1&lt;&lt;off); else//对应块位图置0 bmap[array_idx] &amp;= ~(1&lt;&lt;off); return 0;&#125; 写入引导块初始化超级块（struct MISER_fs_super_block super_block）和块位图(uint8_t* bmap)以后，首先写入第一个块，也就是引导块的数据。引导块全部置0。 1234567891011static int write_dummy(int fd)&#123; char dummy[MISER_BLOCKSIZE] = &#123;0&#125;; ssize_t res = write(fd, dummy, MISER_BLOCKSIZE); if (res != MISER_BLOCKSIZE) &#123; perror("write_dummy error!"); return -1; &#125; return 0;&#125; 使用hexdump命令查看写入的情况，是否正确。第0块，0x00000000-0x00001000，其中的数据全部为0，*省略了重复的内容。 szp@szp-pc:~/code/myfs$ hexdump -C image 00000000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| * 00001000 01 00 00 00 00 00 00 00 da 0e 14 00 00 00 00 00 |................| 写入超级块紧接着在引导块之后，写入刚才初始化好的超级块。 123456789101112static int write_sb(int fd) &#123; ssize_t ret; ret = write(fd, &amp;super_block, sizeof(super_block)); if(ret != MISER_BLOCKSIZE) &#123; perror("Write super block error!\n"); return -1; &#125; printf("Super block written succesfully!\n"); return 0;&#125; 第1块，0x00001000-0x00002000，可以看到版本号占8个字节为值为01，0x140eda代表magic，转换为十进制值为1314522，往下依次类推，都符合我们的设置。 00001000 01 00 00 00 00 00 00 00 da 0e 14 00 00 00 00 00 |................| 00001010 00 10 00 00 00 00 00 00 64 00 00 00 00 00 00 00 |........d.......| 00001020 59 00 00 00 00 00 00 00 64 00 00 00 00 00 00 00 |Y.......d.......| 00001030 02 00 00 00 00 00 00 00 03 00 00 00 00 00 00 00 |................| 00001040 04 00 00 00 00 00 00 00 0a 00 00 00 00 00 00 00 |................| 00001050 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| * 00002000 ff 07 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| 写入块位图块位图即上面我们构建好的bmap数据，依次写入。 12345678910111213static int write_bmap(int fd) &#123; ssize_t ret = -1; ret = write(fd, bmap, bmap_size*MISER_BLOCKSIZE); if (ret != bmap_size*MISER_BLOCKSIZE) &#123; perror("Write_bmap() error!\n"); return -1; &#125; return 0;&#125; 写入后的数据为0xff07，二进制为11111111 111，与bmap数组中的数据相同。 00002000 ff 07 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| 00002010 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| * 写入inode位图根目录需要三个目录项，一个是代表当前目录的‘.’，一个是代表父目录的‘..’，还有一个是我们创建的测试文件。所以需要使用三个inode，在inode位图上对应位，设置前三个位为占用。 1234567891011121314static int write_imap(int fd)&#123; memset(imap, 0, imap_size*MISER_BLOCKSIZE); //根目录需要一个inode,测试文件需要一个inode，所以写入两个inode被占用。 imap[0] |= 0x3; ssize_t res = write(fd, imap, imap_size*MISER_BLOCKSIZE); if (res != imap_size*MISER_BLOCKSIZE) &#123; perror("write_imap() erroe!"); return -1; &#125; return 0;&#125; 如下图0x03，二进制为11，表示inode table中前两个inode已被使用。 00003000 03 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| 00003010 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |................| * 创建根目录创建两个inode，一个表示根目录文件，一个表示测试文件。依次写入到inode table中。然后在data block中写入三个目录项。并与前面创建的两个inode，通过其中的block数组进行关联。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586static int write_itable(int fd)&#123; uint32_t _uid = getuid(); uint32_t _gid = getgid(); //初始化根目录的inode ssize_t ret; struct MISER_inode root_dir_inode; //设置该inode是目录还是文件 root_dir_inode.mode = S_IFDIR; //节点号 root_dir_inode.inode_no = MISER_ROOT_INODE_NUM; //inode所占块数 root_dir_inode.blocks = 1; //block数组存储着每个块的索引，用于定位文件数据块。最大10个块，所以单个文件最大为40KB root_dir_inode.block[0] = super_block.data_block_number; //三个子文件或目录 root_dir_inode.dir_children_count = 3; root_dir_inode.i_gid = _gid; root_dir_inode.i_uid = _uid; root_dir_inode.i_nlink = 2; root_dir_inode.i_atime = root_dir_inode.i_mtime = root_dir_inode.i_ctime = ((int64_t)time(NULL)); ret = write(fd, &amp;root_dir_inode, sizeof(root_dir_inode)); if (ret != sizeof(root_dir_inode)) &#123; perror("write_itable error!\n"); return -1; &#125; //创建根目录下file测试文件的inode节点 struct MISER_inode onefile_inode; onefile_inode.mode = S_IFREG; onefile_inode.inode_no = 1; onefile_inode.blocks = 0; onefile_inode.block[0] = 0; onefile_inode.file_size = 0; onefile_inode.i_gid = _gid; onefile_inode.i_uid = _uid; onefile_inode.i_nlink = 1; onefile_inode.i_atime = onefile_inode.i_mtime = onefile_inode.i_ctime = ((int64_t)time(NULL)); ret = write(fd, &amp;onefile_inode, sizeof(onefile_inode)); if (ret != sizeof(onefile_inode)) &#123; perror("write_itable error!\n"); return -1; &#125;//创建当前目录项 struct MISER_dir_record root_dir_c; const char* cur_dir = "."; const char* parent_dir = ".."; memcpy(root_dir_c.filename, cur_dir, strlen(cur_dir) + 1); root_dir_c.inode_no = MISER_ROOT_INODE_NUM; //创建父目录项 struct MISER_dir_record root_dir_p; memcpy(root_dir_p.filename, parent_dir, strlen(parent_dir) + 1); root_dir_p.inode_no = MISER_ROOT_INODE_NUM;//创建file文件目录项 struct MISER_dir_record file_record; const char* onefile = "file"; memcpy(file_record.filename, onefile, strlen(onefile) + 1); file_record.inode_no = 1;//从当前文件读写位置增加0个偏移量 off_t current_off = lseek(fd, 0L, SEEK_CUR); printf("Current seek is %lu and rootdir at %lu\n", current_off , super_block.data_block_number*MISER_BLOCKSIZE);//设置新的读写位置为super_block.data_block_number*MISER_BLOCKSIZE，即10*4096byte if(-1 == lseek(fd, super_block.data_block_number*MISER_BLOCKSIZE, SEEK_SET)) &#123; perror("lseek error\n"); return -1; &#125; ret = write(fd, &amp;root_dir_c, sizeof(root_dir_c)); ret = write(fd, &amp;root_dir_p, sizeof(root_dir_p)); ret = write(fd, &amp;file_record, sizeof(file_record)); if (ret != sizeof(root_dir_c)) &#123; perror("Write error!\n"); return -1; &#125; printf("Create root dir successfully!\n"); return 0;&#125; 对于根目录来讲，写入的数据为三个目录项，目录项的内容为文件（目录）名以及对应的inode编号。第一个目录项为当前目录和对应的inode编号0，第二个目录项为上一级目录和对应的inode编号0，第三个目录项为欢迎文件，内容为文件名“file”和对应的inode编号1。 编译执行编写完成后，就可以进行编译执行。 123gcc mkfs.c -o mkfs./mkfs ./image 至此，磁盘image中就格式化了我们设计的文件系统，接下来就该实现文件系统的挂载与卸载。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>文件系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IO模型]]></title>
    <url>%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2FIO%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[同步IO缓冲IO与非缓冲IO这个区别是在于调用write和read的api是调用的是标准库的库函数，还是调用的操作系统层面的api。用非缓冲I/O函数每次读写都要进内核，调一个系统调用比调一个用户空间的函数要慢很多，所以在用户空间开辟I/O缓冲区还是必要的，用C标准I/O库函数就比较方便，省去了自己管理I/O缓冲区的麻烦。用C标准I/O库函数要时刻注意I/O缓冲区和实际文件有可能不一致，在必要时需调用fflush()。 直接IO与间接IO直接 I/O，是指跳过操作系统的页缓存，直接跟文件系统交互来访问文件。非直接 I/O 正好相反，文件读写时，先要经过系统的页缓存，然后再由内核或额外的系统调用，真正写入磁盘。想要实现直接 I/O，需要你在系统调用中，指定 O_DIRECT 标志。如果没有设置过，默认的是非直接 I/O。 page cache文件系统中页缓存，以页为单位，通常包含多个物理上不连续的磁盘块，缓存文件的逻辑内容，加速对文件内容的访问，缓存inode，相关结构体struct address_space。 buffer cache缓冲区缓冲对一个磁盘块进行缓存，减少程序多次访问同一磁盘块的时间。相关结构体struct buffer_head。 阻塞IO和非阻塞IO阻塞 I/O，是指应用程序执行 I/O 操作后，如果没有获得响应，就会阻塞当前线程，自然就不能执行其他任务。非阻塞 I/O，是指应用程序执行 I/O 操作后，不会阻塞当前的线程，可以继续执行其他的任务，随后再通过轮询或者事件通知的形式，获取调用的结果。 采用轮询方式的非阻塞IO 进程轮询（重复）调用，消耗CPU的资源，所以又有了事件通知的形式。 当进程发起一个IO操作，会向内核注册一个信号处理函数，然后进程返回不阻塞；当内核数据就绪时会发送一个信号给进程，进程便在信号处理函数中调用IO读取数据。也就是在数据到达内核缓冲这段时间，进程是不需要阻塞的，也是异步的，但是收到数据准备好的事件通知后，进程需要主动发起将数据从内核拷贝到用户空间的操作，这段过程还是需要等待的，是同步的。 异步IO当进程发起一个IO操作，进程返回（不阻塞），但也不能返回结果；内核把整个IO处理完后（包括数据拷贝到用户空间），会通知进程结果。如果IO操作成功则进程直接获取到数据。 用户进程发起aio_read操作之后，给内核传递描述符、缓冲区指针、缓冲区大小等，告诉内核当整个操作完成时，如何通知进程，然后就立刻去做其他事情了。当内核收到aio_read后，会立刻返回，然后内核开始等待数据准备，数据准备好以后，直接把数据拷贝到用户控件，然后再通知进程本次IO已经完成。 事件通知的方式难道不是异步的么？ 事件通知，内核是在数据准备好之后通知进程，然后进程再通过recvfrom操作进行数据拷贝。我们可以认为数据准备阶段是异步的，但是，数据拷贝操作是同步的。所以，整个IO过程也不能认为是异步的。 IO复用模型-生产常用 多个的进程的IO可以注册到一个复用器（select）上，然后用一个进程调用该select， select会监听所有注册进来的IO；如果select没有监听的IO在内核缓冲区都没有可读数据，select调用进程会被阻塞；而当任一IO在内核缓冲区中有可数据时，select调用就会返回；而后select调用进程可以自己或通知另外的进程（注册进程）来再次发起读取IO，读取内核中准备好的数据。可以看到，多个进程注册IO后，只有另一个select调用进程被阻塞。 典型应用：Linux中的select、poll、epoll三种方案，Java NIO;]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IO吞吐量与IO延迟]]></title>
    <url>%2FLinux%2FIO%E5%90%9E%E5%90%90%E9%87%8F%E4%B8%8EIO%E5%BB%B6%E8%BF%9F%2F</url>
    <content type="text"><![CDATA[什么是I/O？I/O 的概念，从字义来理解就是输入输出。操作系统从上层到底层，各个层次之间均存在 I/O。比如，CPU 有 I/O，内存有 I/O, VMM 有 I/O, 底层磁盘上也有 I/O，这是广义上的 I/O。通常来讲，一个上层的 I/O 可能会产生针对磁盘的多个 I/O，也就是说，上层的 I/O 是稀疏的，下层的 I/O 是密集的。我们通常所说的IO都是指磁盘等设备IO。 什么是IO设备？I/O设备一般包括机械部件和电子部件两个部分:电子部件-设备控制器（适配器）和机械部件-设备本身。控制器是插在电路板上的一块芯片或一组芯片，这块电路板物理地控制设备。它从操作系统接收命令，例如，从设备读数据，并且完成数据的处理。 I/O设备的另一个部分是实际设备的自身。设备本身有个相对简单的接口，这是因为接口既不能做很多工作，又已经被标准化了。例如，标准化后任何一个SATA磁盘控制器就可以适配任一种SATA磁盘。 现在SATA是很多计算机的标准硬盘接口。由于实际的设备接口隐藏在控制器中，所以，操作系统看到的是对控制器的接口，这个接口可能和设备接口有很大的差别。 每类设备控制器都是不同的，所以，需要不同的软件进行控制。专门与控制器对话，发出命令并接收响应的软件，称为设备驱动程序(device driver)。每个控制器厂家必须为所支持的操作系统提供相应的设备驱动程序。例如，一台扫描仪会配有用于OSX、Windows 7、Windows 8以及Linux的设备驱动程序。 为了能够使用设备驱动程序，必须把设备驱动程序装人操作系统中，这样它可在核心态运行。 I/O设备常见分类字符设备字符设备指能够像字节流串行顺序依次进行访问的设备，对它的读写是以字节为单位。字符设备的上层没有磁盘文件系统，所以字符设备的file_operations成员函数就直接由字符设备驱动提供（一般字符设备都会实现相应的fops集），因此file_operations 也就成为了字符设备驱动的核心。 特点： 一个字节一个字节读写的设备 读取数据需要按照先后数据（顺序读取） 每个字符设备在/dev目录下对应一个设备文件，linux用户程序通过设备文件（或称设备节点）来使用驱动程序操作字符设备。 常见的字符设备有鼠标、键盘、串口、控制台等 块设备块设备以数据块的形式存放数据，如NAND Flash以页为单位存储数据，并采用mount方式挂载块设备。 块设备必须能够随机存取(random access)，字符设备则没有这个要求。 块设备除了给内核提供和字符设备一样的接口外，还提供了专门面向块设备的接口，块设备的接口必须支持挂装文件系统，通过此接口，块设备能够容纳文件系统，因此应用程序一般通过文件系统来访问块设备上的内容，而不是直接和设备打交道。 对于块设备而言，上层ext2,jiffs2,fat等文件系统会 实现针对VFS的file_opertations成员函数，所以设备驱动层将看不到file_opeations的存在。磁盘文件系统和设备驱动会将对磁盘上文件的访问转换成对磁盘上柱面和扇区的访问。 特点： 数据以固定长度进行传输，比如512K 从设备的任意位置（可跳）读取，但实际上，块设备会读一定长度的内容，而只返回用户要求访问的内容，所以随机访问实际上还是读了全部内容。 块设备包括硬盘、磁盘、U盘和SD卡等 每个块设备在/dev目录下对应一个设备文件，linux用户程序可以通过设备文件（或称设备节点）来使用驱动程序操作块设备。 块设备可以容纳文件系统，所以一般都通过文件系统来访问，而不是/dev设备节点。 网络设备虽然在Linux系统存在一句话叫一切皆文件，无论是各种文本文件还是具体的硬件设备（硬件由设备文件来实现相应）。但是网络设备在Linux内核中却是唯一不体现一切皆设备思想的驱动架构，因为网络设备使用套接字来实现网数据的接受和发送。 网络设备驱动不同于字符设备和块设备，不在/dev下以文件节点代表，而是通过单独的网络接口来代表。 特点： 网络接口没有像字符设备和块设备一样的设备号和/dev设备节点，只有接口名，如eth0,eth1 通过socket操作，而不是open read write I/O子系统架构 上图概括了一次磁盘 write 操作的过程，假设文件已经被从磁盘中读入了 page cache 中 一个用户进程通过 write() 系统调用发起写请求 内核更新对应的 page cache pdflush 内核线程将 page cache 写入至磁盘中 文件系统层将每一个 block buffer 存放为一个 bio 结构体，并向块设备层提交一个写请求 块设备层从上层接受到请求，执行 IO 调度操作，并将请求放入IO 请求队列中 设备驱动（如 SCSI 或其他设备驱动）完成写操作 磁盘设备固件执行对应的硬件操作，如磁盘的旋转，寻道等，数据被写入到磁盘扇区中 Block LayerBlock layer 处理所有和块设备相关的操作。block layer 最关键是数据结构是 bio 结构体。bio 结构体是 file system layer 到 block layer 的接口。 当执行一个写操作时，文件系统层将数据写入 page cache（由 block buffer 组成），将连续的块放到一起，组成 bio 结构体，然后将 bio 送至 block layer。 block layer 处理 bio 请求，并将这些请求链接成一个队列，称作 IO 请求队列，这个连接的操作就称作 IO 调度（也叫 IO elevator 即电梯算法）. IO schedulerIO 调度器的总体目标是减少磁盘的寻道时间（因此调度器都是针对机械硬盘进行优化的），IO 调度器通过两种方式来减少磁盘寻道：合并和排序。 合并即当两个或多个 IO 请求的是相邻的磁盘扇区，那么就将这些请求合并为一个请求。通过合并请求，多个 IO 请求只需要向磁盘发送一个请求指令，减少了磁盘的开销。 排序就是将不能合并的 IO 请求，根据请求磁盘扇区的顺序，在请求队列中进行排序，使得磁头可以按照磁盘的旋转顺序的完成 IO 操作，可以减小磁盘的寻道次数。 调度器的算法和电梯运行的策略相似，因此 IO 调度器也被称作 IO 电梯( IO Elevator )。由于对请求进行了重排，一部分的请求可能会被延迟，以提升整体的性能。 Linux 2.4 只使用了一种通用的 IO 算法。到 Linux 2.6 实现了 4 种 IO 调度模型，其中 anticipatory 在 2.6.33 中被移除。 读文件函数调用流程以经过页缓存读取一个ext2文件系统上的普通文件的流程为例，看一看IO子系统的各个层级都实现了什么功能。IO系统架构分层如下： 首先是应用程序发起系统调用，进入虚拟文件系统层。 dentry ： 联系了文件名和文件的 i 节点 inode ： 文件 i 节点，保存文件标识、权限和内容等信息 file ： 保存文件的相关信息和各种操作文件的函数指针集合 file_operations ：操作文件的函数接口集合 address_space ：描述文件的 page cache 结构以及相关信息，并包含有操作 page cache 的函数指针集合 address_space_operations ：操作 page cache 的函数接口集合 bio ： IO 请求的描述 I/O性能评价指标IOPS每秒的输入输出量(或读写次数)，也就是在一秒内，磁盘进行多少次 I/O 读写。是衡量磁盘性能的主要指标之一。 吞吐量指单位时间内可以成功传输的数据数量。即磁盘写入加上读出的数据的大小。吞吐量等于IOPS乘以每次IO大小。 使用率使用率，是指磁盘处理I/O的时间百分比，也就是一个时间段内磁盘用于处理IO的时间占这段时间的比例。过高的使用率(比如超过80% ) , 通常意味着磁盘I/O存在性能瓶颈。 饱和度饱和度，是指磁盘处理I/O的繁忙程度，也就是能否接受新的IO请求。过高的饱和度,意味着磁盘存在严重的性能瓶颈。当饱和度为100%时,磁盘无法接受新的I/O请求。 响应时间响应时间,是指I/O请求从发出到收到响应的间隔时间。 性能监测工具提供的指标 性能工具 性能指标 iostat 磁盘I/O使用率、IOPS、 吞吐量、响应时间、I/O平均大小以及等待队列长度 pidstat 进程I/O大小以及I/O延迟 sar 磁盘I/O使用率、IOPS 、吞吐量以及响应时间 dstat 磁盘I/O使用率、IOPS以及吞吐量 iotop 按I/O大小对进程排序 slabtop 目录项、索引节点以及文件系统的缓存 /proc/slabinfo 目录项、索引节点以及文件系统的缓存 /proc/meminfo 页缓存和可回收Slab缓存 /proc/diskstats 磁盘的IOPS、吞吐量以及延迟! /proc/pid/io 进程IOPS、IO大小以及IO延迟 vmstat 缓存和缓冲区用量汇总 blktrace 跟踪块设备I/O事件 biosnoop 跟踪进程的块设备I/O大小 biotop 跟踪进程块I/O并按I/O大小排序 strace 跟踪进程的I/O系统调用 perf 跟踪内核中的I/O事件 df 磁盘空间和索引节点使用量和剩余量 mount 文件系统的挂载路径以及挂载参数 du 目录占用的磁盘空间大小 tune2fs 显示和设置文件系统参数 hdparam 显示和设置磁盘参数 磁盘I/Oiostat是I/O statistics（输入/输出统计）的缩写，iostat工具将对系统的磁盘操作活动进行监视。它的特点是汇报磁盘活动统计情况，同时也会汇报出CPU使用情况。这些指标实际上来自 /proc/diskstats。 szp@szp-pc:~$ iostat -d -x Linux 5.4.0-42-generic (szp-pc) 2020年08月01日 _x86_64_ (4 CPU) Device r/s w/s rkB/s wkB/s rrqm/s wrqm/s %rrqm %wrqm r_await w_await aqu-sz rareq-sz wareq-sz svctm %util loop0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.80 0.00 0.00 2.35 0.00 1.40 0.00 loop1 0.01 0.00 0.05 0.00 0.00 0.00 0.00 0.00 0.49 0.00 0.00 4.46 0.00 0.41 0.00 loop2 0.00 0.00 0.04 0.00 0.00 0.00 0.00 0.00 0.65 0.00 0.00 10.64 0.00 1.14 0.00 loop3 0.05 0.00 0.08 0.00 0.00 0.00 0.00 0.00 2.17 0.00 0.00 1.65 0.00 0.23 0.00 loop4 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.49 0.00 0.00 8.00 0.00 1.66 0.00 loop5 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.40 0.00 0.00 2.47 0.00 1.02 0.00 loop6 0.01 0.00 0.02 0.00 0.00 0.00 0.00 0.00 0.22 0.00 0.00 2.44 0.00 0.49 0.00 loop7 0.01 0.00 0.02 0.00 0.00 0.00 0.00 0.00 0.28 0.00 0.00 2.34 0.00 0.59 以上指标的含义如下： 性能指标 含义 提示 r/s 每秒发送给磁盘的读请求数 合并后的请求数 w/s 每秒发送给磁盘的写请求数 合并后的请求数 rkB/s 每秒从磁盘读取的数据量 单位为kB wkB/s 每秒向磁盘写入的数据量 单位为kB rrqm/s 每秒合并的读请求数 %rrqm表示合并读请求的百分比 wrqm/s 每秒合并的写请求数 %wrqm表示合并写请求的百分比 r_await 读请求处理完成等待时间 包括队列中的等待时间和设备实际处理的时间，单位为毫秒 w_await 写请求处理完成等待时间 包括队列中的等待时间和设备实际处理的时间，单位为毫秒 aqu-sz 平均请求队列长度 旧版中为avgqu-sz rareq-sz 平均读请求大小 单位为kB wareq-sz 平均写请求大小 单位为kB svctm 处理I/O请求所需的平均时间(不包括等待时间) 单位为毫秒。注意这是推断的数据，并不保证完全准确 %util 磁盘处理I/O的时间百分比 即使用率，由于可能存在并行I/O，100%并不一定表明磁盘I/O饱和 关于%util，举个简单的例子，某硬盘处理单个I/O需要0.1秒，有能力同时处理10个I/O请求，那么当10个I/O请求依次顺序提交的时候，需要1秒才能全部完成，在1秒的采样周期里%util达到100%；而如果10个I/O请求一次性提交的话，0.1秒就全部完成，在1秒的采样周期里%util只有10%。可见，即使%util高达100%，硬盘也仍然有可能还有余力处理更多的I/O请求，即没有达到饱和状态。 %util，就是磁盘 I/O 使用率； r/s+ w/s ，就是磁盘IOPS； rkB/s+wkB/s ，就是磁盘吞吐量； r_await，w_await ，就是磁盘响应时间。 进程I/Oiostat只提供磁盘整体的I/O性能数据，缺点在于 ,并不能知道具体是哪些进程在进行磁盘读写。要观察进程的I/O情况,你还可以使用pidstat和iotop这两个工具。 szp@szp-pc:~$ pidstat -d Linux 5.4.0-42-generic (szp-pc) 2020年08月01日 _x86_64_ (4 CPU) 16时26分59秒 UID PID kB_rd/s kB_wr/s kB_ccwr/s iodelay Command 16时26分59秒 0 1 -1.00 -1.00 -1.00 25 systemd 16时26分59秒 0 395 -1.00 -1.00 -1.00 3244 jbd2/sda1-8 16时26分59秒 0 446 -1.00 -1.00 -1.00 46 systemd-journal 从pidstat的输出你能看到,它可以实时查看每个进程的I/O情况,包括下面这些内容。 123456789用户ID (UID)和进程ID( PID)。每秒读取的数据大小( kB_rd/s),单位是 KB。每秒发出的写请求数据大小( kB_wr/s )，单位是KB。每秒取消的写请求数据大小( kB_ ccwr/s ),单位是 KB。块I/O延迟(iodelay) ,包括等待同步块I/O和换入块I/O结束的时间,单位是时钟周期。 kB_rd/s+kB_wr/s = 进程吞吐量 除了可以用pidstat实时查看,iotop可以按照I/O大小对进程排序,然后找到I/O较大的那些进程。 Total DISK READ : 0.00 B/s | Total DISK WRITE : 3.36 M/s Actual DISK READ: 0.00 B/s | Actual DISK WRITE: 4.70 M/s TID PRIO USER DISK READ DISK WRITE&gt; SWAPIN IO COMMAND 4116 be/4 root 0.00 B/s 1146.13 K/s 0.00 % 2.50 % snapd 4109 be/4 root 0.00 B/s 764.09 K/s 0.00 % 0.80 % snapd 4117 be/4 root 0.00 B/s 764.09 K/s 0.00 % 1.31 % snapd 4206 be/4 root 0.00 B/s 764.09 K/s 0.00 % 1.31 % snapd 1 be/4 root 0.00 B/s 0.00 B/s 0.00 % 0.00 % init splash 2 be/4 root 0.00 B/s 0.00 B/s 0.00 % 0.00 % [kthreadd] 上图为按照磁盘写速度进行排序的情况。 从这个输出,你可以看到,前两行分别表示,进程的磁盘读写大小总数和磁盘真实的读写大小总数。因为缓存、缓冲区、I/O 合并等因素的影响，它们可能并不相等。 剩下的部分,则是从各个角度来分别表示进程的1/O情况,包括线程ID、I/O优先级、每秒读磁盘的大小、每秒写磁盘的大小、换入和等待I/O的时钟百分比等。 IO延迟任何一个系统都有两个指标和性能紧密联系：响应时间和吞吐量。两个很关键的指标则是系统的IO延迟(响应时间)与IO吞吐量。 什么是IO延迟？1个完整的I/O传输，可能会经过以下路径：CPU -&gt;内存-&gt;硬盘细分一下如下：进程-&gt;虚拟文件系统-&gt;具体文件系统（页缓存）-&gt;通用块层（加入请求队列）-&gt;IO调度层-&gt;块设备驱动层-&gt;块设备层(操作磁盘)-&gt;磁盘 在上述I/O传输途径中消耗的时间就是IO延迟。我们习惯性的把IO的延迟分为以下几部分： 系统OS等软件配置产生的延迟：异步IO的情况下整个系统的延迟就会比同步IO下好很多。 硬件延迟：存储的延迟则是自身主控器+硬盘的延迟。存储主控上的缓存都比较大，加上存储厂家对命中率算法持续改进，以往单个硬盘因寻址、旋转产生的IO延迟时间都可以在一定比例上规避。 可以对io延迟做这样的划分：IO延迟时间=常量延迟+变量延迟 常量就是操作系统和数据库参数配置完毕后，那些硬件上的固定延迟加上一定存储命中率下软硬件共同作用产生的延迟时间，变量延迟就是io压力变大时，特别是海量随机小IO请求，导致的延迟变大。 BCC中提供的IO延迟监测工具biotopbiotop是跟踪块设备I/O事件，并按照总的传输字节大小进行排序的工具，biotop总结了哪些事件执行了磁盘I / O，并打印出在一定时间间隔内，事件发送的进程id,平均等待时间，磁盘设备号等信息。 12345678910# ./biotopTracing... Output every 1 secs. Hit Ctrl-C to end08:04:11 loadavg: 1.48 0.87 0.45 1/287 14547PID COMM D MAJ MIN DISK I/O Kbytes AVGms14501 cksum R 202 1 xvda1 361 28832 3.396961 dd R 202 1 xvda1 1628 13024 0.5913855 dd R 202 1 xvda1 1627 13016 0.59 默认情况下，屏幕每1秒刷新一次，并显示前20个磁盘消费者，按总KB排序。 打印的第一行是标题，其中包含时间，然后包含/proc/loadavg的内容。其他bcc工具可用于获取更多详细的内容，例如biolatency 和 biosnoop。 biolatency该工具跟踪块设备IO(磁盘IO)的延迟分布情况： 123456789101112131415161718192021# ./biolatencyTracing block device I/O... Hit Ctrl-C to end.^C usecs : count distribution 0 -&gt; 1 : 0 | | 2 -&gt; 3 : 0 | | 4 -&gt; 7 : 0 | | 8 -&gt; 15 : 0 | | 16 -&gt; 31 : 0 | | 32 -&gt; 63 : 0 | | 64 -&gt; 127 : 1 | | 128 -&gt; 255 : 12 |******** | 256 -&gt; 511 : 15 |********** | 512 -&gt; 1023 : 43 |******************************* | 1024 -&gt; 2047 : 52 |**************************************| 2048 -&gt; 4095 : 47 |********************************** | 4096 -&gt; 8191 : 52 |**************************************| 8192 -&gt; 16383 : 36 |************************** | 16384 -&gt; 32767 : 15 |********** | 32768 -&gt; 65535 : 2 |* | 65536 -&gt; 131071 : 2 |* | 第一列表示延迟的时间范围，单位是微秒；第二列表示有多少事件属于该延迟时间段；第三列表示直方图。 ext4dist我们通常在块设备层（ block device layer）研究存储I/O延迟问题，如上面的biolatency和biotop工具，但是对文件系统进行检测可以提供更多相关的指标来了解应用程序如何受到影响。ext4dist工具针对ext4文件系统，跟踪读取（reads），写入（writes），打开（opens）和同步（fsync），并将其延迟汇总为直方图。 12345678910111213141516171819202122232425262728293031323334353637# ext4distTracing ext4 operation latency... Hit Ctrl-C to end.^Coperation = 'read' usecs : count distribution 0 -&gt; 1 : 1210 |****************************************| 2 -&gt; 3 : 126 |**** | 4 -&gt; 7 : 376 |************ | 8 -&gt; 15 : 86 |** | 16 -&gt; 31 : 9 | | 32 -&gt; 63 : 47 |* | 64 -&gt; 127 : 6 | | 128 -&gt; 255 : 24 | | 256 -&gt; 511 : 137 |**** | 512 -&gt; 1023 : 66 |** | 1024 -&gt; 2047 : 13 | | 2048 -&gt; 4095 : 7 | | 4096 -&gt; 8191 : 13 | | 8192 -&gt; 16383 : 3 | |operation = 'write' usecs : count distribution 0 -&gt; 1 : 0 | | 2 -&gt; 3 : 0 | | 4 -&gt; 7 : 0 | | 8 -&gt; 15 : 75 |****************************************| 16 -&gt; 31 : 5 |** |operation = 'open' usecs : count distribution 0 -&gt; 1 : 1278 |****************************************| 2 -&gt; 3 : 40 |* | 4 -&gt; 7 : 4 | | 8 -&gt; 15 : 1 | | 16 -&gt; 31 : 1 | 此输出显示读取延迟的双峰分布，其中分别为延迟小于7微秒的较快事件和256至1023微秒之间的较慢事件。 计数列显示有多少事件属于该延迟范围。 较快的事件可能是内存中文件系统高速缓存命中的原因，较慢的事件是从存储设备（磁盘）中读取的。 主要代码实现： 1234567891011121314151617181920// time operation 给内核函数创建一个kprobeint trace_entry(struct pt_regs *ctx)&#123; //获取当前进程pid u32 pid = bpf_get_current_pid_tgid(); //FILTER_PID是为过滤pid设置的占位符，根据后面的python程序可知， //如果用户没有在参数里面指定pid，FILTER_PID替换为0；否则，FILTER_PID替换为“pid != 用户输入的pid” //即如果当前进程pid不是用户指定的进程pid，函数返回，不进行监测。 if (FILTER_PID) return 0; //用户没有指定pid，或者用户指定的pid等于当前进程pid，执行下面的函数获取当前的时间，以纳秒为单位。 u64 ts = bpf_ktime_get_ns(); //将当前进程pid地址作为key，以及当前时间ts地址作为value，更新保存到hash map。 start.update(&amp;pid, &amp;ts); return 0;&#125; 该函数在文件系统执行读写函数的时候，例如：ext4_file_read_iter,ext4_file_write_iter，加入kprobe，用以记录本次读写开始的时间。 123456789101112131415161718192021222324252627282930313233// 按照操作名称，统计操作执行的延迟，pt_regs保存寄存器信息和bpf上下文，op为操作名称，例如read，writestatic int trace_return(struct pt_regs *ctx, const char *op)&#123; //定义变量存储开始追踪的时间 u64 *tsp; u32 pid = bpf_get_current_pid_tgid(); // fetch timestamp and calculate delta 从hash map中取出保存的开始时间 tsp = start.lookup(&amp;pid); if (tsp == 0) &#123; return 0; // missed start or filtered 当前进程没有记录，函数返回。 &#125; //获取当前系统时间，并且计算出与开始时间的时间增量，保存到delta。 u64 delta = bpf_ktime_get_ns() - *tsp; //删除hash map中当前进程的开始监测时间。 start.delete(&amp;pid); // Skip entries with backwards time: temp workaround for #728 舍弃时间增量为负值的情况，函数返回 if ((s64) delta &lt; 0) return 0; //时间单位转换，FACTOR占位符表示进制，将纳秒转换为微妙或者毫秒 delta /= FACTOR; // store as histogram 函数bpf_log2l构造直方图索引 dist_key_t key = &#123;.slot = bpf_log2l(delta)&#125;; //操作名称保存到结构体变量 __builtin_memcpy(&amp;key.op, op, sizeof(key.op)); //根据key值累加 dist.increment(key); return 0;&#125; 该函数为在读写函数执行完成，返回的时候，加入的kprobe，结合函数开始执行时记录下的时间与当前的时间，计算出时间差，即本次读写所用的时间。 ext4slowerext4slower显示ext4的读取，写入，打开和同步操作，其速度低于阈值的所有事件。该工具与上一个工具不同之处在于，该工具提供了一个延迟时间阈值参数，可以监测到所有延迟低于某个阈值的事件，并打印输出。当然这仅能跟踪常见文件系统操作，其他文件系统操作（例如，inode操作，getattr()）不会跟踪。 1234567891011121314151617181920212223# ./ext4slowerTracing ext4 operations slower than 10 msTIME COMM PID T BYTES OFF_KB LAT(ms) FILENAME06:35:01 cron 16464 R 1249 0 16.05 common-auth06:35:01 cron 16463 R 1249 0 16.04 common-auth06:35:01 cron 16465 R 1249 0 16.03 common-auth06:35:01 cron 16465 R 4096 0 10.62 login.defs06:35:01 cron 16464 R 4096 0 10.61 login.defs06:35:01 cron 16463 R 4096 0 10.63 login.defs06:35:01 cron 16465 R 2972 0 18.52 pam_env.conf06:35:01 cron 16464 R 2972 0 18.51 pam_env.conf06:35:01 cron 16463 R 2972 0 18.49 pam_env.conf06:35:01 dumpsystemstat 16473 R 128 0 12.58 date06:35:01 debian-sa1 16474 R 283 0 12.66 sysstat06:35:01 debian-sa1 16474 R 128 0 10.39 sa106:35:01 dumpsystemstat 16491 R 128 0 13.22 ifconfig06:35:01 DumpThreads 16534 R 128 0 12.78 cut06:35:01 cron 16545 R 128 0 14.76 sendmail06:35:01 sendmail 16545 R 274 0 10.88 dynamicmaps.cf06:35:02 postdrop 16546 R 118 0 32.94 Universal06:35:02 pickup 9574 R 118 0 21.02 localtime[...] 以上是所有延迟时间大于10毫秒的事件。 此“延迟时间”是从虚拟文件系统（VFS）向文件系统的接口发起操作时算起的，直到完成。 这涵盖了所有内容：块设备I/O（磁盘I/O），文件系统CPU周期，文件系统锁，请求队列等待时间等。这是对延迟时间的更好衡量。因为应用程序从文件系统读取，而不是通过块设备层接口读取。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[块IO层]]></title>
    <url>%2FLinux%2F%E5%9D%97IO%E5%B1%82%2F</url>
    <content type="text"><![CDATA[Linux中设备分为字符设备，块设备，网络设备三大类。对于字符设备与块设备区别在于能否随机访问数据，对于字符设备键盘来说，键盘驱动程序会按照和用户输入完全相同的顺序读取字符，并且得到一个字符流。但是对于块设备硬盘来说，硬盘驱动程序可能需要读取磁盘上任意块的内容，这些块不一定是连续的，所以说可以被随机访问。 所以事实上内核不必提供一个专门的子系统来管理字符设备，但是对块设备的管理却必须要有一个专门的提供服务的子系统。不仅仅是因为块设备的复杂性远远高于字符设备，更重要的原因是块设备对执行性能的要求很高﹔对硬盘每多一份利用都会对整个系统的性能带来提升。另外，我们将会看到，块设备的复杂性会为这种优化留下很大的施展空间。对块设备和块设备的请求进行管理，在内核中称作块IO层。 块设备块设备中最小的可寻址单元为扇区，扇区最常见的大小为512字节。块是文件系统的一种抽象–只能基于块来访问文件系统。虽然物理磁盘寻址是按照扇区级进行的，但是内核执行的所有磁盘操作都是按照块进行的。内核（对有扇区的硬件设备）要求块大小是2的整数倍，而且不能超过一个页的长度。所以，对块大小的最终要求是，必须是扇区大小的2的整数倍，并且要小于页面大小。所以通常块大小是512KB、1KB或4KB。 块设备基本操作单元缓冲区和缓冲区头高速缓存这一层分为页缓存（page cache）和缓冲区（buffer cache），buffer cache是建立在page cache之上的。page cache是面向文件的抽象，而buffer cache则是面向块设备的抽象。 当一个块被调入内存时（也就是说，在读入后或等待写出时)，它要存储在一个缓冲区中。每个缓冲区与一个块对应，它相当于是磁盘块在内存中的表示。前面提到过，块包含一个或多个扇区，但大小不能超过一个页面，所以一个页可以容纳一个或多个内存中的块。由于内核处理数据时需要一些相关的控制信息（比如块属于哪一个块设备，块对应于哪个缓冲区等)，所以每一个缓冲区都有一个对应的描述符。该描述符用buffer_head结构体表示，称作缓冲区头，在文件&lt;linux/buffer_head.h&gt;（基于内核4.1.14）中定义，它包含了内核操作缓冲区所需要的全部信息。 随着时代的发展，访问单独磁盘块的场景越来越少，页缓存（page cache）慢慢占了主要地位，在Linux kernel 2.4以后，缓冲区（buffer cache）已经不单独实现了，而是建立在页缓存（page cache）的基础上。 1234567891011121314151617struct buffer_head &#123; unsigned long b_state; /* 缓冲区状态标志 */ struct buffer_head *b_this_page;/* 页面中的缓冲区，递归列表 */ struct page *b_page; /* 存储该缓冲区的页面 */ sector_t b_blocknr; /* 起始块号 */ size_t b_size; /* 块大小 */ char *b_data; /* 块起始位置，位于b_page中所指明的页面的某个位置上 */ struct block_device *b_bdev; /* 表示该请求指向哪个块设备。*/ bh_end_io_t *b_end_io; /* I/O 完成方法 */ void *b_private; /* reserved for b_end_io */ struct list_head b_assoc_buffers; /* associated with another mapping */ struct address_space *b_assoc_map; /* mapping this buffer is associated with */ atomic_t b_count; /* 该缓冲区使用计数 */&#125;; b_count域表示缓冲区的使用记数，可通过两个定义在文件&lt;linux/buffer_head.h&gt;中的内联函数对此域进行增减。 12345678910static inline void get_bh(struct buffer_head *bh)&#123; atomic_inc(&amp;bh-&gt;b_count);&#125;static inline void put_bh(struct buffer_head *bh)&#123; smp_mb__before_atomic(); atomic_dec(&amp;bh-&gt;b_count);&#125; 在操作缓冲区头之前，应该先使用get_bh()函数增加缓冲区头的引用计数，确保该缓冲区头不会再被分配出去;当完成对缓冲区头的操作之后，还必须使用put_bh()函数减少引用计数。 与缓冲区对应的磁盘物理块由b__blocknr-th域索引，该值是b__bdev域指明的块设备中的逻辑块号。与缓冲区对应的内存物理页由b_ page 域表示，另外，b_data域直接指向相应的块(它位于b_ page域所指明的页面中的某个位置上)，块的大小由b_size 域表示，所以块在内存中的起始位置在b_data处，结束位置在(b_data + b_size)处。 在2.6内核以前，将缓冲区作为I/O操作单元带来了两个弊端。 缓冲区头是一个很大且不易控制的数据结构体(现在是缩减过的了)，而且缓冲区头对数据的操作既不方便也不清晰。对内核来说，它更倾向于操作页面结构，因为页面操作起来更为简便，同时效率也高。 缓冲区头仅能描述单个缓冲区，当作为所有I/O的容器使用时，缓冲区头会促使内核把对大块数据的I/O操作(比如写操作)分解为对多个buffer_head结构体进行操作。这样做必然会造成不必要的负担和空间浪费。所以2.5开发版内核的主要目标就是为块I/O操作引入一种新型、灵活并且轻量级的容器，bio结构体。 bio结构体目前内核中块I/O操作的基本容器由bio结构体表示，它定义在文件&lt;linux/blk_types.h&gt;中。该结构体代表了正在现场的(活动的)以片断(segment)链表形式组织的块I/O操作，对缓冲区进行了封装，使用一个结构体数组存储多个缓冲区片段。结构体数组的一个元素是一小块连续的内存缓冲区，整个结构体数组表示了一个完整的缓冲区。这样的话，就不需要保证单个缓冲区一定要连续。 所以通过用片段来描述缓冲区，即使一个缓冲区分散在内存的多个位置上，bio结构体也能对内核保证I/O操作的执行。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970struct bio &#123; struct bio *bi_next; /* request queue link */ struct gendisk *bi_disk; unsigned int bi_opf; /* bottom bits req flags, * top bits REQ_OP. Use * accessors. */ unsigned short bi_flags; /* status, etc and bvec pool number */ unsigned short bi_ioprio; unsigned short bi_write_hint; blk_status_t bi_status; u8 bi_partno; /* Number of segments in this BIO after * physical address coalescing is performed. */ unsigned int bi_phys_segments; /* * To keep track of the max segment size, we account for the * sizes of the first and last mergeable segments in this bio. */ unsigned int bi_seg_front_size; unsigned int bi_seg_back_size; struct bvec_iter bi_iter; atomic_t __bi_remaining; bio_end_io_t *bi_end_io; void *bi_private;#ifdef CONFIG_BLK_CGROUP /* * Optional ioc and css associated with this bio. Put on bio * release. Read comment on top of bio_associate_current(). */ struct io_context *bi_ioc; struct cgroup_subsys_state *bi_css;#ifdef CONFIG_BLK_DEV_THROTTLING_LOW void *bi_cg_private; struct blk_issue_stat bi_issue_stat;#endif#endif union &#123;#if defined(CONFIG_BLK_DEV_INTEGRITY) struct bio_integrity_payload *bi_integrity; /* data integrity */#endif &#125;; unsigned short bi_vcnt; /* how many bio_vec's 代表内存段的数目。*/ /* * Everything starting with bi_max_vecs will be preserved by bio_reset() */ unsigned short bi_max_vecs; /* max bvl_vecs we can hold */ atomic_t __bi_cnt; /* pin count */ struct bio_vec *bi_io_vec; /* 记录了高速缓存层要提交给磁盘的数据。一个bio_io_vec可看作一个连续的内存段。 */ struct bio_set *bi_pool; /* * We can inline a number of vecs at the end of the bio, to avoid * double allocations for a small number of bio_vecs. This member * MUST obviously be kept at the very end of the bio. */ struct bio_vec bi_inline_vecs[0];&#125;; 使用bio结构体的目的主要是代表正在现场执行的I/O操作，所以该结构体中的主要城都是用来管理相关信息的，其中最重要的几个域是bi_io_vecs、bi_vcnt。 下图显示了bio结构体及其他结构体之间的关系。 I/O向量bi_io_vec指针指向一个 bio_vec 结构体数组，该结构体链表包含了一个特定IO操作所需要使用到的所有片段。每个bio_vec结构都是一个形式为&lt;page,offset,len&gt;的向量，它描述的是一个特定的片段:片段所在的物理页、块在物理页中的偏移位置、从给定偏移量开始的块长度。整个bio_io_vec结构体数组表示了一个完整的缓冲区。bio_vec结构定义在&lt;linux/bvec.h&gt; 文件中: 12345678struct bio_vec &#123; /*指向该缓冲区片段所在的页面*/ struct page *bv_page; /* 缓冲区片段以字节为单位的长度*/ unsigned int bv_len; /*该缓冲区片段在物理页面的偏移位置*/ unsigned int bv_offset;&#125;; 总而言之，每一个块I0请求都通过一个bio结构体表示。每个请求包含一个或多个块，这些块存储在bio_vec 结构体数组中。这些结构体描述了每个片段在物理页中的实际位置，并且像向量一样被组织在一起。I/O 操作的第一个片段由b_io_vec结构体所指，其他的片段在其后依次放置，共有bi_vcnt个片段。当块I/O层开始执行请求、需要使用各个片段时，bi jidx 域会不断更新，从而总指向当前片段。 bi_cnt 域记录bio结构体的使用计数，如果该域值减为0，就应该撤销该bio结构体，并释放它占用的内存。通过下面两个函数管理使用计数。 123456789101112131415static inline void bio_get(struct bio *bio)&#123; bio-&gt;bi_flags |= (1 &lt;&lt; BIO_REFFED); smp_mb__before_atomic(); atomic_inc(&amp;bio-&gt;__bi_cnt);&#125;static inline void bio_cnt_set(struct bio *bio, unsigned int count)&#123; if (count != 1) &#123; bio-&gt;bi_flags |= (1 &lt;&lt; BIO_REFFED); smp_mb(); &#125; atomic_set(&amp;bio-&gt;__bi_cnt, count);&#125; 前者增加使用计数，后者减少使用计数(如果计数减到0,则撤销bio结构体)。在操作正在活动的bio结构体时，-定要首先增加它的使用计数，以免在操作过程中该bio结构体被释放;相反在操作完毕后，要减少使用计数。 请求队列块设备将它们挂起的块I/O请求保存在请求队列中，该队列由reques_queue结构体表示，定义在文件&lt;linux/blkdev.h&gt;中，包含一个双向请求链表以及相关控制信息。通过内核中像文件系统这样高层的代码将请求加入到队列中。请求队列只要不为空，队列对应的块设备驱动程序就会从队列头获取请求，然后将其送入对应的块设备上去。请求队列表中的每一项都是一个单独的请求，由reques结构体表示。 队列中的请求由结构体requcst 表示，它定义在文件&lt;linux/blkdev.h&gt;中。因为一个请求可能要操作多个连续的磁盘块，所以每个请求可以由多个bio结构体组成。注意，虽然在磁盘上的块必须连续，但是在内存中这些块并不一定要连续，每一个bio结构体都可以描述多个片段(回忆一下，片段是内存中连续的小区域)，而每个请求也可以包含多个bio结构体。 I/O调度程序I/O调度程序虚拟块设备给多个磁盘请求，以便降低磁盘寻址时间，确保磁盘性能的最优化。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[系统调用机制]]></title>
    <url>%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2F%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[系统调用机制现在所有的通用OS都提供了许多系统调用，但它们所提供的系统调用会有一定的差异。对于一般通用的OS而言，可将系统调用分为如下三大类。 进程控制类系统调用主要用于对进程控制的系统调用有： 创建和终止进程的系统调用。 获得和设置进程属性的系统调用。 等待某事件出现的系统调用。 文件操纵类系统调用对文件进行操纵的主要系统调用如下： 创建和删除文件。 打开和关闭文件的系统调用。 读和写文件的系统调用。 进程通信类系统调用除上诉三类系统调用外，常用的系统调用还包括设备管理类系统调用和信息维护类系统调用，前者主要用于实现申请设备、释放设备、设备I/O和重定向、获得和设置设备属性等功能，后者主要用来获得包括有关系统和文件的时间、日期信息、操作系统版本、当前用户以及有关空闲内存和磁盘空间大小等多方面的信息。 Linux三种接口Linux提供了用户接口（shell），库函数接口（C语言程序调用库），系统调用接口三种接口。在系统调用接口这一层中，许多操作系统都提供了上面所介绍的各种类型的系统调用，实现的功能也相类似，但在实现的细节和形式方面却相差很大，这种差异给实现应用程序与操作系统平台的无关性带来了很大的困难。为解决这一问题，国际标准化组织ISO给出的有关系统调用的国际标准POSIX1003.1 (Portable OperatingSystem IX)，也称为“基于UNIX的可移植操作系统接口”。 POSIX定义了标准应用程序接口(API)，用于保证编制的应用程序可以在源代码一级上在多种操作系统上移植运行。只有符合这一标准的应用程序，才有可能完全兼容多种操作系统,即在多种操作系统下都能够运行。 POSIX标准定义了一组过程，这组过程是构造系统调用所必须的，这组过程也就是下面要介绍的系统调用处理过程。通过调用这些过程所提供的服务，确定了一系列系统调用的功能。 需要明确的是，POSIX标准所定义的一组过程虽然指定了系统调用的功能，但并没有明确规定系统调用是以什么形式实现的，是库函数还是其它形式。 如早期操作系统的系统调用使用汇编语言编写，这时的系统调用可看成是扩展的机器指令，因而，能在汇编语言编程中直接使用。而在一些高级语言或C语言中，尤其是最新推出的一些操作系统，如UNIX新版本、Linux、Windows 和OS/2等，其系统调用干脆用C语言编写，并以库函数形式提供，所以在用C语言编写的应用程序中，可直接通过使用对应的库函数来使用系统调用，库函数的目的是隐藏访管指令的细节，使系统调用更像过程调用。但一般地说，库函数属于用户程序而非系统调用程序。如下图UNIX/Linux的系统程序、库函数、系统调用的层次关系。 系统调用与API有什么区别？区别：API是函数的定义，规定了这个函数的功能，跟内核无直接关系，最流行的API标准就是posix标准。 系统调用是通过中断向内核发请求，实现内核提供的某些服务。 C语言库函数则是基于Posix标准的具体实现。 联系：一个API可能会需要一个或多个系统调用来完成特定功能。通俗点说就是如果这个API需要跟内核打交道就需要系统调用，否则不需要。程序员调用的是API（如C库函数），然后通过与系统调用共同完成函数的功能。因此，API是一个提供给应用程序的接口，一组函数，是与程序员进行直接交互的。系统调用则不与程序员进行交互的，它根据API函数，通过一个软中断机制向内核提交请求，以获取内核服务的接口。并不是所有的API函数都一一对应一个系统调用，有时一个API函数会需要几个系统调用来共同完成函数的功能，甚至还有一些API函数不需要调用相应的系统调用（因此它所完成的不是内核提供的服务）。 strace命令跟踪进程中的系统调用strace常用来跟踪进程执行时的系统调用和所接收的信号。 在Linux世界，进程不能直接访问硬件设备，当进程需要访问硬件设备(比如读取磁盘文件，接收网络数据等等)时，必须由用户态模式切换至内核态模式，通过系统调用访问硬件设备。strace可以跟踪到一个进程产生的系统调用,包括参数，返回值，执行消耗的时间。 使用-c参数可以打印出每一个系统调用执行的时间，被调用的次数，出错的次数，系统调用的名称。其他参数如下所示。 命令详细参数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556-c 统计每一系统调用的所执行的时间,次数和出错的次数等.-d 输出strace关于标准错误的调试信息.-f 跟踪由fork调用所产生的子进程.-ff 如果提供-o filename,则所有进程的跟踪结果输出到相应的filename.pid中,pid是各进程的进程号.-F 尝试跟踪vfork调用.在-f时,vfork不被跟踪.-h 输出简要的帮助信息.-i 输出系统调用的入口指针.-q 禁止输出关于脱离的消息.-r 打印出相对时间关于,,每一个系统调用.-t 在输出中的每一行前加上时间信息.-tt 在输出中的每一行前加上时间信息,微秒级.-ttt 微秒级输出,以秒了表示时间.-T 显示每一调用所耗的时间.-v 输出所有的系统调用.一些调用关于环境变量,状态,输入输出等调用由于使用频繁,默认不输出.-V 输出strace的版本信息.-x 以十六进制形式输出非标准字符串-xx 所有字符串以十六进制形式输出.-a column设置返回值的输出位置.默认 为40.-e expr指定一个表达式,用来控制如何跟踪.格式如下:[qualifier=][!]value1[,value2]...qualifier只能是 trace,abbrev,verbose,raw,signal,read,write其中之一.value是用来限定的符号或数字.默认的 qualifier是 trace.感叹号是否定符号.例如:-eopen等价于 -e trace=open,表示只跟踪open调用.而-etrace!=open表示跟踪除了open以外的其他调用.有两个特殊的符号 all 和 none.注意有些shell使用!来执行历史记录里的命令,所以要使用\\.-e trace=set只跟踪指定的系统 调用.例如:-e trace=open,close,rean,write表示只跟踪这四个系统调用.默认的为set=all.-e trace=file只跟踪有关文件操作的系统调用.-e trace=process只跟踪有关进程控制的系统调用.-e trace=network跟踪与网络有关的所有系统调用.-e strace=signal跟踪所有与系统信号有关的 系统调用-e trace=ipc跟踪所有与进程通讯有关的系统调用-e abbrev=set设定 strace输出的系统调用的结果集.-v 等与 abbrev=none.默认为abbrev=all.-e raw=set将指 定的系统调用的参数以十六进制显示.-e signal=set指定跟踪的系统信号.默认为all.如 signal=!SIGIO(或者signal=!io),表示不跟踪SIGIO信号.-e read=set输出从指定文件中读出 的数据.例如:-e read=3,5-e write=set输出写入到指定文件中的数据.-o filename将strace的输出写入文件filename-p pid跟踪指定的进程pid.-s strsize指定输出的字符串的最大长度.默认为32.文件名一直全部输出.-u username以username 的UID和GID执行被跟踪的命令 从用户态函数到系统调用 比如在程序中调用fwrite函数，而fwrite函数在 glibc库中调用系统调用write()，然后从用户态陷入内核态，查找系统调用表，对应的系统调用服务例程为sys_ write。 系统调用中涉及的基本概念系统态和用户态在计算机系统中，通常运行着两类程序：系统程序和应用程序。为了防止应用程序对OS的破坏，应用程序和OS的内核是运行在不同的状态，即OS的内核是运行在系统态，而应用程序是运行在用户态。 在计算机系统中设置了两种状态：系统态(或称为核心态)和用户态。在实际运行过程中，处理机会在系统态和用户态间切换。相应地，现代多数OS将CPU的指令集分为特权指令和非特权指令两类。 特权指令。特权指令是指在系统态运行的指令，它对内存空间的访问范围基本不受限制，不仅能访问用户空间，也能访问系统空间。如启动外部设备、设置系统时钟时间、关中断、转换执行状态等。特权指令只允许OS使用，不允许应用程序使用，以避免引起系统混乱。 非特权指令。非特权指令是在用户态运行的指令。应用程序所使用的都是非特权指令，它只能完成一般性的操作和任务，不能对系统中的硬件和软件直接进行访问，对内存的访问范围也局限于用户空间。这样，可以防止应用程序的运行异常对系统造成破坏。 这种限制是由硬件实现的，如果在应用程序中使用了特权指令，就会发出权限出错信号，操作系统捕获到这个信号后，将转入相应的错误处理程序，将停止该应用程序的与逆行，重新调度。 中断和陷入中断是指CPU对系统发生某事件时的这样一种响应: CPU暂停正在执行的程序，在保留现场后自动地转去执行该事件的中断处理程序;执行完后，再返回到原程序的断点处继续执行。下图表示中断时CPU的活动轨迹。还可进一步把中断分为外中断和内中断。所谓外中断，是指由于外部设备事件所引起的中断，如通常的磁盘中断、打印机中断等;而内中断则是指由于CPU内部事件所引起的中断，如程序出错(非法指令、地址越界)、电源故障等。内中断(trap)也被译为“捕获”或“陷入”。 通常，陷入是由于执行了现行指令所引起的;而中断则是由于系统中某事件引起的，该事件与现行指令无关。由于系统调用引起的中断属于内中断，因此把由于系统调用引起中断的指令称为陷入指令。 系统调用号和参数在Linux中，每个系统调用被赋予一个系统调用号。这样，通过这个独一无二的号就可以关联系统调用。当用户空间的进程执行一个系统调用的时候，这个系统调用号就被用来指明到底是要执行哪个系统调用。进程不会提及系统调用的名称。 系统调用号相当关键，一旦分配就不能再有任何变更，否则编译好的应用程序就会崩溃。 因为所有的系统调用陷入内核的方式都一样，所以仅仅是陷入内核空间是不够的。因此必须把系统调用号一并传给内核。在系统调用命令(陷入指令)中把相应的系统调用号传递给中断和陷入机制的方法有很多种，在有的系统中，直接把系统调用号放在系统调用命令(陷入指令)中;如IBM 370和早期的UNIX系统，是把系统调用命令的低8位用于存放系统调用号;在另一些系统中,则将系统调用号装入某指定寄存器或内存单元中，如MS-DOS是将系统调用号放在AH寄存器中，Linux则是利用EAX寄存器来存放应用程序传递的系统调用号。 在陷人内核之前，用户空间就把相应系统调用所对应的号放入eax中了。这样系统调用处理程序一旦运行，就可以从eax中得到数据。其他体系结构上的实现也都类似。 内核记录了系统调用表中的所有已注册过的系统调用的列表，存储在sys_call_table中。 sys_call_table是一张由指向实现各种系统调用的内核函数的函数指针组成的表,一般在entry.s中定义。这个表中为每一个有效的系统调用指定了惟一的系统调用号。 除了系统调用号以外，大部分系统调用都还需要一些外部的参数输入。在执行系统调用时，如何设置系统调用所需的参数，即如何将这些参数传递给陷入处理机构和系统内部的子程序(过程)，常用的实现方式有以下几种： 陷入指令自带方式。陷入指令除了携带一个系统调用号外，还要自带几个参数进入系统内部，由于一条陷入指令的长度是有限的，因此自带的只能是少量的、有限的参数。 直接将参数送入相应的寄存器中。MS-DOS便是采用的这种方式，即用MOV指令将各个参数送入相应的寄存器中。系统程序和应用程序都可以对这些寄存器进行访问。这种方式的主要问题是这种寄存器数量有限，限制了所设置参数的数目。 参数表方式。将系统调用所需的参数放入一张参数表中，再将指向该参数表的指针放在某个指定的寄存器中。当前大多数的OS中，如UNIX系统和Linux系统，便是采用了这种方式。在x86系统上，ebx, ecx, edx, esi和edi按照顺序存放前五个参数。 如何处理系统调用在设置了系统调用号和参数后，便可执行一条系统调用命令。不同的系统可采用不同的执行方式。在UNIX系统中，是执行CHMK命令；而在MS-DOS中则是执行INT21软中断。系统调用的处理过程可分成以下三步：首先，将处理机状态由用户态转为系统态；之后，由硬件和内核程序进行系统调用的一般性处理，即首先保护被中断进程的CPU环境，将处理机状态字PSW、程序计数器PC、系统调用号、用户栈指针以及通用寄存器内容等压入堆栈；然后，将用户定义的参数传送到指定的地址并保存起来。 其次，分析系统调用类型，转入相应的系统调用处理子程序。为使不同的系统调用能方便地转向相应的系统调用子程序，在系统中配置了一张系统调用入门表。表中的每个每个表目都对应一个系统调用，其中包含该系统调用自带参数的数目、系统调用处理子程序的入口地址等。因此，核心可利用系统调用号去查找该表，即可找到相应处理子程序的入口地址而转去执行它。 最后，在系统调用处理子程序执行完后，应恢复被中断的或设置新进程的CPU现场，然后返回被中断进程或新进程，继续向下执行。 下面我们从用户态跟踪一个系统调用到内核态。 操作系统使用系统调用表将系统调用编号翻译为特定的系统调用。系统调用表包含有实现每个系统调用的函数的地址。例如，fork() 系统调用函数名为sys_fork。fork()系统调用编号是2，所以sys_fork() 位于系统调用表的第四个条目中（因为系统调用起始编号为0）。从地址 sys_call_table + (2 * word_size) 读取数据，得到sys_fork()的地址。 找到正确的系统调用地址后，它将控制权转交给那个系统调用。 读取函数结束后，从sys_fork()返回，它将控制权切换给 ret_from_sys。它会去检查那些在切换回用户空间之前需要完成的任务。如果没有需要做的事情，那么就恢复用户进程的状态，并将控制权交还给用户程序。 系统调用实例简介系统调用安全，直接关系操作系统内核的安全，试想一个程序可以肆意的调用内核程序，将会非常危险，因此设计一个日志收集系统记录下每次系统调用的时间，调用的程序，将有助于系统管理员排查故障。 举例来说，与文件I/O相关的系统调用必须检查文件描述符是否有效。与进程相关的函数必须检查提供的PID是否有效。必须检查每个参数，保证它们不但合法有效，而且正确。最重要的一种检查就是检查用户提供的指针是否有效。试想，如果一个进程可以给内核传递指针而又无须被检查，那么它就可以给出一个它根本就没有访问权限的指针，哄骗内核去为它拷贝本不允许它访问的数据，如原本属于其他进程的数据。具体的实现过程将在下一节详细介绍。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>系统调用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件共享与保护]]></title>
    <url>%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2F%E6%96%87%E4%BB%B6%E5%85%B1%E4%BA%AB%E4%B8%8E%E4%BF%9D%E6%8A%A4%2F</url>
    <content type="text"><![CDATA[文件共享在现代计算机系统中，必须提供文件共享手段，即指系统应允许多个用户(进程)共享同一份文件。这样，在系统中只需保留该共享文件的一份副本。如果系统不能提供文件共享功能，就意味着凡是需要该文件的用户，都须各自备有此文件的副本，显然这会造成对存储空间的极大浪费。 随着计算机技术的发展，文件共享的范围也在不断扩大，从单机系统中的共享，扩展为多机系统的共享，进而又扩展为计算机网络范围的共享，甚至实现全世界的文件共享。 早在20世纪的60和70年代，已经出现了不少实现文件共享的方法，如绕弯路法、连访法，以及利用基本文件实现文件共享的方法;而现代的一些文件共享方法，也是在早期这些方法的基础上发展起来的。下面我们仅介绍当前常用的两种文件共享方法，它们是在树形结构目录的基础上经适当修改形成的。 基于有向无循环图实现文件共享（硬链接）有向无循环图DAG(Directed Acyclic Graph)在严格的树形结构目录中，每个文件只允许有一个父目录，父目录可以有效地拥有该文件，其它用户要想访问它，必须经过其属主目录来访问该文件。这就是说，对文件的共享是不对称的，或者说，树形结构目录是不适合文件共享的。如果允许一个文件可以有多个父目录，即有多个属于不同用户的多个目录，同时指向同一个文件，这样虽会破坏树的特性，但这些用户可用对称的方式实现文件共享，而不必再通过其属主目录来访问。 上图展示出了一个有向无循环图，它允许每一个文件都可以有多个父目录。如图中的文件F8有三个父目录，它们分别是D5、D6和D3,其中D5和D3还使用了相同的名字p,目录D6有两个父目录D2和D1。 由上所述得知，当有多个用户要共享一个子目录或文件时，必须将共享文件或子目录链接到多个用户的父目录中，才能方便地找到该文件。现在的问题是，如何建立父目录D5与共享文件F8之间的链接呢? 如果在文件目录中所包含的是文件的物理地址,即文件所在盘块的盘块号，则在链接时，必须将文件的物理地址拷贝到D5目录中去。但如果以后D5或D6还要继续向该文件中添加新内容，也必然要相应地再增加新的盘块，这些是由附加操作Append 来完成的。而这些新增加的盘块也只会出现在执行了操作的目录中。 可见, 这种变化对其他用户而言，是不可见的，因而新增加的这部分内容已不能被共享。 利用索引结点为了解决这个问题，可以引用索引结点，即诸如文件的物理地址及其它的文件属性等信息，不再是放在目录项中，而是放在索引结点中。在文件目录中只设置文件名及指向相应索引结点的指针，如下图所示。 图中的用户Wang和Lee的文件目录中，都设置有指向共享文件的索引结点指针。此时，由任何用户对共享文件所进行的Append操作或修改，都将引起其相应结点内容的改变(例如，增加了新的盘块号和文件长度等)，这些改变是其他用户可见的，从而也就能提供给其他用户来共享。 在索引结点中还应有一个链接计数count,用于表示链接到本索引结点(亦即文件)上的用户目录项的数目。当count=3 时，表示有三个用户目录项连接到本文件上，或者说是有三个用户共享此文件。 当用户C创建一个新文件时，他便是该文件的所有者，此时将count置1。当有用户B要共享此文件时，在用户B的目录中增加一目录项,并设置一指针指向该文件的索引结点，此时，文件主仍然是C,count=2。如果用户C不再需要此文件，是否能将此文件删除呢?回答是否定的。因为，若删除了该文件，也必然删除了该文件的索引结点，这样便会使B的指针悬空，而B则可能正在此文件上执行写操作，此时将因此半途而废。但如果C不删除此文件而等待B继续使用，这样，由于文件主是C,如果系统要记账收费，则C必须为B使用此共享文件而付账，直至B不再需要。下图展示出了B链接到文件前后的情况。 ln命令可以创建硬链接： ln 源文件 目标文件 这里顺便说一下目录文件的”链接数”。创建目录时，默认会生成两个目录项：”.”和”..”。前者的inode号码就是当前目录的inode号码，等同于当前目录的”硬链接”；后者的inode号码就是当前目录的父目录的inode号码，等同于父目录的”硬链接”。所以，任何一个目录的”硬链接”总数，总是等于2加上它的子目录总数（含隐藏目录）。 基于符号链的共享方式（软链接）符号链接基本思想利用符号链接实现文件共享的基本思想，是允许一个文件或子目录有多个父目录，但其中仅有一个作为主(属主)父目录，其它的几个父目录都是通过符号链接方式与之相链接的(简称链接父目录)。 当访问“c”时，操作系统判断文件“c”属于Link 类型文件，于是会根据其中记录的路径层层查找目录，最终找到目录表1中的“a”表项，于是就找到了文件1的索引结点。 符号链接实现共享的优点在利用符号链方式实现文件共享时，只是文件主才拥有指向其索引结点的指针；而共享该文件的其他用户则只有该文件的路径名，并不拥有指向其索引结点的指针。这样，也就不会发生在文件主删除一共享文件后留下一悬空指针的情况。当文件的拥有者把一个共享文件删除后，如果其他用户又试图通过符号链去访问一个已被删除的共享文件，则会因系统找不到该文件而使访问失败，于是再将符号链删除，此时不会产生任何影响。 值得一提的是，在计算机网络中，Web浏览器在进行浏览时所使用的文件是HTML类型的文件。在HTML文件中有着许多链接符，通过这些链接符能够链接(通过计算机网络)世界上任何地方的机器中的文件。在利用符号链实现共享时，同样可以通过网络链接到分布在世界各地的计算机系统中的文件。 &lt; a href=”url”&gt;链接文本&lt;/ a&gt; ln -s命令可以创建软链接： ln -s 源文文件或目录 目标文件或目录 小结 文件系统的安全性安全性两个方面确保未经授权的用户不能存取某些文件。涉及到技术、管理、法律、道德和政治等问题。 安全性的两个重要方面： 数据丢失。 灾难 硬件或软件故障 人的失误。可通过磁盘容错技术和备份(存放在另一处)来解决。 入侵者。（积极的或消极的) 非技术人员的偶然窥视 入侵者的窥视 明确的偷窃企图 商业或军事间谍活动 设计安全时要考虑是那一类入侵者. 文件的保护机制(1) 文件保护 用于提供安全性的特定的操作系统机制。 (有权限的用户, 应让其进行相应操作, 否则, 应禁止) 实现：用户验证、存取控制 (2) 用户验证用户登录, 检验其身份 口令 物理鉴定 磁卡，指纹，签名分析，手指长度分析 (3) 存取控制 审查用户的权限 审查本次操作的合法性 为了确保文件系统的安全性，可针对上述原因而采取以下措施: (1)通过存取控制机制来防止由人为因素所造成的文件不安全性。 (2)通过磁盘容错技术来防止由磁盘部分的故障所造成的文件不安全性。 (3)通过“后备系统”来防止由自然因素所造成的不安全性。 接下来我们主要看一下第二方面磁盘容错技术。 第一级 磁盘容错技术SFT-1第一级容错技术(SFT- I )是最基本的一种磁盘容错技术,主要用于防止因磁盘表面缺陷所造成的数据丢失。它包含双份目录、双份文件分配表及写后读校验等措施。 双份目录和双份文件分配表在磁盘上存放的文件目录和文件分配表FAT，是文件管理所用的重要数据结构。 为了防止这些表格被破坏，可在不同的磁盘上或在磁盘的不同区域中，分别建立(双份)目录表和FAT。 其中一份为主目录及主FAT;另一份为备份目录及备份FAT。一但由于磁盘表面缺陷而造成主文件目录或主FAT的损坏时，系统便自动启用备份文件目录及备份FAT，从而可以保证磁盘上的数据仍是可访问的。 热修复重定向和写后读校验由于磁盘价格昂贵，当磁盘表面有少量缺陷时，则可采取某种补救措施后继续使用磁盘。一般主要采取以下两个补救措施: 热修复重定向:系统将磁盘容量的一部分(例如2%~ 3%)作为热修复重定向区，用于存放当发现磁盘有缺陷时的待写数据，并对写入该区的所有数据进行登记，以便于以后对数据进行访问。 写后读校验方式。为了保证所有写入磁盘的数据都能写入到完好的盘块中，应该在每次从内存缓冲区向磁盘中写入一个数据块后，又立即从磁盘上读出该数据块，并送至另缓冲区中，再将该缓冲区内容与内存缓冲区中在写后仍保留的数据进行比较。若两者一致，便认为此次写入成功, 可继续写下一一个盘块;否则，再重写。若重写后两者仍不一致,则认为该盘块有缺陷，此时，便将应写入该盘块的数据，写入到热修复重定向区中。 第二级磁盘容错技术SFT-2磁盘镜像-两个磁盘驱动器互为备份为了避免磁盘驱动器发生故障而丢失数据，便增设了磁盘镜像功能。为实现该功能，须在同一磁盘控制器下再增设一个完全相同的磁盘驱动器，如图所示。当采用磁盘镜像方式时，在每次向主磁盘写入数据后，都需要将数据再写到备份磁盘上，使两个磁盘上具有完全相同的位像图。 把备份磁盘看作是主磁盘的一面镜子，当主磁盘驱动器发生故障时，由于有备份磁盘的存在，在进行切换后，使主机仍能正常工作。磁盘镜像虽然实现了容错功能，但未能使服务器的磁盘I/O速度得到提高，却使磁盘的利用率降至仅为50%。 磁盘双工(Disk Duplexing)如果控制这两台磁盘驱动器的磁盘控制器发生故障，或主机到磁盘控制器之间的通道发生了故障，磁盘镜像功能便起不到数据保护的作用。 因此，在第二级容错技术中，又增加了磁盘双工功能，即将两台磁盘驱动器分别接到两个磁盘控制器上，同样使这两台磁盘机镜像成对，如图所示。在磁盘双工时，文件服务器同时将数据写到两个处于不同控制器下的磁盘上，使两者有完全相同的位像图。如果某个通道或控制器发生故障时，另一个通道上的磁盘仍能正常工作，不会造成数据的丢失。在磁盘双工时，由于每一个磁盘都有自己的独立通道，故可同时(并行)地将数据写入磁盘,或读出数据。 文件系统的数据一致性控制同一数据存放在不同的文件中, 对它修改时应对不同的文件都统一修改, 才能保证数据的一致性。修改时数据的流向是, 磁盘块、内存、写回磁盘块。若在写回之前, 系统崩溃, 则文件系统数据出现不一致。 系统应配置保证数据一致性的软件和相应的硬件，硬件采取冗余技术配置一个高度可靠的存储系统, 称为稳定存储器; 目前广泛采用磁盘双工方式来实现稳定存储器。设计保证数据一致性的实用程序, 当系统再次启动时, 运行该程序, 检查磁盘块和目录系统。 事务定义事务是用于访问和修改数据的一个程序单位, 由一系列相关的读写操作组成; 被访问的数据可以分散在不同位置, 只有一系列读写操作全部完成才能以托付操作(Commit Operation)终止操作; 而只要有一个操作失败就执行夭折操作(Abort Operation)。 为了保证数据的一致性, 对于夭折事务所操作过的数据必须恢复原来的状态, 使该事务退回(rolled back),保证一个事务对一批数据修改操作,要么全部完成要么 一个也不修改, 这种特性称事务的原子性。 事务记录为了实现上述的原子修改，通常须借助于称为事务记录的数据结构来实现。这些数据结构被放在稳定存储器中，用来记录在事务运行时数据项修改的全部信息，故又称为运行记录(Log)。该记录中包括有下列字段: 事务名:用于标识该事务的惟一名字; 数据项名:指被修改数据项的惟一名字; 旧值:修改前数据项的值; 新值:修改后数据项将具有的值。恢复算法由于一组被事务Ti修改的数据以及它们被修改前和修改后的值都能在事务记录表中找到，因此，利用事务记录表， 系统能处理任何故障而不致使故障造成非易失性存储器中信 系统能处理任何故障而不致使故障造成非易失性存储器中信息的丢失。恢复算法可利用以下两个过程: undo〈Ti〉。 该过程把所有被事务Ti修改过的数据 恢复为修改前的值。 恢复为修改前的值。 redo〈Ti〉。 该过程把所有被事务Ti修改过的数据设置为新值。 检查点检查点的作用使对为了对事务记录的清理工作经常化, 设置检查点记录; 每隔一定的时间做一次清理: 将内存中的当前事务记录表的所有记录, 和所有已修改的数据, 输出到稳定存储器中; 再将检查点记录输出到稳定存储器中; 每当出现一个检查点记录便执行恢复操作。 新的恢复算法发生故障后,恢复算法只需对最后一个检查点之后的事务记录进行处理。 即从最后一个检查点之后的第一个事务记录开始,对所有的事务Tk , 在Log表中出现(Tk托付)记录则执行redo(Tk ), 未出现(Tk托付)记录则执行undo(Tk )。 并发控制由于事务具有的原子性, 使得一个事务执行完后才允许另一事务执行, 即事务对数据项的修改是互斥的,事务的这种特性称为顺序性,将实现顺序性的技术称为并发控制。可以用互斥信号量来保证事务处理的顺序性,但用的最广的是“锁”。 利用互斥锁实现顺序性设置一种用于实现互斥的锁, 简称互斥锁,为每一个共享对象设一把互斥锁, 如果事务 Ti 需要对一批对象进行访问,则为了保证事务操作的原子性, 应先获得这批对象的互斥锁, 将他们全部锁住, 如果成功便可以对这批对象执行读写操作,然后全部开锁, 若某对象已被其它事务锁住, 则Ti 要将已锁住的对象全部开锁。 重复数据的一致性问题重复文件的一致性以UNIX类型的文件系统为例,通常文件一个文件的目录项由一个文件名和一个索引结点号组成; 当有重复文件时, 一个文件的目录项由一个文件名和若干个索引结点号组成。保证重复文件的一致性用两种方法： (1) 当一个文件被修改后,可查目录, 从各i结点找到各拷贝的物理位置, 对这些拷贝做同样修改。 (2) 为新修改的文件建立几个新拷贝取代原来的拷贝。 盘块号一致性的检查两张表，每块对应一个表中的计数器，初值为0，正常情况下空闲块计数表与数据块计数表对应项的值应该是互补关系。 表一：记录了每块在空闲块表中出现的次数 表二：记录了每块在文件中出现的次数 链结数一致性的检查在UNIX的文件目录中,索引结点中有一个链接计数 count, 它用来表示共享文件计数, 它应该和目录中指向该索引结点的数目一致。否则出错。 为检查不一致差错, 同样配置一张计数器表, 为每个文件建立一个表项, 检查时从根结点开始, 查找并计数,检查完后, 将计数器表的每个计数值与count 对比, 一致则正确, 否则发生链接数不一致差错。 count值大于计数表的计数值则该文件永远不会被删除; count值小于计数表的计数值则该文件会被误删除; 这两种情况解决方法是将count值修改正确。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>文件系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux字符设备驱动程序]]></title>
    <url>%2FLinux%2FLinux%E5%AD%97%E7%AC%A6%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[5.4 Linux设备驱动分类Linux系统将设备分为三个类：字符设备、块设备、网络设备，在这三大类中，字符设备相对比较简单，应用程序通过字符设备文件来访问字符设备，本讲主要介绍字符设备，如果对块设备和网络设备感兴趣的话，可以参看相关资料，并对其进行深入了解。 什么是字符设备？字符设备是指只能一个字节一个字节进行读写操作的设备，不能随机读取设备中的某一数据、读取数据要按照先后顺序。字符设备是面向流的设备，常见的字符设备有鼠标、键盘、串口、控制台和LED等。 一般每个字符设备或者块设备都会在/dev目录下对应一个设备文件，并且每个设备文件都必须有主/次设备号，主设备号相同的设备是同类设备，使用同一个驱动程序。 Linux用户层程序通过设备文件来使用驱动程序操作字符设备或块设备。 可以通过 cat /proc/devices 命令查看当前已经加载的设备驱动程序的主设备号。 通过在/dev目录下执行命令 ls -l 可以看到所有设备文件的主设备号和次设备号： 对常见设备文件作如下说明： 点击查看详细内容 /dev/hd[a-t]：IDE设备 /dev/sd[a-z]：SCSI设备 /dev/fd[0-7]：标准软驱 /dev/md[0-31]：软raid设备 /dev/loop[0-7]：本地回环设备 /dev/mem：内存 /dev/null：无限数据接收设备,相当于黑洞 /dev/zero：无限零资源 /dev/tty[0-63]：虚拟终端 /dev/ttyS[0-3]：串口 /dev/lp[0-3]：并口 /dev/console：控制台 /dev/fb[0-31]：framebuffer /dev/cdrom =&gt; /dev/hdc /dev/modem =&gt; /dev/ttyS[0-9] /dev/pilot =&gt; /dev/ttyS[0-9] 如何建立设备文件？建立设备文件有两种方式，一是通过系统调用mknod()，编程中调用该函数可以建立一个新的设备文件名，另外一种就是通过mknod命令，命令的第一个参数为设备文件名，第二个参数为设备类型，比如c表示字符设备，第三、四个参数为设备文件的主设备号和次设备号，比如231和0。主设备号和次设备号合起来唯一的确定一个设备，同一个设备不同类型的主设备号是一样的，次设备号不同，比如一个硬盘的多个分区就有不同的次设备号，通过主设备号就可以把设备文件与驱动程序关联起来。 mknod filename type major minor filename：要创建的设备文件名； type：设备类型，c代表一个字符设备，b代表一个块设备； major：主设备号； minor：次设备号； 如何描述字符设备？Linux内核中抽象出struct cdev结构体来表示一个字符设备，cdev 定义于 &lt;linux/cdev.h&gt; 中其中，其中最关键的是file_operations结构，它是实现字符设备的操作集。 12345678struct cdev &#123; struct kobject kobj; // 内嵌内核对象 struct module *owner; //该字符设备所在的内核模块 const struct file_operations *ops; //文件操作结构体 struct list_head list; //已注册字符设备链表 dev_t dev; //由主、次设备号构成的设备号 unsigned int count;//同一主设备号的次设备号的个数&#125;; Linux使用file_operations结构访问驱动程序的函数，这个结构的每一个成员的名字都对应着一个系统调用。 123456789101112131415161718struct file_operations &#123; struct module *owner; loff_t (*llseek) (struct file *, loff_t, int); ssize_t (*read) (struct file *, char *, size_t, loff_t *); ssize_t (*write) (struct file *, const char *, size_t, loff_t *); int (*readdir) (struct file *, void *, filldir_t); unsigned int (*poll) (struct file *, struct poll_table_struct *); int (*ioctl) (struct inode *, struct file *, unsigned int, unsigned long); int (*mmap) (struct file *, struct vm_area_struct *); int (*open) (struct inode *, struct file *); int (*flush) (struct file *); int (*release) (struct inode *, struct file *); int (*fsync) (struct file *, struct dentry *, int datasync); int (*fasync) (int, struct file *, int); int (*lock) (struct file *, int, struct file_lock *); ssize_t (*readv) (struct file *, const struct iovec *, unsigned long,loff_t *); ssize_t (*writev) (struct file *, const struct iovec *, unsigned long, loff_t *); &#125;; 用户进程利用在对设备文件进行诸如read，write操作的时候，系统调用通过设备文件的主设备号找到相应的设备驱动程序，然后读取这个数据结构相应的函数指针，接着把控制权交给该函数，这是Linux的设备驱动程序工作的基本原理。 字符设备与文件系统的接口 如图，在Linux内核中，最左边， 使用cdev结构体来描述字符设备;通过其成员dev_t来定义设备号（分为主、次设备号）以确定字符设备的唯一性;通过其成员file_operations来定义字符设备驱动提供给虚拟文件系统VFS的接口函数，如常见的open()、read()、write()等,这些函数真正的操作硬件设备。 在上一个图的基础上我们看这个图，字符设备驱动程序是以内核模块的形式加载到内核中的，首先模块加载函数按静态或者动态方式获取设备号；然后字符设备初始化函数建立cdev与 file_operations之间的连接， 通过注册函数向系统添加一个cdev以完成注册; 模块卸载时与加载对应，要注销cdev，并释放设备号。 在用户程序中，可以通过系统调用open(), read(), write()等调用驱动程序在内核中所实现的这些函数。这样用户态到内核驱动之间的通路就打通了。 编写简单的字符设备驱动程序 如图，编写字符设备驱动分为三大步骤： 驱动的初始化，其中又分为四个步骤，调用相关的函数达到。 实现设备的操作，具体的操作取决于你自己所要实现的功能，这里只列出了基本的操作 驱动的注销，注销就是释放资源。 其中调用的接口函数功能如下： 第1个函数是分配函数，动态申请cdev的内存，给该结构分配内存空间。 第2个函数是初始化函数，初始化cdev的成员，并建立cdev和file_operations之间关联. 第3个函数注册cdev设备对象，也就是把字符设备添加到字符设备表中，就像大家入学时进行注册一样。 第4个函数是注销驱动程序调用，将cdev对象从系统中删除。 第5个函数释放cdev数据结构所占的内存。 设备号的申请和释放一个字符设备或块设备都有一个主设备号和一个次设备号。主设备号用来标识与设备文件相连的驱动程序，用来反映设备类型。次设备号被驱动程序用来辨别操作的是哪个设备，用来区分同类型的设备。注册时申请设备号，注销时释放设备号，就像大家入学是有一个学号，毕业离开时就释放掉这个学号。 用户空间与内核空间数据的传送当我们在用户程序中调用read（）函数时，陷入内核空间，实际上要通过内核的copy_to_user()函数把内核空间缓冲区中的数据拷贝到用户空间的缓冲区，反之，当我们调用write（)函数时，内核通过调用copy_from_user()函数把用户空间的数据拷贝到内核缓冲区。 小结如何具体编写一个字符驱动程序，主要有三个步骤，一是驱动的初始化，二是实现对设备的具体操作，三是注销驱动程序， 在动手实践一节，将给出一个字符设备驱动程序的编写过程以及运行机制。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>驱动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[I/O设备驱动程序]]></title>
    <url>%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2FIO%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[5.3 I/O设备驱动程序 什么是设备驱动程序？设备处理程序通常又称为设备驱动程序，它是I/O系统的高层与设备控制器之间的通信程序，其主要任务是接收上层软件发来的抽象I/O要求，如read或write命令，文件系统把这些请求转换为具体要求后，发送给设备控制器，启动设备去执行；反之，它也将由设备控制器发来的信号传送给上层软件。 思考一下为何不同的设备需要不同的设备驱动程序？ 由于驱动程序与硬件密切相关，故通常应为每一类设备配置一种驱动程序。例如，打印机和显示器需要不同的驱动程序。 不同设备的内部硬件特性也不同，这些特性只有厂家才知道，因此厂家须提供与设备相对应的驱动程序，CPU执行驱动程序的指令序列，来完成设置设备寄存器，检查设备状态等工作。 设备驱动程序的功能实现I/O进程与设备控制器之间的通信,设备驱动程序应具有以下功能 接收由与设备无关的软件发来的命令和参数，并将命令中的抽象要求转换为与设备相关的低层操作序列。 例如，将磁盘块号转换为磁盘的盘面、磁道号及扇区号。 检查用户I/O请求的合法性，了解I/O设备的工作状态，传递与I/O设备操作有关的参数，设置设备的工作方式。 发出I/O命令，如果设备空闲，立即启动I/O设备，完成指定I/O操作；如果设备忙碌，则将请求者的请求块挂在设备队列上等待。 及时响应由设备控制器发来的中断请求，并根据其中断类型，调用相应的中断处理程序进行处理。 设备驱动程序的特点设备驱动程序属于低级的系统例程，它与一般的应用程序及系统程序之间有下述明显差异。 驱动程序主要是指在请求I/O的进程与设备控制器之间的一个通信和转换程序。 它将进程的I/O请求经过转换后，传送给控制器；又把控制器中所记录的设备状态和I/O操作完成情况及时的反应给请求I/O的进程。 驱动程序与设备控制器和I/O设备的硬件特性紧密相关，因而对不同类型的设备应配置不同的驱动程序。 例如，可以为相同的多个终端设置一个终端驱动程序，但有时即使是同一类型的设备，由于其生产厂家不同，他们也可能并不完全兼容，此时也需为它们配置不同的驱动程序。 驱动程序与I/O设备所采用的I/O控制方式紧密相关。 常用的I/O控制方式是中断驱动和DMA方式，这两种方式的驱动程序明显不同，后者是按数组方式启动设备及进行中断处理。 由于驱动程序与硬件紧密相关，因而其中的一部分必须用汇编语言编写。目前有很多驱动程序的基本部分，已经固化在ROM中。 驱动程序应允许可重入。一个正在运行的驱动程序常会在一次调用完成前被再次调用。例如，网络驱动程序正在处理一个到来的数据包时，另一个数据包可能已经到达。 设备驱动程序的处理过程不同类型的设备应有不同的设备驱动程序，但大体上它们都可以分成两部分： 能够驱动I/O设备工作的驱动程序 设备中断处理程序（处理I/O完成后的工作）。 设备驱动程序的主要任务是启动指定设备。但在启动之前，还必须完成必要的准备工作，如检测设备状态是否为“忙”等。在完成所有的准备工作后，才向设备控制器发送一条启动命令。以下是设备驱动程序的处理过程： 将抽象要求转换为具体要求 由于用户及上层软件对设备控制器的具体情况毫无了解，因而只能向它发出抽象的要求（命令），但这些命令无法传送给设备控制器。因此就需要将这些抽象要求转换为具体要求。这一转换工作只能由驱动程序来完成，因为在OS中只有驱动程序才同时了解抽象要求和设备控制器中的寄存器情况；也只有它才知道命令、参数和数据应分别送往哪个寄存器。如:将逻辑盘块号转换为具体的盘面、磁道和扇区 检查I/O请求的合法性 如:打印机请求读, 以读方式打开磁盘后请求写 读出和检查设备的状态 如:读出并检查状态是否为就绪, 确定启动控制器或等待 传送必要的参数 对于许多设备，特别是块设备，除必须向其控制器发送启动命令外，还需传送必要的参数。例如在启动磁盘进行读/写之前，应先将本次要传送的字节数和数据应到达的主存始址，送入控制器的相应寄存器中。 工作方式的设置 如:异步通信, 先设置波特率、校验方式、停止位等 启动I/O设备 驱动程序发出I/O命令后，基本的I/O操作是在设备控制器的控制下进行的。通常，I/O操作所要完成的工作较多，需要一定的时间，如读/写一个盘块中的数据，此时驱动（程序）进程把自己阻塞起来，直到中断到来时才将它唤醒。 当I/O任务完成时，I/O控制器会发送一个中断信号，系统会根据中断信号类型找到相应的中断处理程序并执行。中断处理程序的处理流程如下: I/O控制方式设备管理的主要任务之一是控制设备和内存或处理机之间的数据传送。对I/O设备的控制，随着软硬件的发展前后出现过如下的控制方式：使用轮询的可编程I/O方式，使用中断的可编程I/O方式，直接存储器访问（DMA）方式，I/O通道控制方式. 前两种是以字节为单位进行数据传递，后两种是以数据块为单位进行数据传递。下面对这些方式进行介绍。 程序直接控制方式该图是展示了程序I/O方式的流程。 以完成一次读操作为例程序直接控制的流程： CPU通过控制线向I/O控制器发出一个读指令，I/O控制器会根据CPU的要求启动相应的设备，并将这个设备相应的状态设置为1，表示设备忙碌（未就绪）。 接下来设备就准备CPU想要读入的数据，但是由于设备和CPU处理速度的差异，所以在设备还没有完成I/O之前，CPU会一直不断的轮询检查设备的状态，即状态寄存器的值。其实就是在不在的执行程序的循环，若状态为一直是1，说明设备还没有准备好要输入的数据，于是CPU会不断轮询。 如果设备已经准备好了输入的数据，设备会向I/O控制器传送要输入的数据，并且报告自身的状态是已就绪状态。 之后I/O控制器将要输入的数据放到数据寄存器中，并且将状态寄存器的值改为0（已就绪）。 在状态寄存器改为0时，CPU轮询检查到了数据已经准备好了，设备已就绪，即可将数据寄存器中的内容读入CPU自己的寄存器中，再把寄存器的内容放入内存（数据从设备到内存需要经过CPU）。这样就完成了一次读操作。 由于CPU速度远远快于I/O设备，因此CPU需要不断地测试I/O设备，这种控制方式又称为轮询或忙等。 可以看出，缺点是CPU利用率相当低，由于CPU速度远远快于I/O设备，致使绝大部分时间都在测试I/O设备是否已经完成数据传输，从而造成CPU的极大浪费。另外，每个字的读/写都需要CPU的帮忙。 中断控制方式 该图是展示中断驱动I/O方式的流程。 引入中断机制。由于I/O设备速度很慢，因此在CPU发出读/写命令后，可将等待I/0的进程阻塞，先切换到别的进程执行。 以数据输入为例，当用户进程需要输入数据时，由处理器向设备控制器发出一条I/O指令启动设备进行输入。在输入数据的同时，CPU可以做其他工作。当输入完成时，设备管理器向CPU发出一个中断信号，CPU接收到中断信号以后，转去执行设备中断处理程序。设备终中断处理程序将输入数据寄存器中的数据传送到内存的指定单元中，供要求输入的进程使用，然后再启动设备去读下一个数据。 优点：有了中断硬件的支持后，CPU与I/O设备之间可以并行工作，CPU只需要收到中断后处理即可，大大提高了CPU利用率。 缺点：如果每台设备每输入/输出一个数据，都要求中断CPU，这样在一次数据传送过程中的中断次数太多，从而耗费大量CPU时间。设备与CPU之间的数据交换仍以字（节）为单位。 直接存储器访问（DMA）方式DMA方式的进入：为了适应一次传送大量数据的应用要求，以及尽量减少CPU对高速外设的干预。与“中断驱动方式”相比，DMA（Direct Memory Access，直接存储器存取。主要用于块设备的I/O控制）有这样几个改进： 数据传送的单位是块，不再是一个字一个字的传送。 数据的流向是从设备直接放入内存，或者直接从内存到设备，不再需要CPU干预。 仅在一个块或多个块的开始和结束时，才需要CPU干预。 DMA控制器的组成DMA控制器组成：主机（CPU）—控制块的接口、I/O控制逻辑、块设备—控制器接口。 命令/状态寄存器CR：用于存放CPU发来的I/O命令，或设备的状态信息。 内存地址寄存器MAR：在设备向内存输入数据时，MAR表示输入的数据应该存放到内存的什么位置，在内存向设备输出数据时，MAR表示要输出的数据放在内存的什么位置。 数据寄存器DR:暂存从设备到内存，或者从内存到设备的数据。 数据计数器DC:表示剩余要读/写的字节数。 DMA工作过程 该图就是DMA工作过程。 以数据输入为例，当用户进程需要输入数据时，CPU将准备存放数据的内存起始地址以及要传送的字节数分别送入DMA控制器中的内存地址寄存器和传送字节计数器中，并启动设备开始进行输入。 在输入数据的同时，CPU可以去做其他事情，输入设备不断地挪用CPU工作周期，将数据寄存器中的数据源源不断地写入内存，直到要求传送的数据全部传输完毕。 DMA控制器在传输完毕时向CPU发送一个中断信号，CPU收到中断信号后转中断处理程序，中断结束后返回被中断程序。 DMA控制方式的特点为：数据传输的基本单位是数据块，而且数据是单向传输，从设备到内存或者相反。仅在传送一个数据块的开始和结束时，才需要CPU干预，整块数据的传送是在控制器的控制下完成。 优点：设备和CPU可以并行工作，同时设备与内存的数据交换速度更快，并且不需要CPU干预。 缺点：数据传送的方向、存放输入数据的内存起始地址及传送数据的长度等都由CPU控制，并且每台设备都需要一个DMA控制器，当设备增加时，多个DMA控制器的使用也不经济。 I/O通道控制方式为了进一步减少CPU对I/O操作的干预，引入了通道。 通道控制方式与DMA类似，也是一种以内存为中心，实现设备与内存直接数据交换的控制方式。与DMA相比，通道需要的CPU干预更少，即把对一个数据块的读写为单位的干预减少为对一组数据块读写为单位的干预，而且可以做到一个通道控制多台设备。 通道本质上是一个简单的处理器，它独立于CPU，有运算和逻辑，有自己的指令系统，也在程序控制下工作，专门负责输入、输出控制，具有执行I/O指令的能力，并通过执行通道I/O程序来控制I/O操作。通道的指令系统比较简单，一般只有数据传送指令、设备控制指令等。 在通道控制方式中，CPU只需要发出启动指令，指出要求通道执行的操作和使用的I/O设备，该指令就可以启动通道并使该通道从内存中调出相应的通道程序执行。 通道控制工作方式 以数据输入为例，当用户进程需要输入数据时，CPU发出启动指令指明要执行的I/O操作、所使用的设备和通道。 当对应通道接收到CPU发来的启动指令后，把存放在内存中的通道程序读出，并执行通道程序，控制设备将数据传送到内存中指定的区域。在设备进行输入的同时，CPU可以去做其他事情。 当数据传送结束后时，设备控制器向CPU发送一个中断请求，CPU收到中断信号后转中断处理程序，中断结束后返回被中断程序。 优点：解决了I/O操作的独立性和各部件工作的并行性。不仅能实现CPU与通道的并行操作，而且通道与通道之间也能实现并行操作，各个通道上的外设也能实现并行操作，从而提高了整个系统效率。 缺点：需要更多硬件（通道处理器），成本较高，常用于大型数据交互的场合。 小结]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[指针2]]></title>
    <url>%2FC%E8%AF%AD%E8%A8%80%2F%E6%8C%87%E9%92%882%2F</url>
    <content type="text"><![CDATA[指针除了可以和一维数组、二维数组进行结合，也可以作为函数的返回值或者指向一个函数。下面就从这两个方面入手，看看指针与函数的有趣结合。 指针作为函数返回值函数作为C语言的基本组成模块，包括函数首部和函数体两部分。函数首部由函数返回值类型、名字、参数类型、参数等组成；函数体又可以分为说明语句与执行语句。 12345返回值类型 函数名(形参类型 形参)//函数首部&#123; //函数体 说明语句 执行语句&#125; 其中返回值类型除了几种基本数据类型以外，还可以返回一个指针值，则该函数就称为指针类型的函数。例如： int *fun(int x) 该函数名为fun，fun两侧的运算符分别为“”和“()”，由于“()”的优先级高于“”，故fun()首先代表一个函数，然后与前面的“*”结合，表示返回值是一个指针，而int则表示该指针指向的数据类型为整型，即返回值为指向整型变量的指针类型。常见的返回指针值的函数有字符串复制函数。 123456789101112char *strcpy(char *s1,char *s2)&#123; char *p=s1; while(*s1++=*s2++); return p;&#125;void main()&#123; char s[20]=&quot;welcome&quot;; printf(&quot;%s\n&quot;,strcpy(s,&quot;you&quot;));&#125; 运行结果为： you 指针指向一个函数在说明这个问题之前，先对函数在内存中的存储做个简单的介绍。指针之所以可以指向函数，是因为函数被分配在代码区一段连续的内存里面，函数名称存储的为该内存块的入口地址，故可以使用指针来调用函数。当函数被调用时，函数的形参从右至左依次压入栈中，且连续。因此对于不确定参数个数的函数，只要取得了第一个形参的地址，就可以依次取得后面参数的值。这也是printf函数能够格式化打印任意个数变量值的原因，其参数从右至左依次压栈，输出时从左至右依次弹出。下面以求n个数的最大值为例: 123456789101112int max(int n,int ...)&#123; int maxnum = *(&amp;n+1); for(int i=2;i&lt;=n;i++) &#123; if(maxnum&lt;(*(&amp;n+i))) maxnum = *(&amp;n+i); &#125; return maxnum;&#125; 其中n为要输入参数的个数，(&amp;n+1)就得到了第一个待比较数字的值，(&amp;n+i)就得到了第i个待比较数字的值，i一直取到n。 下面继续，指向函数的指针变量的定义如下： int (*p)(); 首先(*p)说明p是一个指针变量，后面的()则代表了该指针指向为某一个函数，而int代表了该函数的返回值后为整型。使用函数对该指针变量进行赋值操作： p = funname; 则可以使用(*p)代替函数名funname进行函数调用。]]></content>
      <categories>
        <category>C语言</category>
      </categories>
      <tags>
        <tag>指针</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[指针]]></title>
    <url>%2FC%E8%AF%AD%E8%A8%80%2F%E6%8C%87%E9%92%88%2F</url>
    <content type="text"><![CDATA[指针重要吗指针在C语言中占据着重要的地位，指针有效的取代了在低端语言（如汇编语言与机器代码）直接使用内存地址。指针又比较贴近硬件，编译器能够很容易的将指针翻译为机器代码，这使指针操作时的计算机负担较少，同时简化一些C语言编程任务，能够提高程序的运作速度。 正是由于比较贴近硬件，并且指针可以与C语言中的其他数据结构（如数组、链表、结构体、函数）进行结合，可以说是贯穿了整本C语言。使得对于初学计算机编程的人理解起来有些难度。本文将结合图像和示例对指针的知识进行整理汇总，记录指针的学习过程，让指针也变得简单起来。 指针的定义指针（英语：Pointer）在百科中给出的定义是编程语言中的一类数据类型及其对象或变量，用来表示或存储一个存储器地址，这个地址的值直接指向（points to）存在该地址的对象的值。 从这个定义中可以得到以下信息，首先指针也是一种数据类型，就像我们前面学到的int,char,float,double等基础数据类型。同样的指针也可以像其他基础数据类型一样定义变量，不同的地方在于指针变量中保存的是一个存储器的地址。（补充：程序中所有的数据都是存储在内存中的，内存被分成一个个的内存单元，每个内存单元又有一个地址。系统内存就像是带有门牌号的小房间，如果要使用某个房间，就需要得到房间的门牌号。而指针变量存储的就是门牌号，即也就是存储器地址）。指针定义变量的方式如下： 123456int *a,b=5;char *c,d=&apos;m&apos;;float *f,g=6.0;a = &amp;b;c = &amp;d;f = &amp;g; int *为整型指针数据类型，定义了一个存储整型变量存储地址的指针变量a char *为字符型指针数据类型，定义了一个存储字符型变量存储地址的变量c float *为单精度浮点型指针数据类型，定义了一个存储浮点型变量存储地址的变量f 指针变量的赋值两个指针运算符： 地址运算符&amp;取内存中变量的地址，例如&amp;b,值为整型变量b的内存地址。 取值运算符* 获取指针变量所指向变量的值，例如a,值为5。（注意与int *中的\相区别） a = &b;代表取出整型变量b的地址赋值给a，图中简单表示为2000赋值给a,即a指向变量b,a中存储了变量b的内存地址2000。指针变量c和f的赋值也一样如图所示。 这里要区分两个容易混淆的概念： 指针的类型指针的类型是指针变量本身的类型，指针变量在内存中占据4个字节，存储32位的内存地址码。 指针变量a的类型为int * 指针变量c的类型为char * 指针变量f的类型为float * 指针指向的类型指向的类型是当前指针存储的内存地址中所存储变量的类型。 a指向的类型为int c指向的类型为char f指向的类型为float 指针与一维数组我们都知道数组名是代表数组首元素地址的符号常量，而指针变量又可以存储地址，所以数组不仅可以通过数组名+下标的方式访问，也可以通过指向它的指针变量来访问。 指向数组的指针变量我们前面定义的指针变量都是指向一个基础数据类型的变量，指向数组的指针变量定义方式也类似。 12int c[5];int *p = c; 表达式(p+1)或(c+1)都代表数组元素c[1]，其中p+1和c+1都代表c[1]的地址&amp;c[1]。 数组名称c为常量，故任何对数组名称赋值的操作都是错误的。 C语言不会对地址操作做越界检查，故数组越界不会报错，这也是程序容易出错的地方。 12int (*p)[5];int *p[5]; 上面第一个定义方式为，首先计算()，*p代表p是一个指针变量，然后是[5],代表p指向一个大小为5的数组，最后int，代表p指向的是一个存储整型变量的数组。即定义数组指针p，指向一个存储5个整型元素的数组。 [5]的优先级要高于*，故p首先是一个大小为5的数组，其存储的数据类型为指针变量，而指针指向的数据类型为整型int。 指针与二维数组定义二维整型数组如下 1int a[3][3] = &#123;&#123;14,16,19&#125;,&#123;22,55,88&#125;,&#123;33,54,32&#125;&#125;; 二维数组a的元素是按行进行存储的，可以将a数组的3行看成3个分数组：a[0],a[1],a[2]。每个分数组是含3个列元素的一维数组。如下图所示： 数组名a是指向0号分数组的指针常量，值为2000，同样不能够给a赋值。a+0，a+1，a+2则分别表示0号分数组，1号分数组，2号分数组，对应存储器地址值为2000，2004，2008。其中a[0],a[1],a[2]为三个一维分数组的数组名称，这三个数组名存储的地址又分别指向a[0][0],a[1][0],a[2][0]，对应的数值为3000，3006，3012。a[0]+1和a[0]+2则是分别指向a[0][1],a[0][2]的指针常量,对应的地址值为3002，3004。 虽然a与a[0]的地址都是2000，但是并不等价，a的基类型字节数为4，a[0]为2，也可以从a+1与a[0]+1的不同结果看出。以下是二维数组不同表示形式的含义与内容：]]></content>
      <categories>
        <category>C语言</category>
      </categories>
      <tags>
        <tag>指针</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ARP网关欺骗]]></title>
    <url>%2F%E7%BD%91%E7%BB%9C%2FARP%E7%BD%91%E5%85%B3%E6%AC%BA%E9%AA%97%2F</url>
    <content type="text"><![CDATA[大家好，唐三藏奉旨投西，我是随行记者小唐。就在前几日，孙行者因故打死几个剪径的强盗，唐长老盛怒之下将其赶走。正好让六耳猕猴钻了空子，打昏了三藏，夺走了行李。沙悟净前去索要，竟被六耳猕猴告之他要自行前去取经。悟净不以为然，言道：“老哥啊，没有唐僧去，哪个佛祖都不会传经给你，你这才是白忙活哩。”六耳道：“贤弟啊，你一直就挺懞懂，不太了解情况。其实啊，我这已经人员齐备，整装待发了，你来看。”说着便请出来一匹白马，一个唐三藏，跟着一个八戒，一个沙僧。从后来事态发展的情况来看，六耳的取经队伍恐怕是瞒不过佛祖，自然也无法取来真经。但是如果将真经类比为数据包，六耳猕猴类比为ARP攻击程序，在计算机网络的世界中，这却是特别容易成功的。那这究竟是怎么一回事呢？ARP又是什么呢？ 以太网络别忙，解释这些问题之前，我们先来简单回顾一下计算机之间时如何进行联系的。首先整个计算机网络是由一个又一个的子网所构成，子网可以简单理解为所有连接同一个路由器的计算机所构成。在同一个子网内的计算机，相互之间是可以直接进行通信的，使用的是以太协议，又叫以太网。但是如果想同另一个子网的计算机进行通信就必须经过路由器，此时的路由器又可以叫做网关。在局域网中通信时使用的是MAC地址，而不是常见的IP地址。所以在局域网的两台主机间通信时，必须要知道对方的MAC地址，这就是ARP协议要做的事，将IP地址转换为MAC地址。 通过 ARP 查询目标路由器的 MAC 地址ARP 就是利用广播对所有设备提问：“×× 这个 IP 地址是谁的？请把你的 MAC 地址告诉我。”然后就会有设备回答：“这个 IP 地址是我的，我的 MAC 地址是××××。”如果对方和自己处于同一个子网中，那么通过上面的操作就可以得到对方的 MAC 地址。 ARP缓存如果每次发送包都要这样查询一次，网络中就会增加很多 ARP包，因此我们会将查询结果放到一块叫作 ARP 缓存的内存空间中留着以后用。也就是说，在发送包时，先查询一下 ARP 缓存，如果其中已经保存了对方的 MAC 地址，就不需要发送 ARP 查询，直接使用 ARP 缓存中的地址，而当 ARP 缓存中不存在对方 MAC 地址时，则发送 ARP 查询。 ARP欺骗ARP攻击程序就是一直在应答：“这个 IP 地址（其实是网关的IP地址）是我的，我的 MAC 地址是××××。”这是时候其他设备就会将该设备当作网关，所有的与其他子网交互的数据都会发到该设备，从而实现了ARP欺骗，就会出现无法上网或者数据泄露的风险。使用kali Linux 的arpspoof工具可以轻松实现这一功能。命令参数如下： arpspoof [-i interface] [-c own|host|both] [-t target] [-r] host(-i指定网卡接口，-t指定目标机器，-r当前网关)]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C语言版本之谜]]></title>
    <url>%2FC%E8%AF%AD%E8%A8%80%2FC%E8%AF%AD%E8%A8%80%E7%89%88%E6%9C%AC%E4%B9%8B%E8%B0%9C%2F</url>
    <content type="text"><![CDATA[赋闲在家，可能反而给了我们更多的时间来进行思考。偶然看到一段蛮有吸引力的视频，视频在文章的末尾，只有十分钟，即使是鸡汤，感觉也是一碗可以榨取出营养的鸡汤。 为什么大多数人不能成功作者提出，为什么大多数人都不会真正的成功？我们周围的环境都在试图将我们拉向平庸，并且引力非常大，而且我们中的大多数人都无法摆脱这些诱惑。 这些诱惑可以使你的大脑感到愉悦，甚至有的诱惑还能欺骗我们自己的大脑，让我们自己感觉到自己在学习东西，从而减少一些无所事事的负罪感。我们以电子媒介中的新闻为例，这个电子媒介勾画的世界不存在秩序和意义，我们不必把它当回事。再残忍的谋杀，再具破坏力的地震，再严重的政治错误，只要新闻播音员说一声“好，现在我们看下一篇报道”，一切就可以马上从我们的脑海中消失。新闻播音员意思是我们为上一条新闻花费的时间已经够多了，不必一直念念不忘，应该将注意力转移到下一篇新闻了。在这里我们看到的不仅是零散不全的新闻，而且没有背景，没有结果，更加没有价值，新闻成了纯粹的娱乐。 信息过剩我们每天刷着手机，接受海量的信息，几天下来，可能比古人一辈子获得的信息都要多了。但是能够对我们有价值，能够促成我们行动的信息却少之又少。我们可能一辈子都无法参与到中东的军事行动，英国脱欧的政治博弈，顶多在与朋友闲聊的时候多说两句话。新闻尚如此，可见我们每天对着手机和电脑会遇到多少诱惑。当我们获得了有价值的信息，打算有所行动的时候，会开始惧怕失败。 惧怕失败作者讲的第一个原因就是人们会本能的惧怕失败，在他们眼里如果他们把一件事情搞砸了，那就意味着他们本身就很糟糕，任何失败都会证明他们不够好。但是你可以从失败中获得气馁，也可以从中获得经验。避免了失败也就失去了许多获得经验的机会。这里很多人会说，人们惧怕失败是因为有的人根本承担不起失败的后果。没错，但是生活中不一定都是一些大事情，还有一些小事情，比如作者说的写博客的例子。 花时间去比较作者还提到不要过多的关注别人，嫉妒别人。如果你关注别人过多，就会容易失去自我，你的价值和行动会很难保持一致，这会让你感到空虚和不快乐。作者还说了一句很有道理的话，“嫉妒和怨恨就像自己喝下毒药，却期待别人死去一样”。我们中的大多数人都会选择娱乐和消遣，而不是学习和成长，因为后者很难。遇到困难应该感到高兴，因为这意味着大多数人都不会选择做这件事，竞争并不激烈。 希望作者的视频能够促成你的某些行动。不然就是一碗没有意义的鸡汤了。]]></content>
      <categories>
        <category>C语言</category>
      </categories>
      <tags>
        <tag>C语言</tag>
        <tag>编译器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用CRC校验算法JAVA版]]></title>
    <url>%2FIot%2F%E5%B8%B8%E7%94%A8CRC%E6%A0%A1%E9%AA%8C%E7%AE%97%E6%B3%95JAVA%E7%89%88%2F</url>
    <content type="text"><![CDATA[CRC校验又称为循环冗余校验，是数据通讯中常用的一种校验算法。它可以有效的判别出数据在传输过程中是否发生了错误，从而保障了传输的数据可靠性。 CRC16 Modbus校验算法JAVA版 1234567891011121314151617181920212223242526272829303132333435//大端对齐 static byte[] crc16_tab_h = &#123; (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x01, (byte) 0xC0, (byte) 0x80, (byte) 0x41, (byte) 0x00, (byte) 0xC1, (byte) 0x81, (byte) 0x40 &#125;; //小端对齐 static byte[] crc16_tab_l = &#123; (byte) 0x00, (byte) 0xC0, (byte) 0xC1, (byte) 0x01, (byte) 0xC3, (byte) 0x03, (byte) 0x02, (byte) 0xC2, (byte) 0xC6, (byte) 0x06, (byte) 0x07, (byte) 0xC7, (byte) 0x05, (byte) 0xC5, (byte) 0xC4, (byte) 0x04, (byte) 0xCC, (byte) 0x0C, (byte) 0x0D, (byte) 0xCD, (byte) 0x0F, (byte) 0xCF, (byte) 0xCE, (byte) 0x0E, (byte) 0x0A, (byte) 0xCA, (byte) 0xCB, (byte) 0x0B, (byte) 0xC9, (byte) 0x09, (byte) 0x08, (byte) 0xC8, (byte) 0xD8, (byte) 0x18, (byte) 0x19, (byte) 0xD9, (byte) 0x1B, (byte) 0xDB, (byte) 0xDA, (byte) 0x1A, (byte) 0x1E, (byte) 0xDE, (byte) 0xDF, (byte) 0x1F, (byte) 0xDD, (byte) 0x1D, (byte) 0x1C, (byte) 0xDC, (byte) 0x14, (byte) 0xD4, (byte) 0xD5, (byte) 0x15, (byte) 0xD7, (byte) 0x17, (byte) 0x16, (byte) 0xD6, (byte) 0xD2, (byte) 0x12, (byte) 0x13, (byte) 0xD3, (byte) 0x11, (byte) 0xD1, (byte) 0xD0, (byte) 0x10, (byte) 0xF0, (byte) 0x30, (byte) 0x31, (byte) 0xF1, (byte) 0x33, (byte) 0xF3, (byte) 0xF2, (byte) 0x32, (byte) 0x36, (byte) 0xF6, (byte) 0xF7, (byte) 0x37, (byte) 0xF5, (byte) 0x35, (byte) 0x34, (byte) 0xF4, (byte) 0x3C, (byte) 0xFC, (byte) 0xFD, (byte) 0x3D, (byte) 0xFF, (byte) 0x3F, (byte) 0x3E, (byte) 0xFE, (byte) 0xFA, (byte) 0x3A, (byte) 0x3B, (byte) 0xFB, (byte) 0x39, (byte) 0xF9, (byte) 0xF8, (byte) 0x38, (byte) 0x28, (byte) 0xE8, (byte) 0xE9, (byte) 0x29, (byte) 0xEB, (byte) 0x2B, (byte) 0x2A, (byte) 0xEA, (byte) 0xEE, (byte) 0x2E, (byte) 0x2F, (byte) 0xEF, (byte) 0x2D, (byte) 0xED, (byte) 0xEC, (byte) 0x2C, (byte) 0xE4, (byte) 0x24, (byte) 0x25, (byte) 0xE5, (byte) 0x27, (byte) 0xE7, (byte) 0xE6, (byte) 0x26, (byte) 0x22, (byte) 0xE2, (byte) 0xE3, (byte) 0x23, (byte) 0xE1, (byte) 0x21, (byte) 0x20, (byte) 0xE0, (byte) 0xA0, (byte) 0x60, (byte) 0x61, (byte) 0xA1, (byte) 0x63, (byte) 0xA3, (byte) 0xA2, (byte) 0x62, (byte) 0x66, (byte) 0xA6, (byte) 0xA7, (byte) 0x67, (byte) 0xA5, (byte) 0x65, (byte) 0x64, (byte) 0xA4, (byte) 0x6C, (byte) 0xAC, (byte) 0xAD, (byte) 0x6D, (byte) 0xAF, (byte) 0x6F, (byte) 0x6E, (byte) 0xAE, (byte) 0xAA, (byte) 0x6A, (byte) 0x6B, (byte) 0xAB, (byte) 0x69, (byte) 0xA9, (byte) 0xA8, (byte) 0x68, (byte) 0x78, (byte) 0xB8, (byte) 0xB9, (byte) 0x79, (byte) 0xBB, (byte) 0x7B, (byte) 0x7A, (byte) 0xBA, (byte) 0xBE, (byte) 0x7E, (byte) 0x7F, (byte) 0xBF, (byte) 0x7D, (byte) 0xBD, (byte) 0xBC, (byte) 0x7C, (byte) 0xB4, (byte) 0x74, (byte) 0x75, (byte) 0xB5, (byte) 0x77, (byte) 0xB7, (byte) 0xB6, (byte) 0x76, (byte) 0x72, (byte) 0xB2, (byte) 0xB3, (byte) 0x73, (byte) 0xB1, (byte) 0x71, (byte) 0x70, (byte) 0xB0, (byte) 0x50, (byte) 0x90, (byte) 0x91, (byte) 0x51, (byte) 0x93, (byte) 0x53, (byte) 0x52, (byte) 0x92, (byte) 0x96, (byte) 0x56, (byte) 0x57, (byte) 0x97, (byte) 0x55, (byte) 0x95, (byte) 0x94, (byte) 0x54, (byte) 0x9C, (byte) 0x5C, (byte) 0x5D, (byte) 0x9D, (byte) 0x5F, (byte) 0x9F, (byte) 0x9E, (byte) 0x5E, (byte) 0x5A, (byte) 0x9A, (byte) 0x9B, (byte) 0x5B, (byte) 0x99, (byte) 0x59, (byte) 0x58, (byte) 0x98, (byte) 0x88, (byte) 0x48, (byte) 0x49, (byte) 0x89, (byte) 0x4B, (byte) 0x8B, (byte) 0x8A, (byte) 0x4A, (byte) 0x4E, (byte) 0x8E, (byte) 0x8F, (byte) 0x4F, (byte) 0x8D, (byte) 0x4D, (byte) 0x4C, (byte) 0x8C, (byte) 0x44, (byte) 0x84, (byte) 0x85, (byte) 0x45, (byte) 0x87, (byte) 0x47, (byte) 0x46, (byte) 0x86, (byte) 0x82, (byte) 0x42, (byte) 0x43, (byte) 0x83, (byte) 0x41, (byte) 0x81, (byte) 0x80, (byte) 0x40 &#125;; public byte[] calcCrc16(byte[] data, int offset, int len) &#123; return calcCrc16(data, offset, len, 0xffff); &#125; public byte[] calcCrc16(byte[] data, int offset, int len, int preval) &#123; int ucCRCHi = (preval &amp; 0xff00) &gt;&gt; 8; int ucCRCLo = preval &amp; 0x00ff; int iIndex; for (int i = 0; i &lt; len; ++i) &#123; iIndex = (ucCRCLo ^ data[offset + i]) &amp; 0x00ff; ucCRCLo = ucCRCHi ^ crc16_tab_h[iIndex]; ucCRCHi = crc16_tab_l[iIndex]; &#125; int value = ((ucCRCHi &amp; 0x00ff) &lt;&lt; 8) | (ucCRCLo &amp; 0x00ff) &amp; 0xffff; return int2Bytes(value,2); &#125; public byte[] int2Bytes(int value, int length) &#123; byte[] b = new byte[length]; for (int k = 0; k &lt; length; k++) &#123; b[length - k - 1] = (byte) ((value &gt;&gt; 8 * k) &amp; 0xff); &#125; return b; &#125; CRC16查表算法代码采用的多项式为CRC-16/IBM： X16+X15+X2+1C语言版： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647static uint16_t const CRC16Table[256] = &#123; 0x0000, 0xC0C1, 0xC181, 0x0140, 0xC301, 0x03C0, 0x0280, 0xC241, 0xC601, 0x06C0, 0x0780, 0xC741, 0x0500, 0xC5C1, 0xC481, 0x0440, 0xCC01, 0x0CC0, 0x0D80, 0xCD41, 0x0F00, 0xCFC1, 0xCE81, 0x0E40, 0x0A00, 0xCAC1, 0xCB81, 0x0B40, 0xC901, 0x09C0, 0x0880, 0xC841, 0xD801, 0x18C0, 0x1980, 0xD941, 0x1B00, 0xDBC1, 0xDA81, 0x1A40, 0x1E00, 0xDEC1, 0xDF81, 0x1F40, 0xDD01, 0x1DC0, 0x1C80, 0xDC41, 0x1400, 0xD4C1, 0xD581, 0x1540, 0xD701, 0x17C0, 0x1680, 0xD641, 0xD201, 0x12C0, 0x1380, 0xD341, 0x1100, 0xD1C1, 0xD081, 0x1040, 0xF001, 0x30C0, 0x3180, 0xF141, 0x3300, 0xF3C1, 0xF281, 0x3240, 0x3600, 0xF6C1, 0xF781, 0x3740, 0xF501, 0x35C0, 0x3480, 0xF441, 0x3C00, 0xFCC1, 0xFD81, 0x3D40, 0xFF01, 0x3FC0, 0x3E80, 0xFE41, 0xFA01, 0x3AC0, 0x3B80, 0xFB41, 0x3900, 0xF9C1, 0xF881, 0x3840, 0x2800, 0xE8C1, 0xE981, 0x2940, 0xEB01, 0x2BC0, 0x2A80, 0xEA41, 0xEE01, 0x2EC0, 0x2F80, 0xEF41, 0x2D00, 0xEDC1, 0xEC81, 0x2C40, 0xE401, 0x24C0, 0x2580, 0xE541, 0x2700, 0xE7C1, 0xE681, 0x2640, 0x2200, 0xE2C1, 0xE381, 0x2340, 0xE101, 0x21C0, 0x2080, 0xE041, 0xA001, 0x60C0, 0x6180, 0xA141, 0x6300, 0xA3C1, 0xA281, 0x6240, 0x6600, 0xA6C1, 0xA781, 0x6740, 0xA501, 0x65C0, 0x6480, 0xA441, 0x6C00, 0xACC1, 0xAD81, 0x6D40, 0xAF01, 0x6FC0, 0x6E80, 0xAE41, 0xAA01, 0x6AC0, 0x6B80, 0xAB41, 0x6900, 0xA9C1, 0xA881, 0x6840, 0x7800, 0xB8C1, 0xB981, 0x7940, 0xBB01, 0x7BC0, 0x7A80, 0xBA41, 0xBE01, 0x7EC0, 0x7F80, 0xBF41, 0x7D00, 0xBDC1, 0xBC81, 0x7C40, 0xB401, 0x74C0, 0x7580, 0xB541, 0x7700, 0xB7C1, 0xB681, 0x7640, 0x7200, 0xB2C1, 0xB381, 0x7340, 0xB101, 0x71C0, 0x7080, 0xB041, 0x5000, 0x90C1, 0x9181, 0x5140, 0x9301, 0x53C0, 0x5280, 0x9241, 0x9601, 0x56C0, 0x5780, 0x9741, 0x5500, 0x95C1, 0x9481, 0x5440, 0x9C01, 0x5CC0, 0x5D80, 0x9D41, 0x5F00, 0x9FC1, 0x9E81, 0x5E40, 0x5A00, 0x9AC1, 0x9B81, 0x5B40, 0x9901, 0x59C0, 0x5880, 0x9841, 0x8801, 0x48C0, 0x4980, 0x8941, 0x4B00, 0x8BC1, 0x8A81, 0x4A40, 0x4E00, 0x8EC1, 0x8F81, 0x4F40, 0x8D01, 0x4DC0, 0x4C80, 0x8C41, 0x4400, 0x84C1, 0x8581, 0x4540, 0x8701, 0x47C0, 0x4680, 0x8641, 0x8201, 0x42C0, 0x4380, 0x8341, 0x4100, 0x81C1, 0x8081, 0x4040&#125;;uint16_t CRC16(uint8_t* dataIn, int length)&#123; uint16_t result = 0; uint16_t tableNo = 0; for(int i = 0; i &lt; length; i++) &#123; tableNo = ((result &amp; 0xff) ^ (dataIn[i] &amp; 0xff)); result = ((result &gt;&gt; 8) &amp; 0xff) ^ CRC16Table[tableNo]; &#125; return result; &#125; JAVA版： 123456789101112131415161718192021222324252627282930313233//小端对齐 private static final int[] CRC16Table = &#123; 0x0000, 0xC0C1, 0xC181, 0x0140, 0xC301, 0x03C0, 0x0280, 0xC241, 0xC601, 0x06C0, 0x0780, 0xC741, 0x0500, 0xC5C1, 0xC481, 0x0440, 0xCC01, 0x0CC0, 0x0D80, 0xCD41, 0x0F00, 0xCFC1, 0xCE81, 0x0E40, 0x0A00, 0xCAC1, 0xCB81, 0x0B40, 0xC901, 0x09C0, 0x0880, 0xC841, 0xD801, 0x18C0, 0x1980, 0xD941, 0x1B00, 0xDBC1, 0xDA81, 0x1A40, 0x1E00, 0xDEC1, 0xDF81, 0x1F40, 0xDD01, 0x1DC0, 0x1C80, 0xDC41, 0x1400, 0xD4C1, 0xD581, 0x1540, 0xD701, 0x17C0, 0x1680, 0xD641, 0xD201, 0x12C0, 0x1380, 0xD341, 0x1100, 0xD1C1, 0xD081, 0x1040, 0xF001, 0x30C0, 0x3180, 0xF141, 0x3300, 0xF3C1, 0xF281, 0x3240, 0x3600, 0xF6C1, 0xF781, 0x3740, 0xF501, 0x35C0, 0x3480, 0xF441, 0x3C00, 0xFCC1, 0xFD81, 0x3D40, 0xFF01, 0x3FC0, 0x3E80, 0xFE41, 0xFA01, 0x3AC0, 0x3B80, 0xFB41, 0x3900, 0xF9C1, 0xF881, 0x3840, 0x2800, 0xE8C1, 0xE981, 0x2940, 0xEB01, 0x2BC0, 0x2A80, 0xEA41, 0xEE01, 0x2EC0, 0x2F80, 0xEF41, 0x2D00, 0xEDC1, 0xEC81, 0x2C40, 0xE401, 0x24C0, 0x2580, 0xE541, 0x2700, 0xE7C1, 0xE681, 0x2640, 0x2200, 0xE2C1, 0xE381, 0x2340, 0xE101, 0x21C0, 0x2080, 0xE041, 0xA001, 0x60C0, 0x6180, 0xA141, 0x6300, 0xA3C1, 0xA281, 0x6240, 0x6600, 0xA6C1, 0xA781, 0x6740, 0xA501, 0x65C0, 0x6480, 0xA441, 0x6C00, 0xACC1, 0xAD81, 0x6D40, 0xAF01, 0x6FC0, 0x6E80, 0xAE41, 0xAA01, 0x6AC0, 0x6B80, 0xAB41, 0x6900, 0xA9C1, 0xA881, 0x6840, 0x7800, 0xB8C1, 0xB981, 0x7940, 0xBB01, 0x7BC0, 0x7A80, 0xBA41, 0xBE01, 0x7EC0, 0x7F80, 0xBF41, 0x7D00, 0xBDC1, 0xBC81, 0x7C40, 0xB401, 0x74C0, 0x7580, 0xB541, 0x7700, 0xB7C1, 0xB681, 0x7640, 0x7200, 0xB2C1, 0xB381, 0x7340, 0xB101, 0x71C0, 0x7080, 0xB041, 0x5000, 0x90C1, 0x9181, 0x5140, 0x9301, 0x53C0, 0x5280, 0x9241, 0x9601, 0x56C0, 0x5780, 0x9741, 0x5500, 0x95C1, 0x9481, 0x5440, 0x9C01, 0x5CC0, 0x5D80, 0x9D41, 0x5F00, 0x9FC1, 0x9E81, 0x5E40, 0x5A00, 0x9AC1, 0x9B81, 0x5B40, 0x9901, 0x59C0, 0x5880, 0x9841, 0x8801, 0x48C0, 0x4980, 0x8941, 0x4B00, 0x8BC1, 0x8A81, 0x4A40, 0x4E00, 0x8EC1, 0x8F81, 0x4F40, 0x8D01, 0x4DC0, 0x4C80, 0x8C41, 0x4400, 0x84C1, 0x8581, 0x4540, 0x8701, 0x47C0, 0x4680, 0x8641, 0x8201, 0x42C0, 0x4380, 0x8341, 0x4100, 0x81C1, 0x8081, 0x4040 &#125;;public byte[] CRC16(byte[] bytes) &#123; int result = 0xFFFF; int tableNo = 0; for (int i = 0; i &lt; bytes.length; i++) &#123; tableNo = ((result &amp; 0xff) ^ (bytes[i] &amp; 0xff)); result = ((result &gt;&gt; 8) &amp; 0xff) ^ CRC16Table[tableNo]; &#125; return int2Bytes(result, 2); &#125;]]></content>
      <categories>
        <category>Iot</category>
      </categories>
      <tags>
        <tag>编解码</tag>
        <tag>CRC校验算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[物联网体系结构]]></title>
    <url>%2FIot%2Fiot-architecture%2F</url>
    <content type="text"><![CDATA[目前国内物联网通信领域，主要分为两大阵营：以三大运营商和华为为主的NB通信方式，以阿里巴巴和Lora联盟为主的Lora和LoraWan通信方式（还有传统的GPRS通信方式和SMS通信方式）。 NB借用运营商已有的基站作为网络节点发送数据，而Lora多用于自组网，需要自行搭建网关。两种方式各有优缺点。例如有些共享单车就采用了短信（SMS）+NB+蓝牙的通信方式。其中NB通信方式多采用Coap协议进行。Coap协议是基于UDP的应用层协议，较Http轻量，专为物联网终端设计，采用无状态连接。因为采用无状态连接故Coap并不擅长主动下发命令。Mqtt协议基于TCP协议，有商业和开源的mqtt broker供选择，支持ACL访问控制、集群、共享订阅等高级功能，能够实现实时命令下发，相较于TCP协议大大简化服务器端的开发工作量。采用NB通信方式需要从电信运营商处采购物联卡，并且开通NB流量套餐。目前三大运营商皆有物联卡。 为了方便管理设备的连接，通信协议的解析，以及设备的鉴权，各大运营商都推出了自己的Iot平台。比如中国移动的OneNet平台，华为的OceanConnect平台，以及中国电信在OceanConnect平台基础上，开发出的中国电信Iot平台。目前OceanConnect平台为收费使用且价格不菲，中国电信则免费使用，但是限制只能够使用中国电信的物联卡，且中国电信的物联卡也只能将数据发送到中国电信Iot平台，实现了双向绑定。中国移动的物联卡目前没有这一限制，并且可以使用GPRS（2G\3G\4G\4G+）通信方式。 Iot平台都自带设备接入功能，设备状态管理功能，提供了通信协议解析的接口。用户可根据终端设备的通信协议，安装平台接口标准自行编写编解码插件用于数据的解析，即将Hex格式的数据解析为json格式，反之亦然。Iot平台与用户应用平台的交互也都提供了api接口，用户在应用平台调用对应的接口即可。关于解析好的数据，可以通过订阅/推送的方式发送到应用平台，也可以应用平台通过接口主动请求获取数据。]]></content>
      <categories>
        <category>Iot</category>
      </categories>
      <tags>
        <tag>物联网通信</tag>
        <tag>iot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络是个邮局]]></title>
    <url>%2F%E7%BD%91%E7%BB%9C%2FHistory-of-Computer-Networking%2F</url>
    <content type="text"><![CDATA[网络网络（network）是一组具有通信能力的设备相互连接而形成的。在这个定义中，设备可以是主机（host）,也可以是连接设备，如连接网络到其他网段的路由器（routers）、将设备连接到一起的交换机、对电信号进行数模转换的调制解调器等。 局域网局域网（LAN）通常是私有的，连接一个办公室、大楼或校园内的一些主机。 广域网广域网（WAN）也是由具有通信能力的设备相互连接而形成的。局域网通常覆盖范围受限，广域网则具有更广的地理覆盖范围。 网络是个邮局试想一下，假如你开发了一套应用软件，这些运行在不同终端系统上（计算机、手机、平板）的程序，相互之间需要传输数据。那么运行在一台终端上的程序是如何使用网络设备（网卡、交换机、路由器）将数据传输给另外一台终端上的程序呢？ 实际上计算机（其他终端相同）提供了一套支持多种通信协议的网络通信接口（socket interface），该通信接口明确了，运行在计算机上的程序如何让网络设备将数据发送到另外一台设备。即该网络通信接口制定了一种规则，应用程序必须遵守该规则，才能顺利将数据发送到目标程序。举个例子：艾家庄的艾丽丝小姐想要通过邮局给鲍家庄的鲍博先生写一封信。当然艾丽丝把信写好以后，不能就随手把信往窗外一扔，然后万事大吉，这样鲍博永远也收不到信件。正确的做法是，邮局需要艾丽丝将信装在一个信封里面，信封上写上鲍博的名字、家庭地址、邮政编码，贴上一个邮票。最后把信封投进附近的邮箱里面，这样就能发信件发出去了。因此，邮局提供了一套邮寄信件的“通信接口”，或者说制定了一套规则。艾丽丝必须遵守邮局制定的邮寄信件规则，才能成功将信邮给鲍博。同样，网络也有自己的通信规则（socket interface），应用程序想通过网络发送数据也必须遵守这种规则。当然邮局不止提供一种邮寄信件的服务，同样网络也支持各种协议。 什么是协议协议定义了两个或多个通信实体之间交换的消息的格式和顺序，以及在消息或其他事件的传输或接收上采取的动作。 文章参考自Computer networking: a top-down approach seventh edition。]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>计算机网络 Internet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Next主题开启文章分享功能及去除图片上的分享按钮]]></title>
    <url>%2FHexo%2FNext%E4%B8%BB%E9%A2%98%E5%BC%80%E5%90%AF%E6%96%87%E7%AB%A0%E5%88%86%E4%BA%AB%E5%8A%9F%E8%83%BD%E5%8F%8A%E5%8E%BB%E9%99%A4%E5%9B%BE%E7%89%87%E4%B8%8A%E7%9A%84%E5%88%86%E4%BA%AB%E6%8C%89%E9%92%AE%2F</url>
    <content type="text"><![CDATA[Hexo的Next主题中已经集成了文章分享功能，我们只需要clone下github上的分享插件，并修改Next主题配置文件就可以实现文章的分享功能。具体操作如下。 配置Next主题配置文件打开主题配置文件检索baidushare配置如下： 12345678910111213141516171819baidushare: type: slideneedmoreshare2: enable: true postbottom: enable: true options: iconStyle: true boxForm: horizontal position: bottomCenter networks: Weibo,Wechat,Douban,QQZone,Twitter,Facebook float: enable: false options: iconStyle: box boxForm: horizontal position: middleRight networks: Weibo,Wechat,Douban,QQZone,Twitter,Facebook 下载文章分享所需插件配置文件中已经给出了文章分享所需插件的github仓库地址，即 https://github.com/theme-next/theme-next-needmoreshare2仓库中也给出了使用方法，即首先通过命令行进入themes/next主题目录下，然后将插件clone下来： 12$ cd themes/next$ git clone https://github.com/theme-next/theme-next-needmoreshare2 source/lib/needsharebutton 后期需要更新的话就是通过以下命令： 12$ cd themes/next/source/lib/needsharebutton$ git pull 至此，我们就可以在文章的尾部看到分享文章的按钮了。但是当我把鼠标放在文章中的图片上时，发现图片的右上角也加上了一排分享的按钮，经过分析，找到了添加分享的代码，将其注释一下就大功告成了，实现清清爽爽的页面。找到路径为themes\next\layout_partials\share\baidushare.swig的文件，将代码中的image注释掉。大约在26-30行，代表设置分享按钮在文章底部时的效果；42-46行，代表设置文章分享按钮在侧边时的效果。 12345678910111213141516171819202122232425262728293031323334353637383940 &lt;script&gt; window._bd_share_config = &#123; "common": &#123; "bdText": "", "bdMini": "2", "bdMiniList": false, "bdPic": "" &#125;, "share": &#123; "bdSize": "16", "bdStyle": "0" &#125;, // "image": &#123; // "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"], // "viewText": "分享到：", // "viewSize": "16" // &#125; &#125; &lt;/script&gt;&#123;% elif theme.baidushare.type === "slide" %&#125; &lt;script&gt; window._bd_share_config = &#123; "common": &#123; "bdText": "", "bdMini": "1", "bdMiniList": false, "bdPic": "" &#125;, // "image": &#123; // "viewList": ["tsina", "douban", "sqq", "qzone", "weixin", "twi", "fbook"], // "viewText": "分享到：", // "viewSize": "16" // &#125;, "slide": &#123; "bdImg": "5", "bdPos": "left", "bdTop": "100" &#125; &#125; &lt;/script&gt; 注释掉以后执行Hexo g重新生成页面（记得cd .. 退回到Hexo博客的根目录欧），就可以看到图片上已经没有分享按钮了。]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[本地Git关联Github]]></title>
    <url>%2F%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%2F%E6%9C%AC%E5%9C%B0Git%E5%85%B3%E8%81%94Github%2F</url>
    <content type="text"><![CDATA[本地git关联github1.输入你的github注册邮箱,生成本地ssh key $ ssh-keygen -t rsa -C “your_email@youremail.com“ 2.然后成功后会在User文件夹对应的用户下创建.ssh文件夹，其中有一个id_rsa.pub文件，我们复制其中的key,进入 Account Settings（账户配置），左边选择SSH and GPG Keys选项。其中的title随便填，下面的粘贴在你电脑上生成的key。 3.验证是否绑定本地成功，在git-bash中验证，输入指令： $ ssh -T git@github.com 如果第一次执行该指令，则会提示是否continue继续，如果我们输入yes就会看到成功信息： 4.由于GitHub每次执行commit操作时，都会记录username和email，下面进行设置： $ git config –global user.name “name”//你的GitHub登陆名$ git config –global user.email “123@163.com“//你的GitHub注册邮箱 5.在github上新建一个仓库，并添加一个README.md文件，并pull到本地： $ git pull “仓库的ssh链接” 6.常用git命令 git init //把这个目录变成Git仓库git add README.md //文件添加到仓库git add . //不但可以跟单一文件，还可以跟通配符，添加当前目录下所有文件git commit -m “first commit” //把文件提交到本地仓库git remote add origin git@github.com:yourname/youremail.git //关联远程仓库git pull XXX master –allow-unrelated-histories //允许合并不相关历史的内容git push -u origin master //上传到远程仓库]]></content>
      <categories>
        <category>版本控制</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[换个角度看编解码插件开发]]></title>
    <url>%2FIot%2Foceanconnect-plugin-dev%2F</url>
    <content type="text"><![CDATA[编解码概述书写编解码插件之前首先需要书写Profile文件，定义如下。 Profile文件用于描述一款设备的能力特性。IOT平台通过解析Profile文件，获取该设备支持的服务（通信协议里面的一条完整报文）、属性（报文中的一个字段）、命令（报文中的一个字段）等信息。 IOT平台通过编解码插件，对NB设备上报的数据和下发给NB设备的命令进行格式转换，即将设备上报的16进制格式的报文转换成json，json数据的具体属性名称将由Profile文件来确定。 知识储备熟悉javase基础编程，maven项目管理工具基本命令，jackson类库。 开发工具Eclipse，jdk1.8，maven，NB设备通信协议。 编解码结构分析Profile结构Profile文件是一个深度为四级的文件夹如图所示。 红色方框中为第一级目录，目录名称为当前项目的名称，命名规则为：设备类型-厂商ID-设备型号，例如WellMonitor_Apple_A1865，对应图中功能结构中的Product模块。 绿色方框中为第二级目录，包含两个文件夹profile和service。profile文件夹下放置一个devicetype-capability.json，被用来描述一款设备的能力特征，包括设备类型、厂商、型号、协议类型以及提供的服务类型。相当于对当前整个Profile项目信息的一个概述。service文件夹下则包含设备具备的服务能力，每个服务具备的属性、命令以及命令的参数，对应图中Service模块。 蓝色方框中代表三级目录，代表具体的服务，每个服务中又包含了属性和命令，属性用于描述设备上报数据，命令用于下发指令给设备，分别对应图中的Property模块和Command模块。 此外只有红色与蓝色方框中文件的命名可以改动，其他名称固定不变。 例如图2‑1，service中包含了WellWaterCommand代表井盖+水位设备所有的下行命令；WellWaterDeviceInfo代表该设备上报的设备信息；WellWaterTiming代表该设备定时上报的信息，共计三个服务。关于如何根据设备的通信协议划分服务，将在第三章中详细介绍。 编解码插件结构编解码插件使用java开发且jdk版本必须为1.8，maven进行项目管理，建议使用eclipse开发工具。其工程目录结构如图所示。工程的命名建议和Profile项目相同，即采用：设备类型-厂商ID-设备型号的格式。其余包名类名无需更改，即使用华为提供的名称。 其中下行数据编码表示该类将平台下发的json格式的命令转化为字节数组。 上行数据解码则表示该类将设备上报的字节数组转化为json格式数据。 主类则负责调用下行数据编码或者上行数据解码。 工具类中包含了不同数据类型之间转换的方法。 编解码插件开发Profile开发新建工程新建名称为：设备类型-厂商ID-设备型号的文件夹，例如WellMonitor_Apple_A1865，打开该文件夹新建profile和service两个子文件夹，如图所示。注意profile和service这两个文件夹名称是固定不变的。 创建概述文件打开profile文件夹新建devicetype-capability.json文件，编辑该json文件如图所示。其中每个字段所代表的含义如图所示。serviceTypeCapabilitiesJson数组包含了该Profile文件所有的服务模块，即每增加一个service都需要在这个数组中添加一个对应的json对象。例如Apple公司采用CoAP协议，设备型号为A1865的井盖设备包含三个服务（功能）模块，井盖命令、井盖开机信息，井盖定时上报信息。 Service划分关于一个设备service的划分，可以采用根据设备通信协议中上报报文的功能码进行划分的方法，即协议中一条上报报文对应一个service（通常一条报文占用一个功能码）。 Service划分步骤： 将通信协议中的报文区分为上行数据和下行数据两类。 将下行数据（命令）写在一个service里面。 将上行数据根据功能码的不同，分成若干个不同的service。 创建service打开第二级目录的service文件夹，根据步骤二devicetype-capability.json文件中serviceTypeCapabilities数组中定义的service创建对应的文件夹，文件夹名称为serviceId对应的值。每个文件夹下分别创建profile文件夹，profile文件夹下创建servicetype-capability.json文件，用来描述具体服务的功能，如图所示。 接下来编写每个service对应的servicetype-capability.json，如图所示的Profile，将井盖水位设备的所有命令下发模块单独写在了一个service中，即WellWaterCommand模块；将设备的上报数据信息按照通信协议中每个报文的功能码区分为不同的service，即WellWaterDeviceInfo（开机信息）模块与WellWaterTiming（定时上报）模块。 编写下行service下行命令在service中的书写格式如图3‑5所示。Commands数组里面存放若干个命令对象，每个命令对象包含CommandName命令的名称和paras命令携带的参数数组，以及responses命令回应数组。paras数组里面包含若干个参数对象。参数对象包含paraName等一系列属性。这每一个参数对象都对应通信协议报文中的一个属性。serviceType的值一定要与当前服务文件夹名称保持一致。 编写上行service上行属性service比命令service要简单，properties数组中存放的是设备上报上来的数据，每一个json对象对应通信协议报文中的一个属性。propertyName的值可以随意定义，dataType的值如不能根据通信协议中的默认参数值明显的判断该属性的数据类型，应找相关人员确认，否则解析将会出现数据错误。 打包Profile将WellMonitor_Apple_A1865文件夹下的profile和service文件打包为zip格式，命名为WellMonitor_Apple_A1865，并且压缩包内不能包含WellMonitor_Apple_A1865文件夹这一层目录。 编解码开发使用eclipse配置本地maven仓库，并导入华为提供的demo工程。 修改pom.xml中的&lt;artifactId&gt;XXX&lt;/artifactId&gt;和&lt;Bundle-SymbolicName&gt;XXX&lt;/Bundle-SymbolicName&gt;命名规范：设备类型-厂商ID-设备型号，与Profile文件保持一致。如图所示。 修改ProtocolAdapterImpl.java中的厂商名称与设备型号，注意与Profile保持一致。如图所示。 编写下行编码数据下行编码流程如图所示，通过获取OC平台发送过来的json数据，首先根据serviceId将对应数据保存到全局变量中，然后调用toByte方法，将各种类型的数据装换为16进制的比特数组。拼装成一条完整的报文并返回。其中mid为消息序号，根据通信协议中是否使用消息序号来决定代码中是否使用。详情请参照文末样例代码。 编写上行解码数据上行解码流程如图所示。通过获取OC平台发送过来的16进制比特数组格式数据，首先根据比特数组中的功能码（功能码在数据哪个位置，请从通信协议中查看），将不同服务对应的不同数据，根据通信协议将byte类型的属性转换为所需类型，保存到全局变量中，然后调用toJsonNode方法，拼装成一个Json对象并返回。其中mid为消息序号，根据通信协议中是否使用消息序号来决定代码中是否使用。详情请参照文末样例代码。 打包编解码打包编解码，即将该maven工程打包成jar文件。检查pom.xml中的&lt;packaging&gt;bundle&lt;/packaging&gt;值是否为bundle，不能为jar。以eclipse为例，在工程上右键选择RunAs然后选择Maven build…如图所示。 在Goals中输入clean package点击Run按钮，如图所示。 等待控制台出现BUILDSUCCESS，即可在当前项目的target目录下找到打包好的jar文件，如图所示。 新建package文件夹，文件夹下包含preload文件夹以及package-info.json描述文件，将打包好的jar文件放到preload文件夹下，如图所示。 按照内容修改package-info.json描述文件，如图所示。 最后将package文件夹压缩为package.zip,并且压缩包内不能包含package文件夹这一层目录。 上传离线插件到OC开发者平台上传Profile有两种上传方式。 登录开发者门户，选择产品开发下的添加按钮。 选择从本地导入产品创建-&gt;上传Profil文件。将上文中打包好的Profile文件WellMonitor_Apple_A1865.zip进行上传。 登录管理门户，选择设备管理-&gt;产品模型-&gt;本地导入产品模型，添加完成后会自动同步到开发者门户，然后在开发者门户继续下面的上传插件操作。 上传插件上传Profile以后就会自动生成一个产品，选择第二步编解码插件开发-&gt;插件管理-&gt;上传插件。将上文中打包好的package.zip文件上传。插件上传成功，即可注册设备进行下一步的开发。附录 点击显/隐内容 Nothing to show you.]]></content>
      <categories>
        <category>Iot</category>
      </categories>
      <tags>
        <tag>OceanConnect</tag>
        <tag>编解码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[堪比华为还详细的编解码插件线下开发教程]]></title>
    <url>%2FIot%2F%E7%BC%96%E8%A7%A3%E7%A0%81%E6%8F%92%E4%BB%B6%E4%B9%A6%E5%86%99%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[编解码概述书写编解码插件之前首先需要书写Profile文件，定义如下。 Profile文件用于描述一款设备的能力特性。IOT平台通过解析Profile文件，获取该设备支持的服务（通信协议里面的一条完整报文）、属性（报文中的一个字段）、命令（报文中的一个字段）等信息。 IOT平台通过编解码插件，对NB设备上报的数据和下发给NB设备的命令进行格式转换，即将设备上报的16进制格式的报文转换成json，json数据的具体属性名称将由Profile文件来确定。 前期准备知识储备熟悉javase基础编程，maven项目管理工具基本命令，jackson类库。 开发工具Eclipse，jdk1.8，maven，NB设备通信协议。 编解码结构解析Profile结构Profile文件是一个深度为四级的文件夹如图3‑4所示。 红色方框中为第一级目录，目录名称为当前项目的名称，命名规则为：设备类型-厂商ID-设备型号，例如WellMonitor_Chinastar_CSiTWLM05，对应图3‑4功能结构中的Product模块。 绿色方框中为第二级目录，包含两个文件夹profile和service。profile文件夹下放置一个devicetype-capability.json，被用来描述一款设备的能力特征，包括设备类型、厂商、型号、协议类型以及提供的服务类型。相当于对当前整个Profile项目信息的一个概述。service文件夹下则包含设备具备的服务能力，每个服务具备的属性、命令以及命令的参数，对应图3‑4中Service模块。 蓝色方框中代表三级目录，代表具体的服务，每个服务中又包含了属性和命令，属性用于描述设备上报数据，命令用于下发指令给设备，分别对应图3‑4中的Property模块和Command模块。 此外只有红色与蓝色方框中文件的命名可以改动，其他名称固定不变。 例如图2‑1service中包含了WellWaterCommand代表井盖+水位设备所有的下行命令；WellWaterDeviceInfo代表该设备上报的设备信息；WellWaterTiming代表该设备定时上报的信息，共计三个服务。关于如何根据设备的通信协议划分服务，将在第三章中详细介绍。media/4e8ad1d840e65bfc32b745a7004fb776.png 图2‑1 Profile文件结构 图2‑2 Profile功能结构 编解码插件结构编解码插件使用java开发且jdk版本必须为1.8，maven进行项目管理，建议使用eclipse开发工具。其工程目录结构如图3‑4所示。工程的命名建议和Profile项目相同，即采用：设备类型-厂商ID-设备型号的格式。其余包名类名无需更改，即使用华为提供的名称。 其中下行数据编码表示该类将平台下发的json格式的命令转化为字节数组。 上行数据解码则表示该类将设备上报的字节数组转化为json格式数据。 主类则负责调用下行数据编码或者上行数据解码。 工具类中包含了不同数据类型之间转换的方法。 图2‑3 编解码插件工程结构 编解码插件开发Profile开发新建工程新建名称为：设备类型-厂商ID-设备型号的文件夹，例如WellMonitor_Chinastar_CSiTWLM05，打开该文件夹新建profile和service两个子文件夹，如图3‑4所示。注意profile和service这两个文件夹名称是固定不变的。 图3‑1 Profile文件夹 创建概述文件打开profile文件夹新建devicetype-capability.json文件，编辑该json文件如图3‑4所示。其中每个字段所代表的含义如图3‑4所示。serviceTypeCapabilitiesJson数组包含了该Profile文件所有的服务模块，即每增加一个service都需要在这个数组中添加一个对应的json对象。例如Chinstar公司采用CoAP协议，设备型号为CSiTWLM05的井盖设备包含三个服务（功能）模块，井盖命令、井盖开机信息，井盖定时上报信息。 Service划分关于一个设备service的划分，可以采用根据设备通信协议中上报报文的功能码进行划分的方法，即协议中一条上报报文对应一个service（通常一条报文占用一个功能码）。 Service划分步骤： 将通信协议中的报文区分为上行数据和下行数据两类。 将下行数据（命令）写在一个service里面。 将上行数据根据功能码的不同，分成若干个不同的service。 图3‑2 Profile概述文件 图3‑3 Profile文件字段含义 创建service打开第二级目录的service文件夹，根据步骤二devicetype-capability.json文件中serviceTypeCapabilities数组中定义的service创建对应的文件夹，文件夹名称为serviceId对应的值。每个文件夹下分别创建profile文件夹，profile文件夹下创建servicetype-capability.json文件，用来描述具体服务的功能，如图3‑4所示。 图3‑4 service文件结构 接下来编写每个service对应的servicetype-capability.json，如图3‑4所示的Profile，将井盖水位设备的所有命令下发模块单独写在了一个service中，即WellWaterCommand模块；将设备的上报数据信息按照通信协议中每个报文的功能码区分为不同的service，即WellWaterDeviceInfo（开机信息）模块与WellWaterTiming（定时上报）模块。 编写下行service下行命令在service中的书写格式如图3‑5所示。Commands数组里面存放若干个命令对象，每个命令对象包含CommandName命令的名称和paras命令携带的参数数组，以及responses命令回应数组。paras数组里面包含若干个参数对象。参数对象包含paraName等一系列属性。这每一个参数对象都对应通信协议报文中的一个属性。serviceType的值一定要与当前服务文件夹名称保持一致。 图3‑5 命令service 编写上行service上行属性service比命令service要简单，properties数组中存放的是设备上报上来的数据，每一个json对象对应通信协议报文中的一个属性。propertyName的值可以随意定义，dataType的值如不能根据通信协议中的默认参数值明显的判断该属性的数据类型，应找相关人员确认，否则解析将会出现数据错误。 打包Profile将WellMonitor_Chinastar_CSiTWLM05文件夹下的profile和service文件打包为zip格式，命名为WellMonitor_Chinastar_CSiTWLM05，并且压缩包内不能包含WellMonitor_Chinastar_CSiTWLM05文件夹这一层目录。 编解码开发使用eclipse配置本地maven仓库，并导入华为提供的demo工程。 修改pom.xml中的&lt;artifactId&gt;XXX&lt;/artifactId&gt;和&lt;Bundle-SymbolicName&gt;XXX&lt;/Bundle-SymbolicName&gt;命名规范：设备类型-厂商ID-设备型号，与Profile文件保持一致。如图3‑6、图3‑7所示。 图3‑6 pom修改厂商名称 图3‑7 pom修改厂商名称 修改ProtocolAdapterImpl.java中的厂商名称与设备型号，注意与Profile保持一致。如图3‑8所示。 图3‑8 修改Protocol文件 编写下行编码数据下行编码流程如图3‑9所示，通过获取OC平台发送过来的json数据，首先根据serviceId将对应数据保存到全局变量中，然后调用toByte方法，将各种类型的数据装换为16进制的比特数组。拼装成一条完整的报文并返回。其中mid为消息序号，根据通信协议中是否使用消息序号来决定代码中是否使用。详情请参照文末样例代码。 图3‑9 数据下行编码流程图 编写上行解码数据上行解码流程如图3‑10所示。通过获取OC平台发送过来的16进制比特数组格式数据，首先根据比特数组中的功能码（功能码在数据哪个位置，请从通信协议中查看），将不同服务对应的不同数据，根据通信协议将byte类型的属性转换为所需类型，保存到全局变量中，然后调用toJsonNode方法，拼装成一个Json对象并返回。其中mid为消息序号，根据通信协议中是否使用消息序号来决定代码中是否使用。详情请参照文末样例代码。 图3‑10 数据上行解码流程图 打包编解码打包编解码，即将该maven工程打包成jar文件。检查pom.xml中的&lt;packaging&gt;bundle&lt;/packaging&gt;值是否为bundle，不能为jar。以eclipse为例，在工程上右键选择RunAs然后选择Maven build…如图3‑11所示。 图3‑11 Mavenbuild 在Goals中输入clean package点击Run按钮，如图3‑12所示。 图3‑12 Mavenpackage 等待控制台出现BUILDSUCCESS，即可在当前项目的target目录下找到打包好的jar文件，如图3‑13所示，打包编解码完成。 图3‑13 Build 附录 点击显/隐内容 上行代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557package com.thrid.party.codec.demo;import java.util.Arrays;import com.fasterxml.jackson.databind.ObjectMapper;import com.fasterxml.jackson.databind.node.ArrayNode;import com.fasterxml.jackson.databind.node.ObjectNode;public class ReportProcess &#123;// private String identifier;private String msgType = &quot;deviceReq&quot;;private int hasMore = 0;private int errcode = 0;// 在报文中表示报文类型的字段private byte bFunctionCode;// 开机信息功能码private static final byte DEVICE_POWER_ON = 0x01;// 数据上报功能码private static final byte DEVICE_DATA_REPORT = 0x02;// 应答报文功能码private static final byte DEVICE_REPLY = (byte) 0xAA;// 无后续数据private static final int NO_MORE = 0;private static final String REQUEST =&quot;deviceReq&quot;;private static final String RESPONSE =&quot;deviceRsp&quot;;private int identifier;private int protocolVersion;// 终端类型默认为12private int deviceType;private int iDeviceState = 0;// 终端压力阈值（默认未0），上行数据会对这个值进行覆盖private int distanceAlarmThreshold = 0;;// 终端定时上报时间 ，上行数据会对这个值进行覆盖private int iHeartBeatTime = 0;// 终端报警间隔时间，上行数据会对这个值进行覆盖private int iAlarmReportInterval = 0;private int iSampleInterval = 0;// 终端硬件版本，上行数据会对这个值进行覆盖private int iVersionHW = 1;// 终端软件版本，，上行数据会对这个值进行覆盖private int iVersionFW = 1;// 终端SN号，上行数据会对这个值进行覆盖private int iDeviceSN = 1;private int iNbiotState = 0;// 电池状态，上行数据会对这个值进行覆盖private int iBatteryState = 0;// 云端应答状态，上行数据会对这个值进行覆盖private int iAckState = 0;private int iGuardState = 0;// 报警状态 （默认为0不报警），在定时上报中会对这个值进行覆盖private int iAlarmState = 0;// 当前压力，在定时上报中会对这个值进行覆盖private int iCurPValue = 0;// 当前角度，在定时上报中会对这个值进行覆盖private int iBatteryVoltage = 3600;private int iBatteryCap;// 信号强度，在定时上报中会对这个值进行覆盖private int iSignalStrength = 0;// 信号覆盖等级，在定时上报中会对这个值进行覆盖private int iSignalECL = 0;// 信噪比，在定时上报中会对这个值进行覆盖private int iSignalSNR = 0;// 小区所在Id,在定时上报中会对这个值进行覆盖private int iCellId = 0;// 小区所在得PCI,在定时上报中会对这个值进行覆盖private int iSignalPCI = 0;// 温度，在定时上报中会对这个值进行覆盖// private int iTemperature = 0;// 终端回应设置信息时上发的错误码private int iResult = 0;private int mid = 0;private int curTilt = 0;private int bgTilt = 0;private int gasDensity = 0;private int alarmThreshold = 0;private int waterStatus = 0;private int deviceState = 0;private String strDevPostfix;private String imei;private String imsi;/\*\*\* \@param binaryData 设备发送给平台coap报文\* \@return\*/public ReportProcess(byte[] binaryData) &#123;binaryData = Utilty.getInstance().positionFormat(binaryData);protocolVersion = binaryData[0];bFunctionCode = binaryData[1];identifier = Utilty.getInstance().bytes2Int(binaryData, 2, 2);mid = Utilty.getInstance().bytes2Int(binaryData, 4, 2);Utilty.getInstance().mid = mid;Utilty.getInstance().functioncode = bFunctionCode;System.out.println(&quot;dingmingdong\\n&quot;);System.out.println(Utilty.getInstance().mid);System.out.println(identifier);if (bFunctionCode == DEVICE_POWER_ON) &#123;msgType = REQUEST;hasMore = NO_MORE;iDeviceSN = Utilty.getInstance().bytes2Int(binaryData,8, 4);deviceType = binaryData[12];// iGuardState = binaryData[5];iVersionHW = binaryData[13];iVersionFW = Utilty.getInstance().bytes2Int(binaryData,14, 4);//strDevPostfix = new String(binaryData, 14, 16);//iDeviceState = Utilty.getInstance().bytes2Int(binaryData,30, 2);iHeartBeatTime = Utilty.getInstance().bytes2Int(binaryData, 20, 2);iAlarmReportInterval = Utilty.getInstance().bytes2Int(binaryData, 22, 2);iSampleInterval = Utilty.getInstance().bytes2Int(binaryData, 24, 2);imei = new String(binaryData, 26, 16);imsi = new String(binaryData, 42, 16);alarmThreshold = Utilty.getInstance().bytes2Int(binaryData, 58, 2);//binArray用于存放状态码的二进制数字，由于binaryData[31]转换的二进制数组中目前未携带有用信息，不做解析// int[] binArray = Utilty.getInstance().byte2Binary(binaryData[30]);// iBatteryState = binArray[0];//iAckState = binArray[1];// iNbiotState = binArray[2];// iGuardState = binArray[3];// iAlarmState = binArray[4];// iDeviceState = binArray[5];// iBlueteethState = binArray[6];// strModuleVersion = new String(binaryData, 21, 20);&#125; else if( bFunctionCode == DEVICE_DATA_REPORT )&#123;msgType = REQUEST;hasMore = NO_MORE;iDeviceSN = Utilty.getInstance().bytes2Int(binaryData,8, 4);deviceState = Utilty.getInstance().bytes2Int(binaryData,12, 2);// iBatteryVoltage = Utilty.getInstance().bytes2Int(binaryData, 4, 2);iBatteryCap = Utilty.getInstance().bytes2Int(binaryData, 14, 1);iSignalStrength = Utilty.getInstance().bytes2Int(binaryData, 16, 4);iSignalECL = binaryData[20];iSignalSNR = binaryData[21];iCellId = Utilty.getInstance().bytes2Int(binaryData, 22, 4);iSignalPCI = Utilty.getInstance().bytes2Int(binaryData, 26, 2);bgTilt = Utilty.getInstance().bytes2Int(binaryData, 28, 2);curTilt = Utilty.getInstance().bytes2Int(binaryData, 30, 2);gasDensity = Utilty.getInstance().bytes2Int(binaryData, 32, 2);waterStatus = Utilty.getInstance().bytes2Int(binaryData, 34, 2);&#125;else if (bFunctionCode == DEVICE_REPLY) &#123;msgType = RESPONSE;// 在华为的API中规定 ： errcode 为0表示成功，1表示失败 ；但下位机协议中规定错误码为0无错误，1超范围，2检验错误；在profile文件中终端的应答应当包含一个int型的result参数,// 为了避免数据损失，在这里将终端的错误码放在result中，而errcode按照华为的API规定，只要下位机的错误码不为零，就认为失败，即在error中装入1.errcode = binaryData[8]==0?0:1;iResult = binaryData[8];&#125; else &#123;return;&#125;&#125;public ObjectNode toJsonNode() &#123;try &#123;//组装body体ObjectMapper mapper = new ObjectMapper();ObjectNode root = mapper.createObjectNode();String s = Integer.toString(this.identifier);root.put(&quot;identifier&quot;, s);System.out.println(s);root.put(&quot;msgType&quot;, this.msgType);//根据msgType字段组装消息体if (this.msgType.equals(REQUEST) &amp;&amp; bFunctionCode == DEVICE_POWER_ON) &#123;root.put(&quot;hasMore&quot;, this.hasMore);ArrayNode arrynode = mapper.createArrayNode();// serviceId = Basic 数据组装ObjectNode BasicNode = mapper.createObjectNode();BasicNode.put(&quot;serviceId&quot;, &quot;Basic&quot;);ObjectNode BasicData = mapper.createObjectNode();BasicData.put(&quot;heartBeatTime&quot;, this.iHeartBeatTime);BasicData.put(&quot;alarmReportInterval&quot;, this.iAlarmReportInterval);BasicData.put(&quot;sampleInterval&quot;, this.iSampleInterval);BasicData.put(&quot;mid&quot;, this.mid);//hydrantBasicData.put(&quot;tiltAlarmThreshold&quot;, this.iTiltAlarmThreshold);BasicNode.put(&quot;serviceData&quot;,BasicData);arrynode.add(BasicNode);// serviceId = Tilt 数据组装ObjectNode TiltNode = mapper.createObjectNode();TiltNode.put(&quot;serviceId&quot;, &quot;Tilt&quot;);ObjectNode TiltData = mapper.createObjectNode();TiltData.put(&quot;alarmThreshold&quot;, this.alarmThreshold);//hydrantBasicData.put(&quot;tiltAlarmThreshold&quot;, this.iTiltAlarmThreshold);TiltNode.put(&quot;serviceData&quot;,TiltData);arrynode.add(TiltNode);// serviceId = DeviceInfo 数据组装ObjectNode deviceInfoNode = mapper.createObjectNode();deviceInfoNode.put(&quot;serviceId&quot;, &quot;DeviceInfo&quot;);ObjectNode deviceInfoData = mapper.createObjectNode();deviceInfoData.put(&quot;deviceType&quot;, this.deviceType);deviceInfoData.put(&quot;versionHW&quot;, this.iVersionHW);deviceInfoData.put(&quot;versionFW&quot;, this.iVersionFW);deviceInfoData.put(&quot;deviceSN&quot;, this.iDeviceSN);deviceInfoData.put(&quot;imei&quot;, this.imei);deviceInfoData.put(&quot;imsi&quot;, this.imsi);deviceInfoData.put(&quot;deviceID&quot;, this.identifier);deviceInfoData.put(&quot;protocolVersion&quot;, this.protocolVersion);//deviceInfoData.put(&quot;moduleVersion&quot;, this.strModuleVersion);deviceInfoNode.put(&quot;serviceData&quot;,deviceInfoData);arrynode.add(deviceInfoNode);root.put(&quot;data&quot;, arrynode);&#125; else if(this.msgType.equals(REQUEST) &amp;&amp; bFunctionCode == DEVICE_DATA_REPORT)&#123;root.put(&quot;hasMore&quot;, this.hasMore);ArrayNode arrynode = mapper.createArrayNode();// serviceId = Basic 数据组装ObjectNode BasicNode = mapper.createObjectNode();BasicNode.put(&quot;serviceId&quot;, &quot;Basic&quot;);ObjectNode BasicData = mapper.createObjectNode();//BasicData.put(&quot;batteryVoltage&quot;, this.iBatteryVoltage);BasicData.put(&quot;batteryLevel&quot;,this.iBatteryCap);BasicData.put(&quot;deviceState&quot;, this.deviceState);BasicData.put(&quot;mid&quot;, this.mid);BasicNode.put(&quot;serviceData&quot;,BasicData);arrynode.add(BasicNode);// serviceId = Connectivity 数据组装ObjectNode connectivityNode = mapper.createObjectNode();ObjectNode connectivityData = mapper.createObjectNode();connectivityData.put(&quot;signalStrength&quot;, this.iSignalStrength);connectivityData.put(&quot;cellId&quot;, this.iCellId);connectivityData.put(&quot;signalECL&quot;, this.iSignalECL);connectivityData.put(&quot;signalPCI&quot;, this.iSignalPCI);connectivityData.put(&quot;signalSNR&quot;, this.iSignalSNR);connectivityNode.put(&quot;serviceId&quot;, &quot;Connectivity&quot;);connectivityNode.put(&quot;serviceData&quot;,connectivityData);arrynode.add(connectivityNode);// serviceId = Tilt 数据组装ObjectNode tiltNode = mapper.createObjectNode();ObjectNode tiltData = mapper.createObjectNode();tiltData.put(&quot;curTilt&quot;, this.curTilt);tiltData.put(&quot;bgTilt&quot;, this.bgTilt);tiltData.put(&quot;gasDensity&quot;, this.gasDensity);tiltData.put(&quot;waterStatus&quot;, this.waterStatus);tiltNode.put(&quot;serviceId&quot;, &quot;Tilt&quot;);tiltNode.put(&quot;serviceData&quot;,tiltData);arrynode.add(tiltNode);// serviceId = DeviceInfo 数据组装ObjectNode deviceInfoNode = mapper.createObjectNode();deviceInfoNode.put(&quot;serviceId&quot;, &quot;DeviceInfo&quot;);ObjectNode deviceInfoData = mapper.createObjectNode();// deviceInfoData.put(&quot;deviceType&quot;, this.deviceType);// deviceInfoData.put(&quot;versionHW&quot;, this.iVersionHW);// deviceInfoData.put(&quot;versionFW&quot;, this.iVersionFW);deviceInfoData.put(&quot;deviceSN&quot;, this.iDeviceSN);// deviceInfoData.put(&quot;imei&quot;, this.imei);// deviceInfoData.put(&quot;imsi&quot;, this.imsi);deviceInfoData.put(&quot;deviceID&quot;, this.identifier);deviceInfoData.put(&quot;protocolVersion&quot;, this.protocolVersion);//deviceInfoData.put(&quot;moduleVersion&quot;, this.strModuleVersion);deviceInfoNode.put(&quot;serviceData&quot;,deviceInfoData);arrynode.add(deviceInfoNode);// serviceId = DeviceState 数据组装// ObjectNode deviceStateNode = mapper.createObjectNode();// ObjectNode deviceStateData = mapper.createObjectNode();//deviceStateData.put(&quot;guardState&quot;, this.iGuardState);//deviceStateData.put(&quot;alarmState&quot;, this.iAlarmState);//deviceStateData.put(&quot;moduleState&quot;, this.iNbiotState);//deviceStateData.put(&quot;batteryState&quot;, this.iBatteryState);//deviceStateData.put(&quot;ackState&quot;, this.iAckState);//deviceStateNode.put(&quot;serviceId&quot;, &quot;DeviceState&quot;);//deviceStateNode.put(&quot;serviceData&quot;,deviceStateData);//arrynode.add(deviceStateNode);// serviceId = Temperature 数据组装// ObjectNode temperatureNode = mapper.createObjectNode();// ObjectNode temperatureData = mapper.createObjectNode();// temperatureData.put(&quot;temperature&quot;, this.iTemperature);// temperatureNode.put(&quot;serviceId&quot;, &quot;Temperature&quot;);// temperatureNode.put(&quot;serviceData&quot;,temperatureData);// arrynode.add(temperatureNode);root.put(&quot;data&quot;, arrynode);&#125;else &#123;root.put(&quot;mid&quot;, this.mid);root.put(&quot;errcode&quot;, this.errcode);//组装body体，只能为ObjectNode对象ObjectNode body = mapper.createObjectNode();body.put(&quot;result&quot;, iResult);root.put(&quot;body&quot;, body);&#125;return root;&#125; catch (Exception e) &#123;e.printStackTrace();return null;&#125;&#125;&#125; 下行数据代码。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725package com.thrid.party.codec.demo;import java.util.Arrays;import com.fasterxml.jackson.databind.JsonNode;import com.fasterxml.jackson.databind.node.ObjectNode;public class CmdProcess &#123;private String identifier = &quot;0&quot;;private int intIdentifier = 0;private String msgType = &quot;deviceReq&quot;;private String serviceId = &quot;Brightness&quot;;private int hasMore = 0;private static int mid = 0;private int errcode = 0;private JsonNode paras;private String cmd = &quot;REREAD&quot;;// 云端回应private static final byte B_CMD_RESPONSE = (byte) 0xAA;// 配置信息private static final byte B_TO_CONFIGURATION = 0x03;// 复位命令private static final byte B_TO_RESET = 0x04;// 布防/撤防private static final byte B_TO_GUARD_OR_WITHDRAW = 0x05;// 忽略本次报警private static final byte B_TO_IGNORE = 0x06;// 重新读取开机信息private static final byte B_TO_REREAD = 0x07;private static final byte B_TO_FACTORYDEFAULT = 0x09;private static final byte B_TO_SAMPLE_BGTILT_RSP = 10;// 在报文中表示报文类型的字段private byte bFunctionCode;// 在报文中表示信息长度的字段private byte dataLength;// 将要配置的新IDprivate int iNewId;// 上报时间private int iHeartBeatTime;// 报警间隔private int iAlarmReportInterval;private int iSampleInterval;private byte protocolVersion = 1;// IP设置private int iAddressIP;// 端口号设置private int iAddressPort;// 压力报警阈值private int distanceAlarmThreshold;private int reportmid;private int iPressureAlarmLowThreshold;private int iPressureAlarmUpDiff;private int iPressureAlarmLowDiff;// 倾斜角报警阈值// private int iTiltAlarmThreshold;// 在命令为GUARD_OR_WITHDRAW时是要布防还是撤防： 1表示布防，0表示撤防private byte bGuardOrWithdraw = 1;// 用于存储平台自动应答时返回的request字段，该字段即设备的上行数据byte[] deviceRequest;public CmdProcess() &#123;&#125;public CmdProcess(ObjectNode input) &#123;try &#123;try &#123;this.identifier = input.get(&quot;identifier&quot;).asText();&#125; catch (Exception e) &#123;&#125;mid = mid + 1;this.msgType = input.get(&quot;msgType&quot;).asText();// this.mid = input.get(&quot;mid&quot;).asInt();//未使用mid在此处不能放出来，否则会造成程序异常if (msgType.equals(&quot;cloudRsp&quot;)) &#123;this.errcode = input.get(&quot;errcode&quot;).asInt();deviceRequest = input.get(&quot;request&quot;).binaryValue();byte[] identifierBytes = &#123;deviceRequest[3],deviceRequest[2]&#125;;intIdentifier = Utilty.getInstance().bytes2Int(identifierBytes, 0, 2);byte[] midBytes = &#123;deviceRequest[5],deviceRequest[4]&#125;;reportmid = Utilty.getInstance().bytes2Int(midBytes, 0, 2);bFunctionCode = B_CMD_RESPONSE;dataLength = 0x01;&#125; else &#123;this.cmd = input.get(&quot;cmd&quot;).asText();switch (this.cmd) &#123;case &quot;CONFIGURATION&quot;:bFunctionCode = B_TO_CONFIGURATION;dataLength = 22;this.paras = input.get(&quot;paras&quot;);iNewId = this.paras.get(&quot;newId&quot;).asInt();iHeartBeatTime = this.paras.get(&quot;heartBeatTime&quot;).asInt();iAlarmReportInterval = this.paras.get(&quot;alarmReportInterval&quot;).asInt();iSampleInterval = this.paras.get(&quot;sampleInterval&quot;).asInt();iAddressIP = this.paras.get(&quot;addressIP&quot;).asInt();iAddressPort = this.paras.get(&quot;addressPort&quot;).asInt();distanceAlarmThreshold = this.paras.get(&quot;distanceAlarmThreshold&quot;).asInt();//iPressureAlarmLowThreshold =this.paras.get(&quot;pressureAlarmLowThreshold&quot;).asInt();//iPressureAlarmUpDiff = this.paras.get(&quot;pressureAlarmUpDiff&quot;).asInt();//iPressureAlarmLowDiff = this.paras.get(&quot;pressureAlarmLowDiff&quot;).asInt();//iTiltAlarmThreshold = this.paras.get(&quot;tiltAlarmThreshold&quot;).asInt();break;case &quot;RESET&quot;:bFunctionCode = B_TO_RESET;dataLength = 0x00;break;case &quot;SAMPLE_BGTILT_RSP&quot;:bFunctionCode = B_TO_SAMPLE_BGTILT_RSP;dataLength = 0x00;break;case &quot;GUARD_OR_WITHDRAW&quot;:bFunctionCode = B_TO_GUARD_OR_WITHDRAW;dataLength = 0x01;this.paras = input.get(&quot;paras&quot;);bGuardOrWithdraw = (byte) this.paras.get(&quot;guardOrWithdraw&quot;).asInt();break;case &quot;IGNORE&quot;:bFunctionCode = B_TO_IGNORE;dataLength = 0x00;break;case &quot;REREAD&quot;:bFunctionCode = B_TO_REREAD;dataLength = 0x00;break;case &quot;FACTORYDEFAULT&quot;:bFunctionCode = B_TO_FACTORYDEFAULT;dataLength = 0x00;break;default:break;&#125;&#125;&#125; catch (Exception e) &#123;e.printStackTrace();&#125;&#125;public byte[] toByte() &#123;try &#123;if (this.msgType.equals(&quot;cloudReq&quot;)) &#123;byte[] bytesRead = null;int idid = 0;try &#123;idid = Integer.parseInt(this.identifier);&#125; catch (NumberFormatException e) &#123;e.printStackTrace();&#125;switch (this.cmd) &#123;case &quot;CONFIGURATION&quot;:&#123;//iNewId = this.paras.get(&quot;newId&quot;).asInt();//iHeartBeatTime = this.paras.get(&quot;heartBeatTime&quot;).asInt();//iAlarmReportInterval = this.paras.get(&quot;alarmReportInterval&quot;).asInt();//iAddressIP = this.paras.get(&quot;addressIP&quot;).asInt();//iAddressPort = this.paras.get(&quot;addressPort&quot;).asInt();//iPressureAlarmThreshold = this.paras.get(&quot;pressureAlarmThreshold&quot;).asInt();//iTiltAlarmThreshold = this.paras.get(&quot;tiltAlarmThreshold&quot;).asInt();dataLength = 18;bytesRead= new byte[28];bytesRead[0] = (byte)protocolVersion;bytesRead[1] = bFunctionCode;//byte[] bytesId = Utilty.getInstance().str2Bytes(this.identifier);bytesRead[2] = (byte) (idid &amp; 0xFF);bytesRead[3] = (byte) (idid \&gt;\&gt; 8);byte[] bytesmid = Utilty.getInstance().int2Bytes(CmdProcess.mid, 2);bytesRead[4] = bytesmid[1];bytesRead[5] = bytesmid[0];bytesRead[6] = dataLength;bytesRead[7] = 0;byte[] bytesNewId = Utilty.getInstance().int2Bytes(this.iNewId, 2);bytesRead[8] = bytesNewId[1];bytesRead[9] = bytesNewId[0];byte[] bytesHeartBeatTime = Utilty.getInstance().int2Bytes(this.iHeartBeatTime,2);bytesRead[10] = bytesHeartBeatTime[1];bytesRead[11] = bytesHeartBeatTime[0];byte[] bytesAlarmReportInterval =Utilty.getInstance().int2Bytes(this.iAlarmReportInterval, 2);bytesRead[12] = bytesAlarmReportInterval[1];bytesRead[13] = bytesAlarmReportInterval[0];byte[] bytesSampleInterval =Utilty.getInstance().int2Bytes(this.iSampleInterval, 2);bytesRead[14] = bytesSampleInterval[1];bytesRead[15] = bytesSampleInterval[0];byte[] bytesAddressIP = Utilty.getInstance().int2Bytes(this.iAddressIP, 4);bytesRead[16] = bytesAddressIP[3];bytesRead[17] = bytesAddressIP[2];bytesRead[18] = bytesAddressIP[1];bytesRead[19] = bytesAddressIP[0];byte[] bytesAddressPort = Utilty.getInstance().int2Bytes(this.iAddressPort, 2);bytesRead[20] = bytesAddressPort[1];bytesRead[21] = bytesAddressPort[0];byte[] bytesAlarmThreshold =Utilty.getInstance().int2Bytes(this.distanceAlarmThreshold, 2);bytesRead[22] = bytesAlarmThreshold[1];bytesRead[23] = bytesAlarmThreshold[0];bytesRead[24] = 0;bytesRead[25] = 0;byte[] bytesNoCRC = Arrays.copyOf(bytesRead, bytesRead.length - 2);byte[] bytesCRC = Utilty.getInstance().CRC16(bytesNoCRC);bytesRead[26] = bytesCRC[1];bytesRead[27] = bytesCRC[0];break;&#125;case &quot;RESET&quot;:&#123;bFunctionCode = B_TO_RESET;dataLength = 0x00;bytesRead= new byte[10];bytesRead[0] = (byte)protocolVersion;bytesRead[1] = bFunctionCode;byte[] bytesId = Utilty.getInstance().str2Bytes(this.identifier);bytesRead[2] = (byte) (idid &amp; 0xFF);bytesRead[3] = (byte) (idid \&gt;\&gt; 8);byte[] bytesmid = Utilty.getInstance().int2Bytes(CmdProcess.mid, 2);bytesRead[4] = bytesmid[1];bytesRead[5] = bytesmid[0];bytesRead[6] = dataLength;bytesRead[7] = 0;byte[] bytesNoCRC = Arrays.copyOf(bytesRead, bytesRead.length - 2);byte[] bytesCRC = Utilty.getInstance().CRC16(bytesNoCRC);bytesRead[8] = bytesCRC[1];bytesRead[9] = bytesCRC[0];break;&#125;case &quot;GUARD_OR_WITHDRAW&quot;:&#123;bFunctionCode = B_TO_GUARD_OR_WITHDRAW;bGuardOrWithdraw = (byte) this.paras.get(&quot;guardOrWithdraw&quot;).asInt();dataLength = 0x02;bytesRead= new byte[12];bytesRead[0] = (byte)protocolVersion;bytesRead[1] = bFunctionCode;byte[] bytesId = Utilty.getInstance().str2Bytes(this.identifier);bytesRead[2] = (byte) (idid &amp; 0xFF);bytesRead[3] = (byte) (idid \&gt;\&gt; 8);byte[] bytesmid = Utilty.getInstance().int2Bytes(CmdProcess.mid, 2);bytesRead[4] = bytesmid[1];bytesRead[5] = bytesmid[0];bytesRead[6] = dataLength;bytesRead[7] = 0;bytesRead[8] = bGuardOrWithdraw;bytesRead[9] = 0;byte[] bytesNoCRC = Arrays.copyOf(bytesRead, bytesRead.length - 2);byte[] bytesCRC = Utilty.getInstance().CRC16(bytesNoCRC);bytesRead[10] = bytesCRC[1];bytesRead[11] = bytesCRC[0];break;&#125;case &quot;IGNORE&quot;:&#123;bFunctionCode = B_TO_IGNORE;dataLength = 0x00;bytesRead= new byte[10];bytesRead[0] = (byte)protocolVersion;bytesRead[1] = bFunctionCode;byte[] bytesId = Utilty.getInstance().str2Bytes(this.identifier);bytesRead[2] = (byte) (idid &amp; 0xFF);bytesRead[3] = (byte) (idid \&gt;\&gt; 8);byte[] bytesmid = Utilty.getInstance().int2Bytes(CmdProcess.mid, 2);bytesRead[4] = bytesmid[1];bytesRead[5] = bytesmid[0];bytesRead[6] = dataLength;bytesRead[7] = 0;byte[] bytesNoCRC = Arrays.copyOf(bytesRead, bytesRead.length - 2);byte[] bytesCRC = Utilty.getInstance().CRC16(bytesNoCRC);bytesRead[8] = bytesCRC[1];bytesRead[9] = bytesCRC[0];break;&#125;case &quot;SAMPLE_BGTILT_RSP&quot;:&#123;bFunctionCode = B_TO_SAMPLE_BGTILT_RSP;dataLength = 0x00;bytesRead= new byte[10];bytesRead[0] = (byte)protocolVersion;bytesRead[1] = bFunctionCode;byte[] bytesId = Utilty.getInstance().str2Bytes(this.identifier);bytesRead[2] = (byte) (idid &amp; 0xFF);bytesRead[3] = (byte) (idid \&gt;\&gt; 8);byte[] bytesmid = Utilty.getInstance().int2Bytes(CmdProcess.mid, 2);bytesRead[4] = bytesmid[1];bytesRead[5] = bytesmid[0];bytesRead[6] = dataLength;bytesRead[7] = 0;byte[] bytesNoCRC = Arrays.copyOf(bytesRead, bytesRead.length - 2);byte[] bytesCRC = Utilty.getInstance().CRC16(bytesNoCRC);bytesRead[8] = bytesCRC[1];bytesRead[9] = bytesCRC[0];break;&#125;case &quot;FACTORYDEFAULT&quot;:&#123;bFunctionCode = B_TO_FACTORYDEFAULT;dataLength = 0x00;bytesRead= new byte[10];bytesRead[0] = (byte)protocolVersion;bytesRead[1] = bFunctionCode;byte[] bytesId = Utilty.getInstance().str2Bytes(this.identifier);bytesRead[2] = (byte) (idid &amp; 0xFF);bytesRead[3] = (byte) (idid \&gt;\&gt; 8);byte[] bytesmid = Utilty.getInstance().int2Bytes(CmdProcess.mid, 2);bytesRead[4] = bytesmid[1];bytesRead[5] = bytesmid[0];bytesRead[6] = dataLength;bytesRead[7] = 0;byte[] bytesNoCRC = Arrays.copyOf(bytesRead, bytesRead.length - 2);byte[] bytesCRC = Utilty.getInstance().CRC16(bytesNoCRC);bytesRead[8] = bytesCRC[1];bytesRead[9] = bytesCRC[0];break;&#125;case &quot;REREAD&quot;:&#123;bFunctionCode = B_TO_REREAD;dataLength = 0x00;bytesRead= new byte[10];bytesRead[0] = (byte)protocolVersion;bytesRead[1] = bFunctionCode;byte[] bytesId = Utilty.getInstance().str2Bytes(this.identifier);bytesRead[2] = (byte) (idid &amp; 0xFF);bytesRead[3] = (byte) (idid \&gt;\&gt; 8);byte[] bytesmid = Utilty.getInstance().int2Bytes(CmdProcess.mid, 2);bytesRead[4] = bytesmid[1];bytesRead[5] = bytesmid[0];bytesRead[6] = dataLength;bytesRead[7] = 0;byte[] bytesNoCRC = Arrays.copyOf(bytesRead, bytesRead.length - 2);byte[] bytesCRC = Utilty.getInstance().CRC16(bytesNoCRC);bytesRead[8] = bytesCRC[1];bytesRead[9] = bytesCRC[0];break;&#125;default:break;&#125;return bytesRead;&#125; else if (this.msgType.equals(&quot;cloudRsp&quot;)) &#123;byte[] bytesRead = new byte[12];/\*\*平台对上行数据的自动应答过程中，并不会每次都去调用带参的构造方法（猜测），所以数据的具体组装需要在toByte方法中进行以保证回复及下发的稳定性\*/bFunctionCode = B_CMD_RESPONSE;dataLength = 2;/\*\*在此对捕获的设备命令进行校验，如果无错误，将errcode置为0，若校验错误，将其置为2\*/boolean isValide = false;if(deviceRequest != null)&#123;isValide = Utilty.getInstance().isValid(deviceRequest);&#125;bytesRead= new byte[12];bytesRead[0] = (byte)protocolVersion;bytesRead[1] = B_CMD_RESPONSE;//byte[] bytesId = Utilty.getInstance().int2Bytes(this.intIdentifier, 2);;bytesRead[2] = deviceRequest[2];bytesRead[3] = deviceRequest[3];//byte[] bytesmid = Utilty.getInstance().int2Bytes(this.reportmid, 2);bytesRead[4] = deviceRequest[4];bytesRead[5] = deviceRequest[5];bytesRead[6] = dataLength;bytesRead[7] = 0;bytesRead[8] = 0;//(byte) (isValide?0:2);bytesRead[9] = (byte)Utilty.getInstance().functioncode;byte[] bytesNoCRC = Arrays.copyOf(bytesRead, bytesRead.length - 2);byte[] bytesCRC = Utilty.getInstance().CRC16(bytesNoCRC);bytesRead[10] = bytesCRC[1];bytesRead[11] = bytesCRC[0];return bytesRead;&#125;return null;&#125; catch (Exception e) &#123;// TODO: handle exceptione.printStackTrace();return null;&#125;&#125;&#125;]]></content>
      <categories>
        <category>Iot</category>
      </categories>
      <tags>
        <tag>OceanConnect</tag>
        <tag>编解码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty组件和设计]]></title>
    <url>%2FNetty%2FNetty%E7%BB%84%E4%BB%B6%E5%92%8C%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[Netty的组件和设计Channel 接口 在基于Java的网络编程中，其基本的构造是class socket。Netty的Channel接口所提供的api，大大的降低了直接使用Socket类的复杂性。 EventLoop 接口 EventLoop 定义了 Netty 的核心抽象，用于处理连接的生命周期中所发生的事件。 一个EventLoopGroup包含一个或者多个EventLoop； 一个EventLoop在它的生命周期内只和一个Thread绑定； 所有由EventLoop处理的 I/O 事件都将在它专有的Thread上被处理； 一个Channel在它的生命周期内只注册于一个EventLoop； 一个EventLoop可能会被分配给一个或多个Channel。 ChannelFuture 接口 Netty 中所有的 I/O 操作都是异步的。因为一个操作可能不会 立即返回，所以我们需要一种用于在之后的某个时间点确定其结果的方法。为此，Netty 提供了 ChannelFuture接口，其addListener()方法注册了一个ChannelFutureListener，以 便在某个操作完成时（无论是否成功）得到通知。 ChannelHandler 接口 充当了所有 处理入站和出站数据的应用程序逻辑的容器。 ChannelPipeline 接口 ChannelPipeline 提供了 ChannelHandler 链的容器，并定义了用于在该链上传播入站 和出站事件流的 API。当 Channel被创建时，它会被自动地分配到它专属的ChannelPipeline。 ChannelPipeline中存放的是ChannelHandler链，一条数据可以经过多个ChannelHandler进行处理，类似拦截器。 引导 Bootstrap 客户端配置 ServerBootstrap 服务端配置 引导一个客户端只需要一个 EventLoopGroup，但是一个 ServerBootstrap则需要两个（也可以是同一个实例）。 因为服务器需要两组不同的 Channel。第一组将只包含一个 ServerChannel，代表服务 器自身的已绑定到某个本地端口的正在监听的套接字。而第二组将包含所有已创建的用来处理传 入客户端连接（对于每个服务器已经接受的连接都有一个）的 Channel。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
</search>
